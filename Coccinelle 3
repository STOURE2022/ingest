"""
ingestion.py
-------------
Impl√©mentation des modes d‚Äôingestion WAX (version hybride locale / Databricks).

Modes support√©s :
- FULL_SNAPSHOT
- DELTA_FROM_FLOW
- DELTA_FROM_NON_HISTORIZED
- DELTA_FROM_HISTORIZED
- FULL_KEY_REPLACE
- Fallback (append)
"""

import os
from datetime import datetime
import pyspark.sql.functions as F
from pyspark.sql import DataFrame
from delta.tables import DeltaTable
from pyspark.sql.utils import AnalysisException


# ==============================================================
# 1Ô∏è‚É£ OUTILS G√âN√âRIQUES
# ==============================================================

def is_databricks():
    """Retourne True si on est sur Databricks."""
    return "DATABRICKS_RUNTIME_VERSION" in os.environ


def build_output_path(env: str, zone: str, table_name: str, version: str, parts: dict = None):
    """
    Construit le chemin Delta Lake standardis√© pour la table.
    Adapt√© pour DBFS (Databricks) ou pour un chemin local.
    """
    if zone.startswith("dbfs:") or zone.startswith("/mnt"):
        return f"{zone}/{env}/{version}/{table_name}"
    else:
        base_dir = os.path.join(os.getcwd(), "data", "output", env, version, zone)
        os.makedirs(base_dir, exist_ok=True)
        return os.path.join(base_dir, table_name)


# ==============================================================
# 2Ô∏è‚É£ SAUVEGARDE G√âN√âRALIS√âE
# ==============================================================

def save_delta(
    spark,
    df: DataFrame,
    path: str,
    mode: str = "append",
    add_ts: bool = False,
    parts: dict = None,
    file_name_received: str = None
):
    """Sauvegarde Delta (ou Parquet fallback) avec colonnes yyyy/mm/dd."""

    today = datetime.today()
    y = int((parts or {}).get("yyyy", today.year))
    m = int((parts or {}).get("mm", today.month))
    d = int((parts or {}).get("dd", today.day))

    if add_ts:
        df = df.withColumn("FILE_PROCESS_DATE", F.current_timestamp())

    if file_name_received:
        base_name = os.path.splitext(os.path.basename(file_name_received))[0]
        df = df.withColumn("FILE_NAME_RECEIVED", F.lit(base_name))

    # Ajout colonnes de partition
    df = (
        df.withColumn("yyyy", F.lit(y).cast("int"))
          .withColumn("mm", F.lit(m).cast("int"))
          .withColumn("dd", F.lit(d).cast("int"))
    )

    try:
        (
            df.write.format("delta")
            .option("mergeSchema", "true")
            .mode(mode)
            .partitionBy("yyyy", "mm", "dd")
            .save(path)
        )
        print(f"‚úÖ Delta saved ‚Üí {path} (mode={mode})")
    except AnalysisException as e:
        print(f"‚ö†Ô∏è Erreur Delta ({e}) ‚Üí fallback en Parquet.")
        df.write.format("parquet").mode(mode).partitionBy("yyyy", "mm", "dd").save(path)
        print(f"‚úÖ Parquet saved ‚Üí {path} (mode={mode})")


# ==============================================================
# 3Ô∏è‚É£ FONCTION PRINCIPALE D‚ÄôINGESTION
# ==============================================================

def apply_ingestion_mode(
    spark,
    df_raw: DataFrame,
    column_defs,
    table_name: str,
    ingestion_mode: str,
    env: str,
    zone: str,
    version: str,
    parts: dict,
    FILE_NAME_RECEIVED: str
):
    """
    Applique le mode d‚Äôingestion selon le param√©trage Excel.
    Compatible local / Databricks.
    """

    path_all = build_output_path(env, zone, f"{table_name}_all", version, parts)
    path_last = build_output_path(env, zone, f"{table_name}_last", version, parts)

    specials = column_defs.copy()
    specials["Is Special lower"] = specials["Is Special"].astype(str).str.lower()

    merge_keys = specials[specials["Is Special lower"] == "ismergekey"]["Column Name"].tolist()
    update_cols = specials[specials["Is Special lower"] == "isstartvalidity"]["Column Name"].tolist()
    update_col = update_cols[0] if update_cols else None

    imode = (ingestion_mode or "").strip().upper()
    print(f"\nüß© Ingestion mode : {imode} | Table : {table_name}")

    # 1Ô∏è‚É£ Enregistrement complet (_all)
    save_delta(spark, df_raw, path_all, mode="append", add_ts=True, parts=parts, file_name_received=FILE_NAME_RECEIVED)

    # 2Ô∏è‚É£ FULL_SNAPSHOT
    if imode == "FULL_SNAPSHOT":
        save_delta(spark, df_raw, path_last, mode="overwrite", parts=parts, file_name_received=FILE_NAME_RECEIVED)

    # 3Ô∏è‚É£ DELTA_FROM_FLOW
    elif imode == "DELTA_FROM_FLOW":
        save_delta(spark, df_raw, path_last, mode="append", add_ts=True, parts=parts, file_name_received=FILE_NAME_RECEIVED)

    # 4Ô∏è‚É£ DELTA_FROM_NON_HISTORIZED
    elif imode == "DELTA_FROM_NON_HISTORIZED":
        if not merge_keys:
            print("‚ö†Ô∏è Aucun merge_key d√©fini ‚Äî fallback overwrite.")
            save_delta(spark, df_raw, path_last, mode="overwrite", add_ts=True, parts=parts, file_name_received=FILE_NAME_RECEIVED)
            return

        compare_col = update_col if update_col else "FILE_PROCESS_DATE"
        print(f"üïí Colonne de comparaison : {compare_col}")

        df_raw = (
            df_raw
            .withColumn("FILE_PROCESS_DATE", F.current_timestamp())
            .withColumn("yyyy", F.lit(parts.get("yyyy", datetime.today().year)).cast("int"))
            .withColumn("mm", F.lit(parts.get("mm", datetime.today().month)).cast("int"))
            .withColumn("dd", F.lit(parts.get("dd", datetime.today().day)).cast("int"))
        )

        try:
            if DeltaTable.isDeltaTable(spark, path_last):
                target = DeltaTable.forPath(spark, path_last)
                cond = " AND ".join([f"target.{k}=updates.{k}" for k in merge_keys])
                (
                    target.alias("target")
                    .merge(df_raw.alias("updates"), cond)
                    .whenMatchedUpdateAll()
                    .whenNotMatchedInsertAll()
                    .execute()
                )
                print(f"‚úÖ MERGE effectu√© sur {merge_keys}")
            else:
                print("‚öôÔ∏è Premi√®re √©criture Delta (table inexistante)")
                save_delta(spark, df_raw, path_last, mode="overwrite", add_ts=True, parts=parts, file_name_received=FILE_NAME_RECEIVED)
        except Exception as e:
            print(f"‚ö†Ô∏è MERGE impossible ({e}) ‚Üí fallback append.")
            save_delta(spark, df_raw, path_last, mode="append", add_ts=True, parts=parts, file_name_received=FILE_NAME_RECEIVED)

    # 5Ô∏è‚É£ DELTA_FROM_HISTORIZED
    elif imode == "DELTA_FROM_HISTORIZED":
        save_delta(spark, df_raw, path_last, mode="append", add_ts=True, parts=parts, file_name_received=FILE_NAME_RECEIVED)

    # 6Ô∏è‚É£ FULL_KEY_REPLACE
    elif imode == "FULL_KEY_REPLACE":
        if not merge_keys:
            raise Exception(f"‚ùå Mode FULL_KEY_REPLACE : merge_keys manquantes pour {table_name}.")

        try:
            if DeltaTable.isDeltaTable(spark, path_last):
                target = DeltaTable.forPath(spark, path_last)
                conds = []
                for k in merge_keys:
                    vals = [str(x[k]) for x in df_raw.select(k).distinct().collect()]
                    conds.append(f"{k} IN ({','.join([f'\"{v}\"' for v in vals])})")
                cond = " OR ".join(conds)
                target.delete(cond)
                print(f"‚ôªÔ∏è Suppression existante sur cl√©s {merge_keys}")
                save_delta(spark, df_raw, path_last, mode="append", add_ts=True, parts=parts, file_name_received=FILE_NAME_RECEIVED)
            else:
                save_delta(spark, df_raw, path_last, mode="overwrite", add_ts=True, parts=parts, file_name_received=FILE_NAME_RECEIVED)
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur FULL_KEY_REPLACE ({e}) ‚Üí fallback append.")
            save_delta(spark, df_raw, path_last, mode="append", add_ts=True, parts=parts, file_name_received=FILE_NAME_RECEIVED)

    # 7Ô∏è‚É£ Fallback append
    else:
        print(f"‚ùå Mode inconnu {imode} ‚Äî fallback append.")
        save_delta(spark, df_raw, path_last, mode="append", add_ts=True, parts=parts, file_name_received=FILE_NAME_RECEIVED)

    print(f"‚úÖ Ingestion termin√©e pour {table_name} ({imode})")
