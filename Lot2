# Databricks notebook source
# MAGIC %md
# MAGIC # üöÄ WAX Data Ingestion Pipeline - Version Finale
# MAGIC 
# MAGIC **Corrections appliqu√©es :**
# MAGIC - ‚úÖ Validation stricte des dates dans les noms de fichiers
# MAGIC - ‚úÖ Cast s√©curis√© sans erreurs (gestion des cha√Ænes vides)
# MAGIC - ‚úÖ Logs de type mismatch avec valeurs originales
# MAGIC - ‚úÖ Sch√©ma unifi√© pour les erreurs qualit√©
# MAGIC - ‚úÖ Affichage robuste sans crash Spark
# MAGIC - ‚úÖ Code DRY et maintenable

# COMMAND ----------

# MAGIC %md
# MAGIC ## üìã Configuration des Widgets

# COMMAND ----------

dbutils.widgets.text("zip_path", "dbfs:/FileStore/tables/wax_delta_from_historized.zip", "üì¶ ZIP Source")
dbutils.widgets.text("excel_path", "dbfs:/FileStore/tables/custom_test2_secret_conf.xlsx", "üìë Excel Config")
dbutils.widgets.text("extract_dir", "dbfs:/tmp/unzipped_wax_csvs", "üìÇ Dossier Extraction ZIP")
dbutils.widgets.text("log_exec_path", "/mnt/logs/wax_execution_logs_delta", "üìù Logs Ex√©cution (Delta)")
dbutils.widgets.text("log_quality_path", "/mnt/logs/wax_data_quality_errors_delta", "üö¶ Log Qualit√© (Delta)")
dbutils.widgets.text("env", "dev", "üåç Environnement")
dbutils.widgets.text("version", "v1", "‚öôÔ∏è Version Pipeline")

PARAMS = {k: dbutils.widgets.get(k) for k in [
    "zip_path", "excel_path", "extract_dir",
    "log_exec_path", "log_quality_path", "env", "version"
]}

print("‚úÖ Param√®tres charg√©s :")
for k, v in PARAMS.items():
    print(f"  {k}: {v}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## üìö Imports

# COMMAND ----------

import os, re, zipfile, subprocess, sys, time
from datetime import datetime
from collections import Counter
from functools import reduce

import pandas as pd

from pyspark.sql import SparkSession, DataFrame, Window, Row
from pyspark.sql import functions as F
from pyspark.sql import types as T
from pyspark.sql.types import (
    StructType, StructField, StringType, IntegerType, DoubleType, 
    BooleanType, DateType, TimestampType, LongType, FloatType, DecimalType
)

from delta.tables import DeltaTable
from py4j.protocol import Py4JJavaError

try:
    import openpyxl
except ImportError:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "openpyxl", "--quiet"])
    import openpyxl

print("‚úÖ Imports termin√©s")

# COMMAND ----------

# MAGIC %md
# MAGIC ## üõ†Ô∏è Fonctions Utilitaires

# COMMAND ----------

def parse_bool(x, default=False):
    """Parse une valeur en bool√©en"""
    if x is None:
        return default
    s = str(x).strip().lower()
    if s in ["true", "1", "yes", "y", "oui"]:
        return True
    if s in ["false", "0", "no", "n", "non"]:
        return False
    return default

def normalize_delimiter(raw) -> str:
    """Normalise le d√©limiteur (1 caract√®re)"""
    if raw is None or str(raw).strip() == "":
        return ","
    s = str(raw).strip()
    if len(s) == 1:
        return s
    raise ValueError(f"D√©limiteur '{raw}' invalide")

def parse_header_mode(x) -> tuple:
    """Parse le mode header: (use_header, first_line_only)"""
    if x is None:
        return False, False
    s = str(x).strip().upper()
    if s == "HEADER USE":
        return True, True
    if s == "FIRST LINE":
        return True, False
    return False, False

def parse_tolerance(raw, total_rows: int, default=0.0) -> float:
    """Parse tol√©rance (pourcentage ou absolu)"""
    if raw is None or str(raw).strip().lower() in ["", "nan", "n/a", "none"]:
        return default
    s = str(raw).strip().lower().replace(",", ".").replace("%", "").replace(" ", "")
    m = re.search(r"^(\d+(?:\.\d+)?)%?$", s)
    if not m:
        return default
    val = float(m.group(1))
    if "%" in str(raw):
        return val / 100.0
    if total_rows <= 0:
        return 0.0
    return val / total_rows

def deduplicate_columns(df: DataFrame) -> DataFrame:
    """Supprime colonnes dupliqu√©es (case-insensitive)"""
    seen, cols = set(), []
    for c in df.columns:
        c_lower = c.lower()
        if c_lower not in seen:
            cols.append(c)
            seen.add(c_lower)
    return df.select(*cols)

def safe_count(df: DataFrame) -> int:
    """Count s√©curis√©"""
    try:
        return df.count()
    except Exception:
        return 0

print("‚úÖ Fonctions utilitaires charg√©es")

# COMMAND ----------

# MAGIC %md
# MAGIC ## üìä Gestion des Types Spark

# COMMAND ----------

TYPE_MAPPING = {
    "STRING": StringType(),
    "INTEGER": IntegerType(),
    "INT": IntegerType(),
    "LONG": LongType(),
    "FLOAT": FloatType(),
    "DOUBLE": DoubleType(),
    "BOOLEAN": BooleanType(),
    "DATE": DateType(),
    "TIMESTAMP": TimestampType()
}

def spark_type_from_config(row):
    """Convertit d√©finition Excel en type Spark"""
    t = str(row.get("Field type", "STRING")).strip().upper()
    if t in TYPE_MAPPING:
        return TYPE_MAPPING[t]
    if t == "DECIMAL":
        prec = int(row.get("Decimal precision", 38) or 38)
        scale = int(row.get("Decimal scale", 18) or 18)
        return DecimalType(prec, scale)
    return StringType()

def build_schema_from_config(column_defs: pd.DataFrame) -> StructType:
    """Construit StructType depuis Excel"""
    fields = []
    for _, row in column_defs.iterrows():
        field_name = row["Column Name"]
        field_type = spark_type_from_config(row)
        is_nullable = parse_bool(row.get("Is Nullable", True), True)
        fields.append(StructField(field_name, field_type, is_nullable))
    return StructType(fields)

print("‚úÖ Types Spark configur√©s")

# COMMAND ----------

# MAGIC %md
# MAGIC ## üìù Syst√®me de Logging

# COMMAND ----------

def log_execution(
    table_name: str, filename: str, input_format: str, ingestion_mode: str,
    output_zone: str, row_count: int = 0, column_count: int = 0, masking_applied: bool = False,
    error_count: int = 0, error_msg: str = None, status: str = "SUCCESS",
    start_time: float = None, env: str = None, log_path: str = None
):
    """Enregistre un log d'ex√©cution"""
    if env is None:
        env = PARAMS["env"]
    if log_path is None:
        log_path = PARAMS["log_exec_path"]
    
    today = datetime.today()
    duration = round(time.time() - start_time, 2) if start_time else None
    
    schema = StructType([
        StructField("table_name", StringType(), True),
        StructField("filename", StringType(), True),
        StructField("input_format", StringType(), True),
        StructField("ingestion_mode", StringType(), True),
        StructField("output_zone", StringType(), True),
        StructField("row_count", IntegerType(), True),
        StructField("column_count", IntegerType(), True),
        StructField("masking_applied", BooleanType(), True),
        StructField("error_count", IntegerType(), True),
        StructField("error_message", StringType(), True),
        StructField("status", StringType(), True),
        StructField("duration", DoubleType(), True),
        StructField("env", StringType(), True),
        StructField("log_ts", TimestampType(), True),
        StructField("yyyy", IntegerType(), True),
        StructField("mm", IntegerType(), True),
        StructField("dd", IntegerType(), True)
    ])
    
    row = [(
        str(table_name), str(filename), str(input_format), str(ingestion_mode),
        str(output_zone), int(row_count or 0), int(column_count or 0), bool(masking_applied),
        int(error_count or 0), str(error_msg or ""), str(status),
        float(duration or 0), str(env), datetime.now(),
        today.year, today.month, today.day
    )]
    
    df_log = spark.createDataFrame(row, schema=schema)
    
    try:
        dbutils.fs.ls(log_path)
    except Exception:
        dbutils.fs.mkdirs(log_path)
    
    df_log.write.format("delta").mode("append") \
        .option("mergeSchema", "true").partitionBy("yyyy", "mm", "dd").save(log_path)

def write_quality_errors(df_errors: DataFrame, table_name: str, zone: str = "internal",
                         base_path: str = None, env: str = None):
    """Enregistre erreurs qualit√©"""
    if base_path is None:
        base_path = PARAMS["log_quality_path"]
    if env is None:
        env = PARAMS["env"]
    
    if df_errors is None or df_errors.rdd.isEmpty():
        return
    
    today = datetime.today()
    df_errors = deduplicate_columns(df_errors)
    
    if "raw_value" in df_errors.columns:
        df_errors = df_errors.withColumn("raw_value", F.col("raw_value").cast("string"))
    else:
        df_errors = df_errors.withColumn("raw_value", F.lit(None).cast("string"))
    
    df_log = (
        df_errors
        .withColumn("table_name", F.coalesce(F.col("table_name"), F.lit(table_name)))
        .withColumn("Zone", F.lit(zone))
        .withColumn("Env", F.lit(env))
        .withColumn("log_ts", F.lit(datetime.now()))
        .withColumn("yyyy", F.lit(today.year))
        .withColumn("mm", F.lit(today.month))
        .withColumn("dd", F.lit(today.day))
    )
    
    try:
        dbutils.fs.ls(base_path)
    except Exception:
        dbutils.fs.mkdirs(base_path)
    
    try:
        spark.read.format("delta").load(base_path)
    except Exception:
        df_log.write.format("delta").mode("overwrite") \
            .partitionBy("yyyy", "mm", "dd").save(base_path)
        print(f"‚úÖ Table Delta cr√©√©e : {base_path}")
        return
    
    df_log.write.format("delta").mode("append") \
        .option("mergeSchema", "true").save(base_path)

print("‚úÖ Logging configur√©")

# COMMAND ----------

# MAGIC %md
# MAGIC ## üìÅ Gestion Fichiers

# COMMAND ----------

def extract_parts_from_filename(fname: str) -> dict:
    """Extrait yyyy/mm/dd du nom de fichier"""
    base = os.path.basename(fname)
    m = re.search(r"(?P<yyyy>\d{4})[-_]?(?P<mm>\d{2})[-_]?(?P<dd>\d{2})", base)
    if m:
        parts = {}
        if m.group("yyyy"):
            parts["yyyy"] = int(m.group("yyyy"))
        if m.group("mm"):
            parts["mm"] = int(m.group("mm"))
        if m.group("dd"):
            parts["dd"] = int(m.group("dd"))
        return parts
    return {}

def validate_filename(fname: str, source_table: str, matched_uri: str, log_quality_path: str) -> bool:
    """Valide la date dans le nom de fichier"""
    base = os.path.basename(fname)
    print(f"üîç Validation : {base}")
    
    error_schema = StructType([
        StructField("table_name", StringType(), True),
        StructField("filename", StringType(), True),
        StructField("column_name", StringType(), True),
        StructField("line_id", IntegerType(), True),
        StructField("invalid_value", StringType(), True),
        StructField("error_message", StringType(), True),
        StructField("uri", StringType(), True),
    ])
    
    parts = extract_parts_from_filename(base)
    if not parts:
        print(f"‚ùå Rejet√© : {base} (pas de date)")
        err_data = [(source_table, base, "filename", None, None,
                     "Missing date pattern", matched_uri)]
        err_df = spark.createDataFrame(err_data, error_schema)
        try:
            dbutils.fs.ls(log_quality_path)
        except Exception:
            dbutils.fs.mkdirs(log_quality_path)
        err_df.write.format("delta").mode("append").save(log_quality_path)
        return False
    
    try:
        yyyy = parts.get("yyyy")
        mm = parts.get("mm")
        dd = parts.get("dd", 1)
        
        if not yyyy or not mm:
            raise ValueError("Missing year or month")
        if not (1900 <= yyyy <= 2100):
            raise ValueError(f"Year {yyyy} out of range")
        if not (1 <= mm <= 12):
            raise ValueError(f"Month {mm} invalid (1-12)")
        if not (1 <= dd <= 31):
            raise ValueError(f"Day {dd} invalid (1-31)")
        
        datetime(yyyy, mm, dd)
        print(f"‚úÖ Accept√© : {base} ({yyyy}-{mm:02d}-{dd:02d})")
        return True
        
    except (ValueError, TypeError) as e:
        print(f"‚ùå Rejet√© : {base} ({e})")
        date_str = f"{yyyy}-{mm:02d}-{dd:02d}" if yyyy and mm else "N/A"
        err_data = [(source_table, base, "filename", None, date_str,
                     f"Invalid date: {e}", matched_uri)]
        err_df = spark.createDataFrame(err_data, error_schema)
        try:
            dbutils.fs.ls(log_quality_path)
        except Exception:
            dbutils.fs.mkdirs(log_quality_path)
        err_df.write.format("delta").mode("append").save(log_quality_path)
        return False

def build_regex_pattern(filename_pattern: str) -> tuple:
    """Construit patterns regex depuis Excel"""
    replacements = [
        ("<yyyy>", r"\d{4}"),
        ("<mm>", r"\d{2}"),
        ("<dd>", r"\d{2}"),
        ("<hhmmss>", r"\d{6}")
    ]
    
    rx_with_time = filename_pattern
    for placeholder, regex in replacements:
        rx_with_time = rx_with_time.replace(placeholder, regex)
    rx_with_time = rx_with_time.replace(".", r"\.")
    rx_with_time = f"^{rx_with_time}$"
    
    rx_without_time = filename_pattern
    for placeholder, regex in replacements[:-1]:
        rx_without_time = rx_without_time.replace(placeholder, regex)
    rx_without_time = rx_without_time.replace("_<hhmmss>", "").replace("<hhmmss>", "")
    rx_without_time = rx_without_time.replace(".", r"\.")
    rx_without_time = f"^{rx_without_time}$"
    
    return rx_with_time, rx_without_time

print("‚úÖ Gestion fichiers configur√©e")

# COMMAND ----------

# MAGIC %md
# MAGIC ## üíæ Sauvegarde Delta

# COMMAND ----------

def build_output_path(env: str, zone: str, table_name: str, version: str, parts: dict = None) -> str:
    """Construit chemin Delta"""
    return f"/mnt/wax/{env}/{zone}/{version}/{table_name}"

def save_delta(df: DataFrame, path: str, mode: str = "append", add_ts: bool = False,
               parts: dict = None, file_name_received: str = None):
    """Sauvegarde DataFrame en Delta"""
    today = datetime.today()
    y = int((parts or {}).get("yyyy", today.year))
    m = int((parts or {}).get("mm", today.month))
    d = int((parts or {}).get("dd", today.day))
    
    if add_ts:
        df = df.withColumn("FILE_PROCESS_DATE", F.current_timestamp())
    
    if file_name_received:
        base_name = os.path.splitext(os.path.basename(file_name_received))[0]
        df = df.withColumn("FILE_NAME_RECEIVED", F.lit(base_name))
    
    ordered_cols = []
    for meta_col in ["FILE_NAME_RECEIVED", "FILE_PROCESS_DATE"]:
        if meta_col in df.columns:
            ordered_cols.append(meta_col)
    other_cols = [c for c in df.columns if c not in ordered_cols]
    df = df.select(ordered_cols + other_cols)
    
    df = deduplicate_columns(df)
    
    if DeltaTable.isDeltaTable(spark, path):
        schema = spark.read.format("delta").load(path).schema
        type_map = {f.name: f.dataType.simpleString() for f in schema.fields}
        yyyy_type = type_map.get("yyyy", "int")
        mm_type = type_map.get("mm", "int")
        dd_type = type_map.get("dd", "int")
    else:
        yyyy_type, mm_type, dd_type = "int", "int", "int"
    
    df = (df.withColumn("yyyy", F.lit(y).cast(yyyy_type))
           .withColumn("mm", F.lit(m).cast(mm_type))
           .withColumn("dd", F.lit(d).cast(dd_type)))
    
    row_count = df.count()
    if row_count > 1_000_000:
        num_partitions = max(1, row_count // 1_000_000)
        df = df.repartition(num_partitions, "yyyy", "mm", "dd")
    
    df.write.format("delta").option("mergeSchema", "true").mode(mode) \
        .partitionBy("yyyy", "mm", "dd").save(path)
    
    print(f"‚úÖ Delta sauvegard√© : {path} (mode={mode}, {y}-{m:02d}-{d:02d}, {row_count} lignes)")

def register_table_in_metastore(spark, table_name: str, path: str,
                                database: str = "wax_obs", if_exists: str = "ignore"):
    """Enregistre table Delta dans metastore"""
    full_name = f"{database}.{table_name}"
    exists = any(t.name == table_name for t in spark.catalog.listTables(database))
    
    if exists and if_exists == "ignore":
        print(f"‚ö†Ô∏è Table {full_name} existe d√©j√†")
        return
    elif exists and if_exists == "errorexists":
        raise Exception(f"‚ùå Table {full_name} existe d√©j√†")
    elif exists and if_exists == "overwrite":
        print(f"‚ôªÔ∏è Table {full_name} existe ‚Üí DROP + CREATE")
        spark.sql(f"DROP TABLE IF EXISTS {full_name}")
    elif exists and if_exists == "append":
        print(f"‚ûï Table {full_name} existe ‚Üí append")
        return
    
    spark.sql(f"""
        CREATE TABLE IF NOT EXISTS {full_name}
        USING DELTA
        LOCATION '{path}'
    """)
    print(f"‚úÖ Table {full_name} enregistr√©e")

print("‚úÖ Delta configur√©")

# COMMAND ----------

# MAGIC %md
# MAGIC ## üîç Validation Qualit√©

# COMMAND ----------

# Sch√©ma unifi√© pour les erreurs
ERROR_SCHEMA = StructType([
    StructField("table_name", StringType(), True),
    StructField("filename", StringType(), True),
    StructField("line_id", IntegerType(), True),
    StructField("column_name", StringType(), True),
    StructField("error_message", StringType(), True),
    StructField("raw_value", StringType(), True),
    StructField("error_count", IntegerType(), True)
])

def check_data_quality(df: DataFrame, table_name: str, merge_keys: list,
                       filename: str = None, column_defs: pd.DataFrame = None) -> DataFrame:
    """V√©rifie qualit√© (nulls, doublons)"""
    if "line_id" not in df.columns:
        df = df.withColumn("line_id", F.row_number().over(Window.orderBy(F.monotonically_increasing_id())))
    
    errors_df = spark.createDataFrame([], ERROR_SCHEMA)
    
    data_columns = [c for c in df.columns if c not in 
                    ["line_id", "yyyy", "mm", "dd", "FILE_PROCESS_DATE", "FILE_NAME_RECEIVED"]]
    
    if not data_columns:
        return errors_df
    
    all_null = all(df.filter(F.col(c).isNotNull()).count() == 0 for c in data_columns)
    if all_null:
        return spark.createDataFrame(
            [(table_name, filename, None, "ALL_COLUMNS", "FILE_EMPTY", None, 1)],
            ERROR_SCHEMA
        )
    
    # Cl√©s nulles
    for key in merge_keys or []:
        if key in df.columns:
            null_key_count = df.filter(F.col(key).isNull()).count()
            if null_key_count > 0:
                errs = spark.createDataFrame(
                    [(table_name, filename, None, key, "NULL_KEY", None, null_key_count)],
                    ERROR_SCHEMA
                )
                errors_df = errors_df.union(errs)
    
    # Doublons
    if merge_keys:
        valid_keys = [k for k in merge_keys if k in df.columns]
        if valid_keys:
            dup_df = (
                df.groupBy(*valid_keys).count().filter(F.col("count") > 1)
                  .select(
                      F.lit(table_name).alias("table_name"),
                      F.lit(filename).alias("filename"),
                      F.lit(None).cast("int").alias("line_id"),
                      F.lit(','.join(valid_keys)).alias("column_name"),
                      F.lit("DUPLICATE_KEY").alias("error_message"),
                      F.lit(None).cast("string").alias("raw_value"),
                      F.col("count").alias("error_count")
                  )
            )
            errors_df = errors_df.union(dup_df)
    
    # Nullabilit√©
    if column_defs is not None:
        subset = column_defs[column_defs["Delta Table Name"] == table_name]
        total_rows = df.count()
        
        for idx, crow in subset.iterrows():
            cname = crow["Column Name"]
            is_nullable = parse_bool(crow.get("Is Nullable", "true"), True)
            
            if cname not in df.columns:
                continue
            
            non_null_count = df.filter(F.col(cname).isNotNull()).count()
            
            if non_null_count == 0 and not is_nullable:
                errs = spark.createDataFrame(
                    [(table_name, filename, None, cname, "COLUMN_ALL_NULL", None, total_rows)],
                    ERROR_SCHEMA
                )
                errors_df = errors_df.union(errs)
            
            elif non_null_count > 0 and not is_nullable:
                null_count = df.filter(F.col(cname).isNull()).count()
                if null_count > 0:
                    null_sample = (
                        df.filter(F.col(cname).isNull())
                          .select(
                              F.lit(table_name).alias("table_name"),
                              F.lit(filename).alias("filename"),
                              F.col("line_id"),
                              F.lit(cname).alias("column_name"),
                              F.lit("NULL_VALUE").alias("error_message"),
                              F.lit(None).cast("string").alias("raw_value"),
                              F.lit(1).alias("error_count")
                          )
                          .limit(1000)
                    )
                    errors_df = errors_df.union(null_sample)
    
    return errors_df

def parse_date_with_logs(df: DataFrame, cname: str, patterns: list,
                         table_name: str, filename: str, default_date=None) -> tuple:
    """Parse dates avec logs"""
    raw_col = F.col(cname)
    col_expr = F.when(F.length(F.trim(raw_col)) == 0, F.lit(None)).otherwise(raw_col)
    
    ts_col = None
    for p in patterns:
        cand = F.expr(f"try_to_timestamp({cname}, '{p}')")
        ts_col = cand if ts_col is None else F.coalesce(ts_col, cand)
    
    parsed = F.to_date(ts_col)
    parsed_with_default = F.when(parsed.isNull(), F.lit(default_date)).otherwise(parsed)
    df_parsed = df.withColumn(cname, parsed_with_default)
    
    errs = (
        df.withColumn("line_id", F.monotonically_increasing_id() + 1)
          .select(
              F.lit(table_name).alias("table_name"),
              F.lit(filename).alias("filename"),
              F.col("line_id"),
              F.lit(cname).alias("column_name"),
              F.when(parsed.isNull() & (F.trim(raw_col) == ""), F.lit("EMPTY_DATE"))
               .when(parsed.isNull() & (F.trim(raw_col) != ""), F.lit("INVALID_DATE"))
               .otherwise(F.lit(None)).alias("error_message"),
              raw_col.cast("string").alias("raw_value"),
              F.lit(1).alias("error_count")
          )
          .where(F.col("error_message").isNotNull())
          .limit(1000)
    )
    
    return df_parsed, errs

print("‚úÖ Validation qualit√© configur√©e")

# COMMAND ----------

# MAGIC %md
# MAGIC ## üîÑ Modes d'Ingestion

# COMMAND ----------

def apply_ingestion_mode(df_raw: DataFrame, column_defs: pd.DataFrame, table_name: str,
                         ingestion_mode: str, env: str = None, zone: str = "internal",
                         version: str = None, parts: dict = None, FILE_NAME_RECEIVED: str = None):
    """Applique mode d'ingestion"""
    if env is None:
        env = PARAMS["env"]
    if version is None:
        version = PARAMS["version"]
    
    path_all = build_output_path(env, zone, f"{table_name}_all", version, parts)
    path_last = build_output_path(env, zone, f"{table_name}_last", version, parts)
    
    specials = column_defs.copy()
    specials["Is Special lower"] = specials["Is Special"].astype(str).str.lower()
    merge_keys = specials[specials["Is Special lower"] == "ismergekey"]["Column Name"].tolist()
    update_cols = specials[specials["Is Special lower"] == "isstartvalidity"]["Column Name"].tolist()
    update_col = update_cols[0] if update_cols else None
    
    imode = (ingestion_mode or "").strip().upper()
    print(f"üîÑ Mode : {imode}")
    
    save_delta(df_raw, path_all, mode="append", add_ts=True, parts=parts, file_name_received=FILE_NAME_RECEIVED)
    register_table_in_metastore(spark, f"{table_name}_all", path_all, if_exists="ignore")
    
    if imode == "FULL_SNAPSHOT":
        save_delta(df_raw, path_last, mode="overwrite", parts=parts, file_name_received=FILE_NAME_RECEIVED)
        register_table_in_metastore(spark, f"{table_name}_last", path_last, if_exists="overwrite")
    
    elif imode == "DELTA_FROM_FLOW":
        save_delta(df_raw, path_last, mode="append", add_ts=True, parts=parts, file_name_received=FILE_NAME_RECEIVED)
        register_table_in_metastore(spark, f"{table_name}_last", path_last, if_exists="append")
    
    elif imode == "DELTA_FROM_NON_HISTORIZED":
        if not merge_keys:
            error_msg = f"‚ùå No merge keys for {table_name}"
            print(error_msg)
            raise ValueError(error_msg)
        
        fallback_col = "FILE_PROCESS_DATE"
        compare_col = update_col if update_col else fallback_col
        print(f"üìÖ Comparaison temporelle : {compare_col}")
        
        auto_cols = ["FILE_PROCESS_DATE", "yyyy", "mm", "dd"]
        
        if compare_col in df_raw.columns:
            compare_dtype = str(df_raw.schema[compare_col].dataType)
            if compare_dtype == "StringType":
                df_raw = df_raw.withColumn(compare_col, F.to_timestamp(compare_col))
        
        df_raw = (
            df_raw
            .withColumn("FILE_PROCESS_DATE", F.current_timestamp())
            .withColumn("yyyy", F.lit(parts.get("yyyy", datetime.today().year)).cast("int"))
            .withColumn("mm", F.lit(parts.get("mm", datetime.today().month)).cast("int"))
            .withColumn("dd", F.lit(parts.get("dd", datetime.today().day)).cast("int"))
        )
        
        updates = df_raw.alias("updates")
        
        if DeltaTable.isDeltaTable(spark, path_last):
            target = DeltaTable.forPath(spark, path_last)
            target_cols = [f.name for f in target.toDF().schema.fields]
        else:
            target_cols = df_raw.columns
        
        update_cols_clean = [c for c in df_raw.columns 
                            if c in target_cols and c not in merge_keys and c not in auto_cols]
        insert_cols_clean = [c for c in df_raw.columns 
                            if c in target_cols and c not in auto_cols]
        
        update_expr = {c: f"updates.{c}" for c in update_cols_clean}
        insert_expr = {c: f"updates.{c}" for c in insert_cols_clean}
        
        cond = " AND ".join([f"target.{k}=updates.{k}" for k in merge_keys])
        
        if DeltaTable.isDeltaTable(spark, path_last):
            (target.alias("target")
             .merge(updates, cond)
             .whenMatchedUpdate(condition=f"updates.{compare_col} > target.{compare_col}", set=update_expr)
             .whenNotMatchedInsert(values=insert_expr)
             .execute())
            print(f"‚úÖ Merge sur {compare_col} avec cl√©s {merge_keys}")
            register_table_in_metastore(spark, f"{table_name}_last", path_last, if_exists="append")
        else:
            print(f"üìù Cr√©ation table Delta")
            save_delta(df_raw, path_last, mode="overwrite", add_ts=True, parts=parts, file_name_received=FILE_NAME_RECEIVED)
            register_table_in_metastore(spark, f"{table_name}_last", path_last, if_exists="overwrite")
    
    elif imode == "DELTA_FROM_HISTORIZED":
        save_delta(df_raw, path_last, mode="append", add_ts=True, parts=parts, file_name_received=FILE_NAME_RECEIVED)
        register_table_in_metastore(spark, f"{table_name}_last", path_last, if_exists="append")
    
    elif imode == "FULL_KEY_REPLACE":
        if not merge_keys:
            error_msg = f"‚ùå No merge keys for {table_name} in FULL_KEY_REPLACE"
            print(error_msg)
            raise ValueError(error_msg)
        
        if DeltaTable.isDeltaTable(spark, path_last):
            target = DeltaTable.forPath(spark, path_last)
            conditions = []
            for k in merge_keys:
                values = df_raw.select(k).distinct().rdd.flatMap(lambda x: x).collect()
                values_str = ','.join([f"'{str(x)}'" for x in values])
                conditions.append(f"{k} IN ({values_str})")
            cond = " OR ".join(conditions)
            
            print(f"üóëÔ∏è Suppression sur cl√©s = {merge_keys}")
            target.delete(condition=cond)
            
            print(f"‚ûï Insertion nouvelles lignes")
            save_delta(df_raw, path_last, mode="append", add_ts=True, parts=parts, file_name_received=FILE_NAME_RECEIVED)
            register_table_in_metastore(spark, f"{table_name}_last", path_last, if_exists="append")
        else:
            print(f"‚ö†Ô∏è Table inexistante ‚Üí cr√©ation")
            save_delta(df_raw, path_last, mode="overwrite", add_ts=True, parts=parts, file_name_received=FILE_NAME_RECEIVED)
            register_table_in_metastore(spark, f"{table_name}_last", path_last, if_exists="overwrite")
    else:
        print(f"‚ö†Ô∏è Mode inconnu : {imode} ‚Üí fallback append")
        save_delta(df_raw, path_last, mode="append", add_ts=True, parts=parts, file_name_received=FILE_NAME_RECEIVED)
        register_table_in_metastore(spark, f"{table_name}_last", path_last, if_exists="append")

print("‚úÖ Modes d'ingestion configur√©s")

# COMMAND ----------

# MAGIC %md
# MAGIC ## üìä Rapport Qualit√©

# COMMAND ----------

def print_summary(table_name: str, filename: str, total_rows: tuple, corrupt_rows: int,
                  anomalies_total: int, cleaned_rows: int, errors_df: DataFrame):
    """Affiche r√©sum√© qualit√©"""
    print("\n" + "=" * 80)
    print(f"üìä Rapport | Table={table_name}, File={filename}")
    
    if isinstance(total_rows, tuple):
        print(f"Lignes: {total_rows[0]} ‚Üí {total_rows[1]}, rejet√©es: {corrupt_rows}, "
              f"anomalies: {anomalies_total}, nettoy√©es: {cleaned_rows}")
    else:
        print(f"Lignes: {total_rows}, rejet√©es: {corrupt_rows}, "
              f"anomalies: {anomalies_total}, nettoy√©es: {cleaned_rows}")
    
    print("=" * 80)
    
    if errors_df is not None and not errors_df.rdd.isEmpty():
        print("‚ö†Ô∏è Probl√®mes de qualit√© d√©tect√©s")
        
        error_summary = (
            errors_df
            .groupBy("error_message")
            .agg(F.sum("error_count").alias("total_count"))
            .orderBy(F.desc("total_count"))
            .limit(50)
            .collect()
        )
        
        null_counter = {}
        error_counter = {}
        
        for row in error_summary:
            em = row["error_message"]
            ec = row["total_count"]
            
            if "null" in str(em).lower():
                null_counter[em] = ec
            else:
                error_counter[em] = ec
        
        if error_counter:
            print("\nüî¥ Erreurs de typage/format :")
            for em, total in sorted(error_counter.items(), key=lambda x: x[1], reverse=True):
                print(f"  - {em}: {total} erreurs")
        
        if null_counter:
            print("\n‚ö™ Valeurs nulles :")
            for em, total in sorted(null_counter.items(), key=lambda x: x[1], reverse=True):
                print(f"  - {em}: {total} cas")
    else:
        print("\n‚úÖ Aucun probl√®me de qualit√©")
    
    print("=" * 80 + "\n")

print("‚úÖ Rapport configur√©")

# COMMAND ----------

# MAGIC %md
# MAGIC ## üöÄ Extraction ZIP et Config Excel

# COMMAND ----------

print("üì¶ Extraction ZIP...")
dbutils.fs.mkdirs(PARAMS["extract_dir"])

extract_dir_local = PARAMS["extract_dir"].replace("dbfs:", "/dbfs")
os.makedirs(extract_dir_local, exist_ok=True)

with zipfile.ZipFile(PARAMS["zip_path"].replace("dbfs:", "/dbfs"), 'r') as zip_ref:
    zip_ref.extractall(extract_dir_local)

print("‚úÖ ZIP extrait")

excel_path = PARAMS["excel_path"]
excel_path_local = excel_path.replace("dbfs:", "/dbfs") if excel_path.startswith("dbfs:") else excel_path

print(f"üìë Lecture Excel : {excel_path}")
file_columns_df = pd.read_excel(excel_path_local, sheet_name="Field-Column")
file_tables_df = pd.read_excel(excel_path_local, sheet_name="File-Table")

print(f"‚úÖ Config charg√©e : {len(file_tables_df)} tables, {len(file_columns_df)} colonnes")

# COMMAND ----------

# MAGIC %md
# MAGIC ## üîÑ Pipeline Principal

# COMMAND ----------

DATE_PATTERNS = [
    "dd/MM/yyyy HH:mm:ss", "dd/MM/yyyy", "yyyy-MM-dd HH:mm:ss",
    "yyyy-MM-dd'T'HH:mm:ss", "yyyyMMddHHmmss", "yyyyMMdd"
]

for table_idx, trow in file_tables_df.iterrows():
    start_table_time = time.time()
    
    source_table = trow["Delta Table Name"]
    filename_pattern = str(trow.get("Filename Pattern", "")).strip()
    input_format = str(trow.get("Input Format", "csv")).strip().lower()
    output_zone = str(trow.get("Output Zone", "internal")).strip().lower()
    ingestion_mode = str(trow.get("Ingestion mode", "")).strip()
    
    print(f"\n{'='*80}\nüìã Table {table_idx+1}/{len(file_tables_df)}: {source_table}\n{'='*80}")
    
    trim_flag = parse_bool(trow.get("Trim", True), True)
    delimiter_raw = str(trow.get("Input delimiter", ","))
    del_cols_allowed = parse_bool(trow.get("Delete Columns Allowed", False), False)
    ignore_empty = parse_bool(trow.get("Ignore empty Files", True), True)
    merge_files_flag = parse_bool(trow.get("Merge concomitant file", False), False)
    charset = str(trow.get("Input charset", "UTF-8")).strip()
    if charset.lower() in ["nan", "", "none"]:
        charset = "UTF-8"
    
    invalid_gen = parse_bool(trow.get("Invalid Lines Generate", False), False)
    
    try:
        rx_with_time, rx_without_time = build_regex_pattern(filename_pattern)
    except Exception as e:
        print(f"‚ùå Erreur pattern : {e}")
        log_execution(source_table, "N/A", input_format, ingestion_mode, output_zone,
                     error_msg=f"Regex error: {e}", status="FAILED", start_time=start_table_time)
        continue
    
    try:
        all_files = dbutils.fs.ls(PARAMS["extract_dir"])
    except Exception as e:
        print(f"‚ùå Erreur listage : {e}")
        log_execution(source_table, "N/A", input_format, ingestion_mode, output_zone,
                     error_msg=f"Directory error: {e}", status="FAILED", start_time=start_table_time)
        continue
    
    matched = [fi for fi in all_files 
               if re.match(rx_with_time, fi.name) or re.match(rx_without_time, fi.name)]
    
    if len(matched) == 0:
        print(f"‚ö†Ô∏è Aucun fichier pour : {filename_pattern}")
        log_execution(source_table, "N/A", input_format, ingestion_mode, output_zone,
                     row_count=0, column_count=0, masking_applied=(output_zone=="secret"),
                     error_msg=f"No file matching {filename_pattern}", status="FAILED", start_time=start_table_time)
        continue
    
    print(f"‚úÖ {len(matched)} fichier(s) trouv√©(s)")
    
    files_to_read = []
    for fi in matched:
        parts = extract_parts_from_filename(fi.name)
        if validate_filename(fi.name, source_table, fi.path, PARAMS["log_quality_path"]):
            files_to_read.append((fi.path, parts))
    
    if not files_to_read:
        print(f"‚ö†Ô∏è Tous les fichiers rejet√©s")
        log_execution(source_table, "N/A", input_format, ingestion_mode, output_zone,
                     row_count=0, column_count=0, masking_applied=(output_zone=="secret"),
                     error_msg="All files rejected", status="FAILED", start_time=start_table_time)
        continue
    
    if not merge_files_flag:
        files_to_read = [files_to_read[0]]
    
    print(f"üìÇ Traitement de {len(files_to_read)} fichier(s)")
    
    try:
        sep_char = normalize_delimiter(delimiter_raw)
    except Exception as e:
        log_execution(source_table, "N/A", input_format, ingestion_mode, output_zone,
                     row_count=0, column_count=0, masking_applied=(output_zone=="secret"),
                     error_msg=f"Delimiter error: {e}", status="FAILED", start_time=start_table_time)
        continue
    
    bad_records = None
    if invalid_gen:
        bad_records = f"/mnt/logs/badrecords/{PARAMS['env']}/{source_table}"
        try:
            dbutils.fs.mkdirs(bad_records)
        except Exception:
            pass
    
    header_mode = str(trow.get("Input header", ""))
    user_header, first_line_only = parse_header_mode(header_mode)
    
    expected_cols = file_columns_df[file_columns_df["Delta Table Name"] == source_table]["Column Name"].tolist()
    
    imposed_schema = None
    try:
        subset = file_columns_df[file_columns_df["Delta Table Name"] == source_table].copy()
        if not subset.empty and "Field Order" in subset.columns:
            subset = subset.sort_values(by=["Field Order"])
            imposed_schema = build_schema_from_config(subset)
    except Exception as e:
        print(f"‚ö†Ô∏è Sch√©ma impos√© impossible : {e}")
    
    df_raw_list = []
    
    for file_idx, (matched_uri, parts) in enumerate(files_to_read):
        print(f"\nüìÑ Fichier {file_idx+1}/{len(files_to_read)} : {os.path.basename(matched_uri)}")
        
        reader = (spark.read
                  .option("sep", sep_char)
                  .option("header", str(user_header).lower())
                  .option("encoding", charset)
                  .option("ignoreEmptyFiles", str(ignore_empty).lower())
                  .option("mode", "PERMISSIVE")
                  .option("enforceSchema", "false")
                  .option("columnNameOfCorruptRecord", "_corrupt_record"))
        
        if bad_records:
            reader = reader.option("badRecordsPath", bad_records)
        
        if input_format in ["csv", "csv_quote", "csv_quote_ml", "csv_deprecated"]:
            if input_format in ["csv_quote", "csv_quote_ml"]:
                reader = reader.option("quote", '"').option("escape", "\\")
            if input_format == "csv_quote_ml":
                reader = reader.option("multiline", "true")
            
            if imposed_schema is not None:
                df_file = reader.schema(imposed_schema).csv(matched_uri)
            elif user_header and not first_line_only:
                df_file = reader.option("header", "true").csv(matched_uri)
            elif user_header and first_line_only:
                tmp_df = reader.option("header", "false").csv(matched_uri)
                tmp_df = tmp_df.withColumn("_rn", F.row_number().over(
                    Window.orderBy(F.monotonically_increasing_id()))
                ).filter(F.col("_rn") > 1).drop("_rn")
                
                if len(expected_cols) != len(tmp_df.columns):
                    raise Exception(f"Expected {len(expected_cols)} cols but found {len(tmp_df.columns)}")
                df_file = tmp_df.toDF(*expected_cols)
            else:
                df_file = reader.option("header", "false").csv(matched_uri)
                if len(expected_cols) == len(df_file.columns):
                    df_file = df_file.toDF(*expected_cols)
        
        elif input_format == "fixed":
            text_df = spark.read.text(matched_uri)
            pos = 1
            exprs = []
            subset_fixed = file_columns_df[
                file_columns_df["Delta Table Name"] == source_table
            ].sort_values(by=["Field Order"])
            
            for _, crow in subset_fixed.iterrows():
                cname = crow["Column Name"]
                size = int(crow.get("Column Size", 0) or 0)
                if size <= 0:
                    size = 1
                exprs.append(F.expr(f"substring(value, {pos}, {size})").alias(cname))
                pos += size
            
            df_file = text_df.select(*exprs)
        
        else:
            log_execution(source_table, os.path.basename(matched_uri), input_format,
                         ingestion_mode, output_zone, row_count=0, column_count=0,
                         masking_applied=(output_zone=="secret"),
                         error_msg=f"Unsupported format: {input_format}",
                         status="FAILED", start_time=start_table_time)
            continue
        
        df_raw = df_file
        
        if not del_cols_allowed and expected_cols:
            missing = [c for c in expected_cols if c not in df_raw.columns]
            if missing:
                print(f"‚ùå Colonnes manquantes : {missing}")
                df_missing = spark.createDataFrame(
                    [(os.path.basename(matched_uri), m, "MISSING_COLUMN") for m in missing],
                    "filename STRING, column_name STRING, error_type STRING"
                )
                write_quality_errors(df_missing, source_table, zone=output_zone)
                log_execution(source_table, os.path.basename(matched_uri), input_format,
                             ingestion_mode, output_zone, row_count=0, column_count=len(df_raw.columns),
                             masking_applied=(output_zone=="secret"),
                             error_msg=f"Missing columns: {missing}",
                             status="FAILED", start_time=start_table_time)
                continue
        
        if expected_cols:
            for c in expected_cols:
                if c not in df_raw.columns:
                    df_raw = df_raw.withColumn(c, F.lit(None).cast(StringType()))
                else:
                    df_raw = df_raw.withColumn(c, df_raw[c].cast(StringType()))
        
        total_rows = safe_count(df_raw)
        corrupt_rows = 0
        if "_corrupt_record" in df_raw.columns:
            corrupt_rows = df_raw.filter(F.col("_corrupt_record").isNotNull()).count()
        
        rej_tol = parse_tolerance(trow.get("Rejected line per file tolerance", "10%"), total_rows)
        
        if corrupt_rows > rej_tol * total_rows:
            print(f"‚ùå Trop de lignes corrompues : {corrupt_rows} > {rej_tol * total_rows}")
            log_execution(source_table, os.path.basename(matched_uri), input_format,
                         ingestion_mode, output_zone, row_count=total_rows, column_count=len(df_raw.columns),
                         masking_applied=(output_zone=="secret"),
                         error_msg=f"Too many corrupted lines",
                         status="FAILED", start_time=start_table_time)
            continue
        
        if "_corrupt_record" in df_raw.columns:
            df_raw = df_raw.drop("_corrupt_record")
        
        if trim_flag:
            for c in df_raw.columns:
                df_raw = df_raw.withColumn(c, F.trim(F.col(c)))
        
        # Typage par colonne
        invalid_flags = []
        all_column_errors = []
        
        for _, crow in file_columns_df[file_columns_df["Delta Table Name"] == source_table].iterrows():
            cname = crow["Column Name"]
            if cname not in df_raw.columns:
                continue
            
            stype = spark_type_from_config(crow)
            tr_type = str(crow.get("Transformation Type", "")).strip().lower()
            tr_patt = str(crow.get("Transformation pattern", "")).strip()
            regex_repl = str(crow.get("Regex replacement", "")).strip()
            is_nullable = parse_bool(crow.get("Is Nullable", "True"), True)
            err_action = str(crow.get("Error action", "ICT_DRIVEN")).strip().upper()
            if err_action in ["", "NAN", "NONE", "NULL"]:
                err_action = "ICT_DRIVEN"
            
            default_inv = str(crow.get("Default when invalid", "")).strip()
            
            if tr_type == "uppercase":
                df_raw = df_raw.withColumn(cname, F.upper(F.col(cname)))
            elif tr_type == "lowercase":
                df_raw = df_raw.withColumn(cname, F.lower(F.col(cname)))
            elif tr_type == "regex" and tr_patt:
                df_raw = df_raw.withColumn(
                    cname,
                    F.regexp_replace(F.col(cname), tr_patt, regex_repl if regex_repl else "")
                )
            
            if isinstance(stype, (DateType, TimestampType)):
                patterns = []
                if tr_patt:
                    patterns.append(tr_patt)
                for p in DATE_PATTERNS:
                    if p not in patterns:
                        patterns.append(p)
                
                df_raw, errs = parse_date_with_logs(
                    df_raw, cname, patterns,
                    source_table, os.path.basename(matched_uri),
                    default_date=default_inv if default_inv else None
                )
                
                if not errs.rdd.isEmpty():
                    all_column_errors.append(errs)
            
            else:
                df_raw = df_raw.withColumn(cname, F.regexp_replace(F.col(cname), ",", "."))
                
                # Sauvegarder valeur originale
                df_raw = df_raw.withColumn(f"{cname}_original", F.col(cname))
                
                # Nettoyer vides ‚Üí NULL
                df_raw = df_raw.withColumn(
                    cname,
                    F.when(
                        (F.col(cname).isNull()) | 
                        (F.trim(F.col(cname)) == ""),
                        F.lit(None)
                    ).otherwise(F.col(cname))
                )
                
                # Cast s√©curis√©
                df_raw = df_raw.withColumn(
                    f"{cname}_cast",
                    F.expr(f"try_cast({cname} as {stype.simpleString()})")
                )
                
                # D√©tection erreurs
                invalid_cond = (F.col(f"{cname}_cast").isNull()) & \
                               (F.col(f"{cname}_original").isNotNull()) & \
                               (F.trim(F.col(f"{cname}_original")) != "")
                
                invalid_count = df_raw.filter(invalid_cond).count()
                
                # Remplacement
                df_raw = df_raw.withColumn(
                    cname,
                    F.when(F.col(f"{cname}_cast").isNotNull(), F.col(f"{cname}_cast"))
                     .otherwise(F.lit(None))
                ).drop(f"{cname}_cast")
                
                if not is_nullable and invalid_count > 0:
                    tolerance = parse_tolerance(
                        crow.get("Rejected line per file tolerance", "10%"),
                        total_rows
                    )
                    
                    if err_action == "REJECT":
                        print(f"   üóëÔ∏è REJECT: {invalid_count} lignes pour {cname}")
                        
                        errs = df_raw.filter(invalid_cond).limit(1000).select(
                            F.lit(os.path.basename(matched_uri)).alias("filename"),
                            F.lit(source_table).alias("table_name"),
                            F.lit(None).cast("int").alias("line_id"),
                            F.lit(cname).alias("column_name"),
                            F.lit("REJECT").alias("error_message"),
                            F.col(f"{cname}_original").cast("string").alias("raw_value"),
                            F.lit(invalid_count).alias("error_count")
                        )
                        all_column_errors.append(errs)
                        
                        df_raw = df_raw.filter(~invalid_cond)
                    
                    elif err_action == "ICT_DRIVEN":
                        flag_col = f"{cname}_invalid"
                        df_raw = df_raw.withColumn(
                            flag_col,
                            F.when(invalid_cond, F.lit(1)).otherwise(F.lit(0))
                        )
                        invalid_flags.append(flag_col)
                        
                        if total_rows > 0 and (invalid_count / float(total_rows)) > tolerance:
                            print(f"   ‚ùå ICT_DRIVEN ABORT: {cname}")
                            errs_summary = spark.createDataFrame(
                                [(os.path.basename(matched_uri), source_table, None, cname, 
                                  "ICT_DRIVEN_ABORT", None, invalid_count)],
                                ERROR_SCHEMA
                            )
                            all_column_errors.append(errs_summary)
                            df_raw = spark.createDataFrame([], df_raw.schema)
                            break
                        
                        elif invalid_count > 0:
                            errs_detailed = df_raw.filter(invalid_cond).limit(1000).withColumn(
                                "line_id", F.monotonically_increasing_id()
                            ).select(
                                F.lit(source_table).alias("table_name"),
                                F.lit(os.path.basename(matched_uri)).alias("filename"),
                                F.col("line_id"),
                                F.lit(cname).alias("column_name"),
                                F.lit("ICT_DRIVEN").alias("error_message"),
                                F.col(f"{cname}_original").cast("string").alias("raw_value"),
                                F.lit(1).alias("error_count")
                            )
                            all_column_errors.append(errs_detailed)
                    
                    elif err_action == "LOG_ONLY":
                        print(f"   ‚ö†Ô∏è LOG_ONLY: {invalid_count} erreurs sur {cname}")
                        errs = spark.createDataFrame(
                            [(source_table, os.path.basename(matched_uri), None, cname, 
                              "LOG_ONLY", None, invalid_count)],
                            ERROR_SCHEMA
                        )
                        all_column_errors.append(errs)
                
                # Nettoyage colonne originale
                df_raw = df_raw.drop(f"{cname}_original")
        
        # Rejet ICT_DRIVEN ligne par ligne
        if invalid_flags:
            df_raw = df_raw.withColumn(
                "invalid_column_count",
                sum([F.col(c) for c in invalid_flags])
            )
            
            max_invalid_per_line = max(1, int(len(invalid_flags) * 0.1))
            
            df_raw_valid = df_raw.filter(F.col("invalid_column_count") <= max_invalid_per_line)
            df_raw_invalid = df_raw.filter(F.col("invalid_column_count") > max_invalid_per_line)
            
            if not df_raw_invalid.rdd.isEmpty():
                invalid_count = df_raw_invalid.count()
                print(f"   üóëÔ∏è {invalid_count} lignes rejet√©es")
                
                df_raw_invalid = df_raw_invalid.withColumn("line_id", F.monotonically_increasing_id())
                err_lines = df_raw_invalid.limit(1000).select(
                    F.lit(source_table).alias("table_name"),
                    F.lit(os.path.basename(matched_uri)).alias("filename"),
                    F.col("line_id"),
                    F.lit("MULTIPLE_COLUMNS").alias("column_name"),
                    F.lit("ICT_DRIVEN_LINE_REJECT").alias("error_message"),
                    F.lit(None).cast("string").alias("raw_value"),
                    F.lit(1).alias("error_count")
                )
                all_column_errors.append(err_lines)
            
            df_raw = df_raw_valid.drop("invalid_column_count", *invalid_flags)
        
        if "line_id" not in df_raw.columns:
            df_raw = df_raw.withColumn(
                "line_id",
                F.row_number().over(Window.orderBy(F.monotonically_increasing_id()))
            )
        
        if all_column_errors:
            df_col_errors = reduce(
                lambda a, b: a.union(b),
                all_column_errors
            )
            write_quality_errors(df_col_errors, source_table, zone=output_zone)
        
        df_raw_list.append(df_raw)
    
    # Fusion DataFrames
    if not df_raw_list:
        print(f"‚ö†Ô∏è Aucun DataFrame pour {source_table}")
        continue
    
    if len(df_raw_list) > 1:
