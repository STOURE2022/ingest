"""
Gestion des tables Delta Lake - Version unifiée Local/Databricks
"""

from datetime import datetime
from pathlib import Path
from pyspark.sql import DataFrame
from pyspark.sql import functions as F

class DeltaManager:
    """Gestionnaire Delta Lake (ou CSV en local)"""
    
    def __init__(self, spark, config, env):
        self.spark = spark
        self.config = config
        self.env = env
        
        # Format selon environnement
        if env.is_databricks:
            self.format = "delta"
        else:
            self.format = "csv"  # CSV en local pour éviter Hadoop
            print("ℹ️ Mode LOCAL : utilisation de CSV au lieu de Parquet/Delta")
    
    def save_delta(self, df: DataFrame, path: str, mode: str = "append",
                   add_ts: bool = False, parts: dict = None, file_name_received: str = None):
        """Sauvegarde DataFrame en Delta/CSV"""
        from utils import deduplicate_columns
        import os
        
        # Normaliser le chemin
        save_path = self.env.normalize_path(path)
        
        # Extraire dates
        today = datetime.today()
        y = int((parts or {}).get("yyyy", today.year))
        m = int((parts or {}).get("mm", today.month))
        d = int((parts or {}).get("dd", today.day))
        
        # Ajouter métadonnées
        if add_ts:
            df = df.withColumn("FILE_PROCESS_DATE", F.current_timestamp())
        
        if file_name_received:
            base_name = os.path.splitext(os.path.basename(file_name_received))[0]
            df = df.withColumn("FILE_NAME_RECEIVED", F.lit(base_name))
        
        # Réorganiser colonnes (métadonnées en premier)
        ordered_cols = []
        for meta_col in ["FILE_NAME_RECEIVED", "FILE_PROCESS_DATE"]:
            if meta_col in df.columns:
                ordered_cols.append(meta_col)
        other_cols = [c for c in df.columns if c not in ordered_cols]
        df = df.select(ordered_cols + other_cols)
        
        # Dédupliquer colonnes
        df = deduplicate_columns(df)
        
        # Ajouter colonnes de partitionnement
        df = (df.withColumn("yyyy", F.lit(y).cast("int"))
               .withColumn("mm", F.lit(m).cast("int"))
               .withColumn("dd", F.lit(d).cast("int")))
        
        # Count pour info
        row_count = df.count()
        
        # Sauvegarder selon le format
        if self.format == "delta":
            # Delta Lake (Databricks)
            if row_count > 1_000_000:
                num_partitions = max(1, row_count // 1_000_000)
                df = df.repartition(num_partitions, "yyyy", "mm", "dd")
            
            df.write.format("delta").option("mergeSchema", "true").mode(mode) \
                .partitionBy("yyyy", "mm", "dd").save(save_path)
        else:
            # CSV (Local) - Simple et sans Hadoop
            # Créer un sous-répertoire avec la date pour simuler le partitionnement
            partition_path = Path(save_path) / f"yyyy={y}/mm={m:02d}/dd={d:02d}"
            partition_path.mkdir(parents=True, exist_ok=True)
            
            # Sauvegarder en CSV
            if mode == "overwrite":
                # Supprimer les anciens fichiers
                import shutil
                if Path(save_path).exists():
                    shutil.rmtree(save_path)
                partition_path.mkdir(parents=True, exist_ok=True)
            
            # Écrire le CSV
            df.coalesce(1).write.format("csv") \
                .option("header", "true") \
                .option("encoding", "UTF-8") \
                .mode("append") \
                .save(str(partition_path))
        
        print(f"✅ {self.format.upper()} sauvegardé : {save_path}")
        print(f"   Mode: {mode}, Date: {y}-{m:02d}-{d:02d}, Lignes: {row_count}")
    
    def register_table_in_metastore(self, table_name: str, path: str,
                                    database: str = "wax_obs", if_exists: str = "ignore"):
        """Enregistre table dans metastore"""
        
        # En local, skip le metastore
        if not self.env.is_databricks:
            print(f"ℹ️ Local : metastore ignoré pour {table_name}")
            print(f"   Accès direct via : spark.read.csv('{path}', header=True)")
            return
        
        # Databricks : enregistrer dans le metastore
        full_name = f"{database}.{table_name}"
        
        try:
            tables = self.spark.catalog.listTables(database)
            exists = any(t.name == table_name for t in tables)
        except Exception:
            try:
                self.spark.sql(f"CREATE DATABASE IF NOT EXISTS {database}")
                exists = False
            except Exception as e:
                print(f"⚠️ Metastore non disponible : {e}")
                return
        
        if exists and if_exists == "ignore":
            print(f"⚠️ Table {full_name} existe déjà")
            return
        elif exists and if_exists == "errorexists":
            raise Exception(f"❌ Table {full_name} existe déjà")
        elif exists and if_exists == "overwrite":
            print(f"♻️ DROP + CREATE {full_name}")
            self.spark.sql(f"DROP TABLE IF EXISTS {full_name}")
        elif exists and if_exists == "append":
            print(f"➕ Append {full_name}")
            return
        
        normalized_path = self.env.normalize_path(path)
        self.spark.sql(f"""
            CREATE TABLE IF NOT EXISTS {full_name}
            USING DELTA
            LOCATION '{normalized_path}'
        """)
        print(f"✅ Table {full_name} enregistrée")
    
    def table_exists(self, path: str) -> bool:
        """Vérifie si une table existe"""
        normalized_path = self.env.normalize_path(path)
        return Path(normalized_path).exists()
    
    def read_table(self, path: str) -> DataFrame:
        """Lit une table Delta/CSV"""
        normalized_path = self.env.normalize_path(path)
        
        if self.format == "delta":
            return self.spark.read.format("delta").load(normalized_path)
        else:
            # CSV avec header
            return self.spark.read.format("csv") \
                .option("header", "true") \
                .option("inferSchema", "true") \
                .load(normalized_path)
