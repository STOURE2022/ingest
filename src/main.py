"""
Module main.py
--------------
Point d‚Äôentr√©e du pipeline WAX.
"""

import os
import sys
import traceback
from pyspark.sql import SparkSession
from .config import get_config, print_config
from .file_processor import extract_zip_file, load_excel_config, process_files

import os



# ============================
# ‚úÖ Configuration Windows - Hadoop
# ============================
if os.name == "nt":  # nt = Windows
    hadoop_path = "C:\\hadoop"
    os.environ["HADOOP_HOME"] = hadoop_path
    os.environ["hadoop.home.dir"] = hadoop_path
    os.environ["PATH"] += os.pathsep + os.path.join(hadoop_path, "bin")

    print(f"‚úÖ HADOOP_HOME d√©fini sur : {os.environ['HADOOP_HOME']}")
    print(f"‚úÖ PATH mis √† jour avec : {os.path.join(hadoop_path, 'bin')}")


def main(dbutils=None, spark=None):
    print("\nüöÄ Lancement du pipeline WAX...\n")

    # ‚úÖ S√©curit√© : SparkSession obligatoire
    if spark is None:
        print("‚ö†Ô∏è Avertissement : SparkSession n'√©tait pas initialis√©e, cr√©ation forc√©e...")
        spark = SparkSession.builder.appName("WAXPipeline_Fallback").getOrCreate()

    # üîß Chargement config (Databricks ou locale)
    config = get_config(dbutils)
    print_config(config)

    try:
        # üì¶ Extraction ZIP
        extract_zip_file(config["zip_path"], config["extract_dir"])

        # üìë Lecture Excel (File-Table, Field-Column)
        file_tables_df, file_columns_df = load_excel_config(config["excel_path"])

    except Exception as e:
        print(f"‚ùå Erreur pr√©paration fichiers : {e}")
        traceback.print_exc()
        sys.exit(1)

    try:
        # ‚öôÔ∏è Lancement du traitement principal
        process_files(
            spark=spark,
            file_tables_df=file_tables_df,
            file_columns_df=file_columns_df,
            params=config
        )

    except Exception as e:
        print(f"‚ùå Erreur processing tables : {e}")
        traceback.print_exc()
        sys.exit(1)

    print("\n‚úÖ Pipeline WAX termin√© avec succ√®s ‚úÖ\n")


if __name__ == "__main__":
    try:
        # =========================================================
        # ‚öôÔ∏è  ENVIRONNEMENT LOCAL / DATARICKS COMPATIBLE
        # =========================================================
        # Force Spark √† utiliser ton installation Python Windows
        os.environ["PYSPARK_PYTHON"] = "python"
        os.environ["PYSPARK_DRIVER_PYTHON"] = "python"


        # =========================================================
        # üî• INITIALISATION SPARK AVEC DELTA LAKE
        # =========================================================
        spark = (
            SparkSession.builder
            .appName("WAXPipeline")
            # Ajout du support Delta Lake
            .config("spark.jars.packages", "io.delta:delta-spark_2.12:3.2.0")
            .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
            .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
            # Optimisations utiles
            .config("spark.sql.shuffle.partitions", "4")
            .config("spark.driver.memory", "4g")
            .config("spark.executor.memory", "4g")
            .getOrCreate()
        )

        print(f"‚ú® SparkSession active ‚Äî version {spark.version}")

    except Exception as e:
        print(f"‚ö†Ô∏è Impossible d'initialiser Spark correctement : {e}")
        traceback.print_exc()
        spark = None

    # üöÄ Lancement du pipeline
    main(spark=spark)
