"""
delta_manager.py
----------------
Gestion des √©critures Delta/Parquet du pipeline WAX :
- Sauvegarde append / overwrite
- Merge Delta (upsert)
- Enregistrement Hive / Unity Catalog
Compatible Databricks et ex√©cution locale.
"""

import os
from pyspark.sql import DataFrame, SparkSession
from delta.tables import DeltaTable
from pyspark.sql.utils import AnalysisException


# ==============================================================
# 1Ô∏è‚É£ D√©tection d'environnement
# ==============================================================

def is_databricks() -> bool:
    """D√©termine si on ex√©cute sur Databricks."""
    return "DATABRICKS_RUNTIME_VERSION" in os.environ


# ==============================================================
# 2Ô∏è‚É£ Sauvegarde Delta ou Parquet (fallback automatique)
# ==============================================================

def save_delta_table(
    spark: SparkSession,
    df: DataFrame,
    params: dict,
    table_name: str,
    mode: str = "append",
    partition_cols: list = None,
):
    """
    √âcrit un DataFrame dans Delta Lake ou Parquet (mode local).
    Fallback automatique si Delta non disponible.
    """
    base_path = params.get("base_output_path") or params.get("log_exec_path", "./data/output")
    output_path = os.path.join(base_path, table_name)

    if df is None or df.rdd.isEmpty():
        print(f"‚ö†Ô∏è Aucun enregistrement √† sauvegarder pour {table_name}.")
        return

    print(f"üíæ Sauvegarde [{table_name}] ‚Üí {output_path} | mode={mode}")

    writer = df.write.mode(mode).option("mergeSchema", "true")
    if partition_cols:
        writer = writer.partitionBy(*partition_cols)

    try:
        writer.format("delta").save(output_path)
        print(f"‚úÖ Table Delta sauvegard√©e ‚Üí {output_path}")
    except Exception as e:
        print(f"‚ö†Ô∏è Delta non disponible ({e}) ‚Üí fallback Parquet.")
        writer.format("parquet").save(output_path)
        print(f"‚úÖ Table Parquet sauvegard√©e ‚Üí {output_path}")


# ==============================================================
# 3Ô∏è‚É£ Merge Delta (Upsert) avec fallback local
# ==============================================================

def merge_delta_table(spark, df_source, target_path, merge_keys):
    """
    Effectue un merge Delta (upsert) sur la table existante.
    Si Delta indisponible, ex√©cute un append local.
    """
    if df_source is None or df_source.rdd.isEmpty():
        print("‚ö†Ô∏è Aucun enregistrement √† fusionner.")
        return

    if not is_databricks():
        print("‚ÑπÔ∏è Merge ignor√© (mode local sans Delta). Append Parquet √† la place.")
        df_source.write.mode("append").format("parquet").save(target_path)
        print(f"‚úÖ Append local ‚Üí {target_path}")
        return

    try:
        if not DeltaTable.isDeltaTable(spark, target_path):
            print(f"‚öôÔ∏è Table Delta inexistante ‚Üí cr√©ation initiale : {target_path}")
            df_source.write.format("delta").mode("overwrite").save(target_path)
            return

        delta_target = DeltaTable.forPath(spark, target_path)
        condition = " AND ".join([f"t.{k}=s.{k}" for k in merge_keys])

        (
            delta_target.alias("t")
            .merge(df_source.alias("s"), condition)
            .whenMatchedUpdateAll()
            .whenNotMatchedInsertAll()
            .execute()
        )
        print(f"‚úÖ MERGE Delta termin√© sur {merge_keys} ‚Üí {target_path}")

    except AnalysisException as e:
        print(f"‚ö†Ô∏è Erreur Delta MERGE ({e}) ‚Üí fallback append Parquet.")
        df_source.write.mode("append").format("parquet").save(target_path)
        print(f"‚úÖ Append Parquet ‚Üí {target_path}")


# ==============================================================
# 4Ô∏è‚É£ Enregistrement dans Hive / Unity Catalog
# ==============================================================

def register_table_in_metastore(spark, df_or_path, params, table_name, if_exists="ignore"):
    """
    Enregistre la table dans Hive ou Unity Catalog (Databricks uniquement).
    - df_or_path : DataFrame ou chemin Delta
    - params : dictionnaire config
    """
    if not is_databricks():
        print(f"‚ÑπÔ∏è Enregistrement Hive ignor√© (mode local). Table: {table_name}")
        return

    env = params.get("env", "dev")
    catalog = params.get("catalog", f"{env}_wax_catalog")
    database = params.get("database", f"{env}_wax_db")
    full_name = f"{catalog}.{database}.{table_name}"

    print(f"üß© Enregistrement Unity Catalog ‚Üí {full_name}")

    try:
        if isinstance(df_or_path, str):
            df = spark.read.format("delta").load(df_or_path)
        else:
            df = df_or_path

        if if_exists == "overwrite":
            spark.sql(f"DROP TABLE IF EXISTS {full_name}")

        df.write.format("delta").mode("overwrite").saveAsTable(full_name)
        print(f"‚úÖ Table {full_name} enregistr√©e dans Unity Catalog.")

    except Exception as e:
        print(f"‚ö†Ô∏è Erreur d‚Äôenregistrement Hive/Unity Catalog : {e}")
