"""
delta_manager.py
----------------
Gestion des √©critures Delta/Parquet du pipeline WAX :
- Sauvegarde append / overwrite
- Merge Delta (upsert)
- Enregistrement Hive / Unity Catalog
Compatible Databricks et ex√©cution locale.
"""

import os
from pyspark.sql import DataFrame, SparkSession
from delta.tables import DeltaTable


# ==============================================================
# 1Ô∏è‚É£ D√©tection d'environnement
# ==============================================================

def is_databricks() -> bool:
    """D√©termine si on ex√©cute sur Databricks."""
    return "DATABRICKS_RUNTIME_VERSION" in os.environ


# ==============================================================
# 2Ô∏è‚É£ Sauvegarde Delta ou Parquet
# ==============================================================

def save_delta_table(
    spark: SparkSession,
    df: DataFrame,
    params: dict,
    table_name: str,
    mode: str = "append",
    partition_cols: list = None,
):
    """
    √âcrit un DataFrame dans Delta Lake ou Parquet (local).
    """
    base_path = params.get("base_output_path") or params.get("log_exec_path", "./data/output")
    output_path = os.path.join(base_path, table_name)

    if df is None or df.rdd.isEmpty():
        print(f"‚ö†Ô∏è Aucun enregistrement √† sauvegarder pour {table_name}.")
        return

    print(f"üíæ Sauvegarde [{table_name}] ‚Üí {output_path} | mode={mode}")

    writer = df.write.mode(mode).option("mergeSchema", "true")
    if partition_cols:
        writer = writer.partitionBy(*partition_cols)

    if is_databricks():
        writer.format("delta").save(output_path)
        print(f"‚úÖ Table Delta sauvegard√©e sur Databricks : {output_path}")
    else:
        writer.format("parquet").save(output_path)
        print(f"‚úÖ Table Parquet sauvegard√©e localement : {output_path}")


# ==============================================================
# 3Ô∏è‚É£ Merge Delta
# ==============================================================

def merge_delta_table(spark, df_source, target_path, merge_keys):
    """
    Effectue un merge Delta (upsert) sur la table existante.
    """
    if not is_databricks():
        print("‚ÑπÔ∏è Merge ignor√© (mode local sans Delta).")
        return

    if not DeltaTable.isDeltaTable(spark, target_path):
        print(f"‚öôÔ∏è Table Delta inexistante ‚Üí cr√©ation initiale : {target_path}")
        df_source.write.format("delta").mode("overwrite").save(target_path)
        return

    delta_target = DeltaTable.forPath(spark, target_path)
    condition = " AND ".join([f"t.{k}=s.{k}" for k in merge_keys])

    (
        delta_target.alias("t")
        .merge(df_source.alias("s"), condition)
        .whenMatchedUpdateAll()
        .whenNotMatchedInsertAll()
        .execute()
    )

    print(f"‚úÖ MERGE termin√© sur {merge_keys} ‚Üí {target_path}")


# ==============================================================
# 4Ô∏è‚É£ Enregistrement dans le m√©tastore Hive / Unity Catalog
# ==============================================================

def register_table_in_metastore(spark, df, params, table_name, if_exists="ignore"):
    """
    Enregistre la table dans Hive ou Unity Catalog (Databricks uniquement).
    """
    if not is_databricks():
        print(f"‚ÑπÔ∏è Enregistrement Hive ignor√© (mode local). Table: {table_name}")
        return

    env = params.get("env", "dev")
    catalog = params.get("catalog", f"{env}_wax_catalog")
    database = params.get("database", f"{env}_wax_db")
    full_name = f"{catalog}.{database}.{table_name}"

    print(f"üß© Enregistrement Unity Catalog ‚Üí {full_name}")

    try:
        if if_exists == "overwrite":
            spark.sql(f"DROP TABLE IF EXISTS {full_name}")
        df.write.format("delta").mode("overwrite").saveAsTable(full_name)
        print(f"‚úÖ Table {full_name} enregistr√©e avec succ√®s dans Unity Catalog.")
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur d‚Äôenregistrement Hive/Unity Catalog : {e}")
