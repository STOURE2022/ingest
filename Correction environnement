"""
DÃ©tection et gestion de l'environnement (Local vs Databricks)
"""

import os
import sys
from pathlib import Path
from typing import Optional

class Environment:
    """Gestionnaire d'environnement unifiÃ©"""
    
    def __init__(self):
        self.is_databricks = self._detect_databricks()
        self.dbutils = None
        
        if self.is_databricks:
            try:
                from pyspark.dbutils import DBUtils
                from pyspark.sql import SparkSession
                spark = SparkSession.builder.getOrCreate()
                self.dbutils = DBUtils(spark)
            except ImportError:
                self.is_databricks = False
        
        print(f"ðŸŒ Environnement dÃ©tectÃ© : {'Databricks' if self.is_databricks else 'Local'}")
    
    def _detect_databricks(self) -> bool:
        """DÃ©tecte si on est sur Databricks"""
        return (
            'DATABRICKS_RUNTIME_VERSION' in os.environ or
            'DB_HOME' in os.environ or
            '/databricks/' in sys.executable
        )
    
    def normalize_path(self, path: str) -> str:
        """Normalise les chemins selon l'environnement"""
        if not path:
            return path
        
        if self.is_databricks:
            # Databricks : garder les chemins DBFS
            if path.startswith('/dbfs/'):
                return f"dbfs:{path[5:]}"
            elif path.startswith('dbfs:/'):
                return path
            elif path.startswith('/mnt/'):
                return path
            return path
        else:
            # Local : convertir DBFS vers chemins locaux
            if path.startswith('dbfs:'):
                path = path.replace('dbfs:', './data')
            elif path.startswith('/mnt/'):
                path = path.replace('/mnt/', './data/mnt/')
            elif path.startswith('/FileStore/'):
                path = path.replace('/FileStore/', './data/FileStore/')
            
            # CrÃ©er les rÃ©pertoires si nÃ©cessaire
            path_obj = Path(path)
            if not path_obj.suffix:  # C'est un rÃ©pertoire
                path_obj.mkdir(parents=True, exist_ok=True)
            else:  # C'est un fichier
                path_obj.parent.mkdir(parents=True, exist_ok=True)
            
            return str(path_obj)
    
    def get_spark(self):
        """Obtient la session Spark"""
        from pyspark.sql import SparkSession
        
        if self.is_databricks:
            return SparkSession.builder.getOrCreate()
        else:
            # Configuration locale SIMPLIFIÃ‰E (sans Delta Lake dans config)
            import findspark
            findspark.init()
            
            # Trouver le JAR Delta Lake
            try:
                import delta
                delta_jar_path = None
                
                # Chercher le JAR dans site-packages
                import site
                for site_dir in site.getsitepackages():
                    delta_jars = list(Path(site_dir).glob("**/delta-core*.jar"))
                    if delta_jars:
                        delta_jar_path = str(delta_jars[0])
                        break
                
                if not delta_jar_path:
                    # Fallback : utiliser le package delta directement
                    print("âš ï¸ JAR Delta non trouvÃ©, utilisation du package Python")
            except ImportError:
                print("âŒ Delta Lake non installÃ©, rÃ©installer avec: pip install delta-spark==3.0.0")
                delta_jar_path = None
            
            builder = (SparkSession.builder
                      .appName("WAX-Local")
                      .master("local[*]")
                      .config("spark.driver.memory", "4g")
                      .config("spark.executor.memory", "4g")
                      .config("spark.sql.warehouse.dir", "./spark-warehouse"))
            
            # Ajouter JAR Delta si trouvÃ©
            if delta_jar_path:
                builder = builder.config("spark.jars", delta_jar_path)
            
            # CrÃ©er la session
            spark = builder.getOrCreate()
            
            # Configurer Delta Lake aprÃ¨s crÃ©ation
            try:
                from delta import configure_spark_with_delta_pip
                spark = configure_spark_with_delta_pip(spark)
                print("âœ… Delta Lake configurÃ© via pip")
            except Exception as e:
                print(f"âš ï¸ Impossible de configurer Delta via pip : {e}")
                print("   Le pipeline continuera sans Delta Lake")
            
            return spark


class FileSystemAdapter:
    """Adaptateur pour accÃ¨s fichiers (compatible dbutils)"""
    
    def __init__(self, env: Environment):
        self.env = env
    
    def ls(self, path: str):
        """Liste les fichiers"""
        normalized_path = self.env.normalize_path(path)
        
        if self.env.is_databricks and self.env.dbutils:
            return self.env.dbutils.fs.ls(path)
        else:
            # Local
            path_obj = Path(normalized_path)
            if not path_obj.exists():
                return []
            
            files = []
            for item in path_obj.iterdir():
                files.append(type('FileInfo', (), {
                    'path': str(item),
                    'name': item.name,
                    'size': item.stat().st_size if item.is_file() else 0
                })())
            return files
    
    def mkdirs(self, path: str):
        """CrÃ©e les rÃ©pertoires"""
        normalized_path = self.env.normalize_path(path)
        
        if self.env.is_databricks and self.env.dbutils:
            return self.env.dbutils.fs.mkdirs(path)
        else:
            Path(normalized_path).mkdir(parents=True, exist_ok=True)
    
    def rm(self, path: str, recurse: bool = False):
        """Supprime fichiers/rÃ©pertoires"""
        normalized_path = self.env.normalize_path(path)
        
        if self.env.is_databricks and self.env.dbutils:
            return self.env.dbutils.fs.rm(path, recurse)
        else:
            import shutil
            path_obj = Path(normalized_path)
            if path_obj.is_file():
                path_obj.unlink()
            elif path_obj.is_dir() and recurse:
                shutil.rmtree(path_obj)


class WidgetsAdapter:
    """Adaptateur pour widgets Databricks"""
    
    def __init__(self, env: Environment):
        self.env = env
        self._widgets = {}
    
    def text(self, name: str, default_value: str, label: str = None):
        """CrÃ©e un widget texte"""
        if self.env.is_databricks and self.env.dbutils:
            self.env.dbutils.widgets.text(name, default_value, label or name)
        else:
            # En local, utiliser les variables d'env ou valeurs par dÃ©faut
            self._widgets[name] = os.environ.get(f"WAX_{name.upper()}", default_value)
            print(f"ðŸ“ Widget '{name}': {self._widgets[name]}")
    
    def get(self, name: str) -> str:
        """RÃ©cupÃ¨re la valeur d'un widget"""
        if self.env.is_databricks and self.env.dbutils:
            return self.env.dbutils.widgets.get(name)
        else:
            return self._widgets.get(name, "")


class DBUtilsAdapter:
    """Adaptateur complet pour dbutils"""
    
    def __init__(self, env: Environment):
        self.env = env
        self.fs = FileSystemAdapter(env)
        self.widgets = WidgetsAdapter(env)
    
    @property
    def notebook(self):
        """Adaptateur notebook (non implÃ©mentÃ© en local)"""
        if self.env.is_databricks and self.env.dbutils:
            return self.env.dbutils.notebook
        return None


# Instance globale
_env = None
_dbutils = None

def get_environment() -> Environment:
    """Obtient l'environnement global"""
    global _env
    if _env is None:
        _env = Environment()
    return _env

def get_dbutils() -> DBUtilsAdapter:
    """Obtient l'adaptateur dbutils"""
    global _dbutils
    if _dbutils is None:
        _dbutils = DBUtilsAdapter(get_environment())
    return _dbutils
