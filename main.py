"""
main.py
--------
Point d‚Äôentr√©e du pipeline WAX modulaire.

√âtapes :
1. Chargement de la configuration
2. Initialisation Spark
3. Extraction du ZIP et lecture Excel
4. Traitement des fichiers (boucle principale)
5. G√©n√©ration des logs d‚Äôex√©cution et qualit√©
"""

import time
from pyspark.sql import SparkSession
from config import get_config
from file_processor import extract_zip_file, load_excel_config, process_files
from logging_manager import log_execution
from datetime import datetime


def init_spark(app_name="WAX_PIPELINE"):
    """
    Initialise la session Spark, compatible local / Databricks.
    """
    print("üöÄ Initialisation SparkSession ...")
    spark = (
        SparkSession.builder
        .appName(app_name)
        .config("spark.sql.shuffle.partitions", "200")
        .config("spark.sql.files.ignoreCorruptFiles", "true")
        .config("spark.sql.session.timeZone", "UTC")
        .getOrCreate()
    )

    print(f"‚úÖ Spark initialis√© : {spark.version}")
    return spark


def main():
    """
    Ex√©cution compl√®te du pipeline WAX.
    """
    start_pipeline = time.time()
    print("=" * 80)
    print("üß† WAX DATA INGESTION PIPELINE - MODE MODULAIRE")
    print("=" * 80)

    # ==============================================================
    # 1Ô∏è‚É£ Chargement des param√®tres
    # ==============================================================
    try:
        params = get_config()
        print("\n‚úÖ Param√®tres charg√©s :")
        for k, v in params.items():
            print(f" - {k}: {v}")
    except Exception as e:
        print(f"‚ùå Erreur chargement configuration : {e}")
        return

    # ==============================================================
    # 2Ô∏è‚É£ Initialisation Spark
    # ==============================================================
    spark = init_spark()

    # ==============================================================
    # 3Ô∏è‚É£ Extraction du ZIP
    # ==============================================================
    try:
        extract_dir_local = extract_zip_file(params["zip_path"], params["extract_dir"])
    except Exception as e:
        print(f"‚ùå √âchec extraction ZIP : {e}")
        log_execution(
            spark,
            params,
            table_name="N/A",
            filename=params["zip_path"],
            ingestion_mode="N/A",
            row_count=0,
            column_count=0,
            masklist_applied=False,
            error_count=1,
            error_message=str(e),
            status="FAILED",
            start_time=start_pipeline
        )
        return

    # ==============================================================
    # 4Ô∏è‚É£ Lecture Excel (config tables & colonnes)
    # ==============================================================
    try:
        file_tables_df, file_columns_df = load_excel_config(params["excel_path"])
    except Exception as e:
        print(f"‚ùå Erreur lecture Excel : {e}")
        log_execution(
            spark,
            params,
            table_name="N/A",
            filename=params["excel_path"],
            ingestion_mode="N/A",
            row_count=0,
            column_count=0,
            masklist_applied=False,
            error_count=1,
            error_message=str(e),
            status="FAILED",
            start_time=start_pipeline
        )
        return

    # ==============================================================
    # 5Ô∏è‚É£ Traitement principal
    # ==============================================================
    try:
        process_files(spark, params, file_tables_df, file_columns_df)
    except Exception as e:
        print(f"‚ùå √âchec pipeline principal : {e}")
        log_execution(
            spark,
            params,
            table_name="GLOBAL",
            filename="GLOBAL",
            ingestion_mode="N/A",
            row_count=0,
            column_count=0,
            masklist_applied=False,
            error_count=1,
            error_message=str(e),
            status="FAILED",
            start_time=start_pipeline
        )
        return

    # ==============================================================
    # 6Ô∏è‚É£ Fin du pipeline + log global
    # ==============================================================
    duration = round(time.time() - start_pipeline, 2)
    print("\n" + "=" * 80)
    print(f"‚úÖ WAX PIPELINE TERMIN√â AVEC SUCC√àS EN {duration} sec.")
    print("=" * 80)

    log_execution(
        spark,
        params,
        table_name="GLOBAL",
        filename="PIPELINE",
        ingestion_mode="FULL_RUN",
        row_count=0,
        column_count=0,
        masklist_applied=False,
        error_count=0,
        error_message="Pipeline termin√© avec succ√®s",
        status="SUCCESS",
        start_time=start_pipeline
    )


if __name__ == "__main__":
    main()
