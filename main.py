# src/main.py
# --------------------------------------------------------------------------------------
# Point d‚Äôentr√©e du pipeline WAX.
# D√©tection d‚Äôenvironnement (Databricks / Local)
# Chargement des param√®tres
# Ex√©cution du pipeline complet.
# --------------------------------------------------------------------------------------

import sys
from pyspark.sql import SparkSession
from environment import is_databricks
from config import get_databricks_config, get_local_config
from file_processor import process_files


# ======================================================================================
# 1Ô∏è‚É£ - INITIALISATION DE SPARK
# ======================================================================================
def create_spark_session(app_name: str = "WAX_Pipeline"):
    """
    Cr√©e la session Spark adapt√©e √† l‚Äôenvironnement :
    - Databricks : session existante
    - Local : session autonome avec Delta support
    """
    if is_databricks():
        print("üíª Ex√©cution d√©tect√©e sur Databricks")
        return SparkSession.getActiveSession()
    else:
        print("üè† Ex√©cution locale d√©tect√©e (VSCode ou Terminal)")
        return (
            SparkSession.builder
            .appName(app_name)
            .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
            .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
            .config("spark.driver.memory", "4g")
            .config("spark.executor.memory", "4g")
            .getOrCreate()
        )


# ======================================================================================
# 2Ô∏è‚É£ - MAIN EXECUTION
# ======================================================================================
def main():
    print("\nüöÄ Lancement du pipeline WAX ‚Äì Main Entry Point\n" + "=" * 80)

    spark = create_spark_session()

    # Chargement des param√®tres selon l‚Äôenvironnement
    if is_databricks():
        try:
            import dbutils
        except ImportError:
            from pyspark.dbutils import DBUtils
            dbutils = DBUtils(spark)

        params = get_databricks_config(dbutils)
    else:
        params = get_local_config()

    # V√©rification rapide
    print("\n‚úÖ Param√®tres charg√©s :")
    for k, v in params.items():
        print(f"  {k}: {v}")

    # Lancement du pipeline principal
    process_files(spark, params)

    print("\nüéâ Pipeline WAX termin√© avec succ√®s !")
    print("=" * 80)


# ======================================================================================
# 3Ô∏è‚É£ - ENTRY POINT
# ======================================================================================
if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"\n‚ùå Erreur inattendue dans le pipeline : {e}")
        sys.exit(1)
