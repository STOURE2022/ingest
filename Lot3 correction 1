Parfait ! Je vois deux probl√®mes :

1. ‚ùå **Erreur Delta** : `[DELTA_FAILED_TO_MERGE_FIELDS] Failed to merge fields 'line_id' and 'line_id'`
2. ‚ö†Ô∏è **Logs incomplets** : Les erreurs de type mismatch ne sont pas logu√©es avant les valeurs nulles

## üîß **Solution 1 : Corriger l'erreur Delta (line_id dupliqu√©)**

Le probl√®me vient du fait que `line_id` existe d√©j√† dans `df_err_quality` et on essaie de l'ajouter √† nouveau.

### **Remplacez la fonction `check_data_quality()` par cette version corrig√©e :**

```python
def check_data_quality(df: DataFrame, table_name: str, merge_keys: list,
                       filename: str = None, column_defs: pd.DataFrame = None) -> DataFrame:
    """V√©rifie qualit√© (nulls, doublons)"""
    
    # Supprimer line_id s'il existe d√©j√†
    if "line_id" in df.columns:
        df = df.drop("line_id")
    
    # Ajouter line_id proprement
    df = df.withColumn("line_id", F.row_number().over(Window.orderBy(F.monotonically_increasing_id())))
    
    errors_df = spark.createDataFrame([], ERROR_SCHEMA)
    
    data_columns = [c for c in df.columns if c not in 
                    ["line_id", "yyyy", "mm", "dd", "FILE_PROCESS_DATE", "FILE_NAME_RECEIVED"]]
    
    if not data_columns:
        return errors_df
    
    all_null = all(df.filter(F.col(c).isNotNull()).count() == 0 for c in data_columns)
    if all_null:
        return spark.createDataFrame(
            [(table_name, filename, None, "ALL_COLUMNS", "FILE_EMPTY", None, 1)],
            ERROR_SCHEMA
        )
    
    # Cl√©s nulles
    for key in merge_keys or []:
        if key in df.columns:
            null_key_count = df.filter(F.col(key).isNull()).count()
            if null_key_count > 0:
                errs = spark.createDataFrame(
                    [(table_name, filename, None, key, "NULL_KEY", None, null_key_count)],
                    ERROR_SCHEMA
                )
                errors_df = errors_df.union(errs)
    
    # Doublons
    if merge_keys:
        valid_keys = [k for k in merge_keys if k in df.columns]
        if valid_keys:
            dup_df = (
                df.groupBy(*valid_keys).count().filter(F.col("count") > 1)
                  .select(
                      F.lit(table_name).alias("table_name"),
                      F.lit(filename).alias("filename"),
                      F.lit(None).cast("int").alias("line_id"),
                      F.lit(','.join(valid_keys)).alias("column_name"),
                      F.lit("DUPLICATE_KEY").alias("error_message"),
                      F.lit(None).cast("string").alias("raw_value"),
                      F.col("count").alias("error_count")
                  )
            )
            errors_df = errors_df.union(dup_df)
    
    # Nullabilit√©
    if column_defs is not None:
        subset = column_defs[column_defs["Delta Table Name"] == table_name]
        total_rows = df.count()
        
        for idx, crow in subset.iterrows():
            cname = crow["Column Name"]
            is_nullable = parse_bool(crow.get("Is Nullable", "true"), True)
            
            if cname not in df.columns:
                continue
            
            non_null_count = df.filter(F.col(cname).isNotNull()).count()
            
            if non_null_count == 0 and not is_nullable:
                errs = spark.createDataFrame(
                    [(table_name, filename, None, cname, "COLUMN_ALL_NULL", None, total_rows)],
                    ERROR_SCHEMA
                )
                errors_df = errors_df.union(errs)
            
            elif non_null_count > 0 and not is_nullable:
                null_count = df.filter(F.col(cname).isNull()).count()
                if null_count > 0:
                    null_sample = (
                        df.filter(F.col(cname).isNull())
                          .select(
                              F.lit(table_name).alias("table_name"),
                              F.lit(filename).alias("filename"),
                              F.col("line_id"),
                              F.lit(cname).alias("column_name"),
                              F.lit("NULL_VALUE").alias("error_message"),
                              F.lit(None).cast("string").alias("raw_value"),
                              F.lit(1).alias("error_count")
                          )
                          .limit(1000)
                    )
                    errors_df = errors_df.union(null_sample)
    
    return errors_df
```

## üéØ **Solution 2 : Logger les erreurs de type AVANT les valeurs nulles**

Le probl√®me est que les erreurs de type sont logu√©es pendant le typage des colonnes, AVANT la validation qualit√© globale. Pour les voir en premier, il faut r√©organiser l'affichage.

### **Remplacez la section "Aper√ßu erreurs" par ce code am√©lior√© :**

```python
# =====================================================================
# APER√áU DES ERREURS AM√âLIOR√â
# =====================================================================

print("\nüìä Aper√ßu erreurs d√©tect√©es :")

if df_err_global is not None and not df_err_global.rdd.isEmpty():
    try:
        error_count = df_err_global.count()
        print(f"‚úÖ {error_count} erreur(s) d√©tect√©e(s)")
        
        # S√©parer les erreurs par type
        type_errors = df_err_global.filter(
            F.col("error_message").contains("TYPE MISMATCH") |
            F.col("error_message").contains("INVALID_DATE") |
            F.col("error_message").contains("REJECT") |
            F.col("error_message").contains("ICT_DRIVEN")
        )
        
        null_errors = df_err_global.filter(
            F.col("error_message").contains("NULL_VALUE") |
            F.col("error_message").contains("NULL_KEY") |
            F.col("error_message").contains("COLUMN_ALL_NULL")
        )
        
        other_errors = df_err_global.filter(
            ~(F.col("error_message").contains("TYPE MISMATCH") |
              F.col("error_message").contains("INVALID_DATE") |
              F.col("error_message").contains("REJECT") |
              F.col("error_message").contains("ICT_DRIVEN") |
              F.col("error_message").contains("NULL_VALUE") |
              F.col("error_message").contains("NULL_KEY") |
              F.col("error_message").contains("COLUMN_ALL_NULL"))
        )
        
        # Affichage par ordre de priorit√©
        if not type_errors.rdd.isEmpty():
            print("\nüî¥ ERREURS DE TYPAGE (TYPE MISMATCH) :")
            type_summary = (
                type_errors
                .groupBy("column_name", "error_message")
                .agg(F.sum("error_count").alias("total_count"))
                .orderBy(F.desc("total_count"))
            )
            type_summary.show(50, truncate=False)
            
            # Afficher quelques exemples avec valeurs
            print("\nüìã Exemples de valeurs invalides :")
            type_errors.filter(F.col("raw_value").isNotNull()).select(
                "column_name", "raw_value", "error_message"
            ).distinct().show(20, truncate=False)
        
        if not null_errors.rdd.isEmpty():
            print("\n‚ö™ ERREURS DE VALEURS NULLES :")
            null_summary = (
                null_errors
                .groupBy("column_name", "error_message")
                .agg(F.sum("error_count").alias("total_count"))
                .orderBy(F.desc("total_count"))
            )
            null_summary.show(50, truncate=False)
        
        if not other_errors.rdd.isEmpty():
            print("\n‚ö†Ô∏è AUTRES ERREURS :")
            other_summary = (
                other_errors
                .groupBy("column_name", "error_message")
                .agg(F.sum("error_count").alias("total_count"))
                .orderBy(F.desc("total_count"))
            )
            other_summary.show(50, truncate=False)
        
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur affichage : {e}")
        print(f"Colonnes : {df_err_global.columns}")
else:
    print("‚úÖ Aucune erreur d√©tect√©e")
```

## üöÄ **Solution 3 : Am√©liorer le r√©sum√© console avec priorit√©**

Pour avoir un r√©sum√© encore plus clair dans la console, ajoutez aussi cette am√©lioration √† la fonction `print_summary()` :

```python
def print_summary(table_name: str, filename: str, total_rows: tuple, corrupt_rows: int,
                  anomalies_total: int, cleaned_rows: int, errors_df: DataFrame):
    """Affiche r√©sum√© qualit√© avec priorit√© erreurs type"""
    print("\n" + "=" * 80)
    print(f"üìä Rapport | Table={table_name}, File={filename}")
    
    if isinstance(total_rows, tuple):
        print(f"Lignes: {total_rows[0]} ‚Üí {total_rows[1]}, rejet√©es: {corrupt_rows}, "
              f"anomalies: {anomalies_total}, nettoy√©es: {cleaned_rows}")
    else:
        print(f"Lignes: {total_rows}, rejet√©es: {corrupt_rows}, "
              f"anomalies: {anomalies_total}, nettoy√©es: {cleaned_rows}")
    
    print("=" * 80)
    
    if errors_df is not None and not errors_df.rdd.isEmpty():
        print("‚ö†Ô∏è Probl√®mes de qualit√© d√©tect√©s")
        
        # S√©parer les types d'erreurs
        type_errors = errors_df.filter(
            F.col("error_message").contains("TYPE MISMATCH") |
            F.col("error_message").contains("INVALID_DATE") |
            F.col("error_message").contains("ICT_DRIVEN")
        )
        
        null_errors = errors_df.filter(
            F.col("error_message").contains("NULL_VALUE") |
            F.col("error_message").contains("NULL_KEY")
        )
        
        # 1. Afficher d'abord les erreurs de TYPE
        if not type_errors.rdd.isEmpty():
            type_summary = (
                type_errors
                .groupBy("error_message")
                .agg(F.sum("error_count").alias("total_count"))
                .orderBy(F.desc("total_count"))
                .limit(50)
                .collect()
            )
            
            print("\nüî¥ Erreurs de typage/format :")
            for row in type_summary:
                em = row["error_message"]
                ec = row["total_count"]
                print(f"  - {em}: {ec} erreurs")
        
        # 2. Ensuite les valeurs nulles
        if not null_errors.rdd.isEmpty():
            null_summary = (
                null_errors
                .groupBy("error_message")
                .agg(F.sum("error_count").alias("total_count"))
                .orderBy(F.desc("total_count"))
                .limit(50)
                .collect()
            )
            
            print("\n‚ö™ Valeurs nulles :")
            for row in null_summary:
                em = row["error_message"]
                ec = row["total_count"]
                print(f"  - {em}: {ec} cas")
    else:
        print("\n‚úÖ Aucun probl√®me de qualit√©")
    
    print("=" * 80 + "\n")
```

## ‚úÖ **R√©sultat attendu apr√®s correction**

Avec ces modifications, vous verrez dans vos logs :

```
üìä Aper√ßu erreurs d√©tect√©es :
‚úÖ X erreur(s) d√©tect√©e(s)

üî¥ ERREURS DE TYPAGE (TYPE MISMATCH) :
+-----------+------------------------------------------------+------------+
|column_name|error_message                                   |total_count |
+-----------+------------------------------------------------+------------+
|contractid |TYPE MISMATCH: Expected INT, found: 'ABC123'    |150         |
|siteid     |TYPE MISMATCH: Expected INT, found: 'SITE_X'    |75          |
+-----------+------------------------------------------------+------------+

üìã Exemples de valeurs invalides :
+-----------+---------+------------------------------------------------+
|column_name|raw_value|error_message                                   |
+-----------+---------+------------------------------------------------+
|contractid |ABC123   |TYPE MISMATCH: Expected INT, found: 'ABC123'    |
|contractid |XYZ456   |TYPE MISMATCH: Expected INT, found: 'XYZ456'    |
|siteid     |SITE_X   |TYPE MISMATCH: Expected INT, found: 'SITE_X'    |
+-----------+---------+------------------------------------------------+

‚ö™ ERREURS DE VALEURS NULLES :
+-----------+------------------------------------------------+------------+
|column_name|error_message                                   |total_count |
+-----------+------------------------------------------------+------------+
|contractid |NULL_VALUE: Column 'contractid' does not allow..|3           |
|siteid     |NULL_VALUE: Column 'siteid' does not allow...   |2           |
+-----------+------------------------------------------------+------------+
```

Appliquez ces trois corrections et relancez votre notebook. Les erreurs de type appara√Ætront maintenant **avant** les valeurs nulles, et l'erreur Delta sur `line_id` sera r√©solue ! üéâ
