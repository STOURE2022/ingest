# WAX Pipeline - ETL Data Ingestion

## üìã Description du Projet

**WAX Pipeline** est un pipeline ETL modulaire et intelligent con√ßu pour ing√©rer des donn√©es CSV/Excel dans Delta Lake sur Databricks.

Le projet se distingue par sa **d√©tection automatique d'environnement** : le m√™me code fonctionne sans modification en local (pour les tests) et sur Databricks (en production). Il utilise l'architecture moderne **Databricks Asset Bundle** pour un d√©ploiement standardis√© et reproductible.

Le pipeline g√®re automatiquement :
- L'extraction de fichiers ZIP contenant des CSV
- La validation des donn√©es selon des r√®gles d√©finies dans Excel
- Les transformations (uppercase, lowercase, regex, dates, etc.)
- L'ingestion en mode FULL_SNAPSHOT, DELTA, ou MERGE
- La g√©n√©ration de logs de qualit√© et d'ex√©cution dans Delta Lake

---

## üìÇ Structure du Projet

```
wax_pipeline/
‚îú‚îÄ‚îÄ databricks.yml                    # Configuration principale du bundle
‚îú‚îÄ‚îÄ .databricks/project.json          # Profil de connexion Databricks
‚îú‚îÄ‚îÄ resources/                        # Configurations YAML des ressources
‚îÇ   ‚îú‚îÄ‚îÄ wax_job.yml                  # Job ETL (cluster, schedule, retry)
‚îÇ   ‚îî‚îÄ‚îÄ wax_pipeline.yml             # Delta Live Tables pipeline
‚îú‚îÄ‚îÄ notebooks/                        # Notebooks Databricks
‚îÇ   ‚îú‚îÄ‚îÄ wax_pipeline_main.py         # Point d'entr√©e principal
‚îÇ   ‚îú‚îÄ‚îÄ wax_dlt_pipeline.py          # Pipeline DLT (Bronze/Silver/Gold)
‚îÇ   ‚îî‚îÄ‚îÄ notebook_original.py         # Notebook monolithique original
‚îú‚îÄ‚îÄ src/                              # Code source Python modulaire
‚îÇ   ‚îú‚îÄ‚îÄ main.py                      # Point d'entr√©e
‚îÇ   ‚îú‚îÄ‚îÄ config.py                    # Configuration auto-d√©tect√©e
‚îÇ   ‚îú‚îÄ‚îÄ file_processor.py            # Traitement des fichiers
‚îÇ   ‚îú‚îÄ‚îÄ ingestion.py                 # Modes d'ingestion
‚îÇ   ‚îú‚îÄ‚îÄ validators.py                # Validation de donn√©es
‚îÇ   ‚îú‚îÄ‚îÄ delta_manager.py             # Gestion Delta Lake
‚îÇ   ‚îú‚îÄ‚îÄ logging_manager.py           # Logs
‚îÇ   ‚îî‚îÄ‚îÄ utils.py                     # Fonctions utilitaires
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ input/                       # Fichiers d'entr√©e (ZIP + Excel)
‚îÇ   ‚îî‚îÄ‚îÄ output/                      # R√©sultats (logs + tables)
‚îú‚îÄ‚îÄ setup.py                          # Configuration du package Python
‚îú‚îÄ‚îÄ requirements.txt                  # D√©pendances
‚îî‚îÄ‚îÄ build.sh                          # Script de build
```

---

## üöÄ Installation des d√©pendances Python

Installer PySpark, Pandas, openpyxl et les autres d√©pendances n√©cessaires au pipeline.

```bash
pip install -r requirements.txt
```

Vous devriez voir :
```
Successfully installed pyspark-3.3.0 pandas-2.0.0 openpyxl-3.1.2 delta-spark-2.3.0
```

---

## ‚òï Installation de Java 17

PySpark a besoin de Java pour fonctionner. Java 17 est la version recommand√©e.

```bash
sudo apt-get update
sudo apt-get install openjdk-17-jdk-headless
export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
```

V√©rifier l'installation :
```bash
java -version
# Output: openjdk version "17.0.x"
```

---

## üîß Installation de Databricks CLI

La CLI Databricks permet de d√©ployer et g√©rer les bundles sur Databricks directement depuis votre terminal.

```bash
curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh | sh
```

V√©rifier l'installation :
```bash
databricks --version
# Output: Databricks CLI v0.x.x
```

---

## üîÑ Comment le code s'adapte automatiquement ?

Le pipeline d√©tecte automatiquement s'il s'ex√©cute en **local** (tests) ou sur **Databricks** (production) gr√¢ce √† 3 m√©canismes.

**D√©tection de l'environnement** dans `src/config.py` :
```python
def is_databricks() -> bool:
    """D√©tecte si on est sur Databricks"""
    return 'DATABRICKS_RUNTIME_VERSION' in os.environ
```

**Session Spark** dans `src/main.py` :
```python
def initialize_spark():
    # Essaie de r√©cup√©rer la session existante (Databricks)
    spark = SparkSession.getActiveSession()
    if spark is not None:
        return spark  # ‚úì Databricks
    # Sinon, cr√©e une nouvelle session (Local)
    return SparkSession.builder.appName("WAX").getOrCreate()
```

**Configuration** dans `src/config.py` :
```python
def get_config(dbutils=None):
    if dbutils is not None:
        return get_databricks_config(dbutils)  # Chemins DBFS
    else:
        return get_local_config()  # Chemins locaux
```

**Adaptations automatiques :**

| Composant | Local | Databricks |
|-----------|-------|------------|
| **Session Spark** | Cr√©√©e | Existante r√©cup√©r√©e |
| **Chemins fichiers** | `data/input/` | `dbfs:/FileStore/` |
| **Format tables** | Parquet | Delta Lake |
| **Format logs** | Parquet | Delta Lake |
| **Metastore** | D√©sactiv√© | Hive metastore |

Aucune modification de code n√©cessaire entre les environnements !

---

## üíª Pr√©parer les donn√©es d'entr√©e (local)

Le pipeline a besoin d'un fichier ZIP contenant les CSV et d'un fichier Excel de configuration. Placer vos fichiers dans le dossier `data/input/`.

```bash
ls data/input/
```

Vous devriez voir :
```
site_20251201_120001.zip
waxsite_config.xlsx
```

---

## ‚ñ∂Ô∏è Ex√©cuter le pipeline en local

Lancer le pipeline en mode local pour tester le traitement.

```bash
python3 src/main.py
```

Vous devriez voir :
```
üíª Mode: LOCAL d√©tect√© (TEST)
üíª Cr√©ation d'une nouvelle session Spark (mode local)...
üì• CONFIGURATION CHARG√âE
  zip_path            : /path/to/data/input/site_20251201_120001.zip
  excel_path          : /path/to/data/input/waxsite_config.xlsx
  ...
üì¶ Extraction ZIP ...
üìë Lecture Excel : waxsite_config.xlsx
‚ñ∂Ô∏è Table : wax_table
‚úîÔ∏è Delta saved (mode: append) -> data/output/wax_table_all
‚úÖ WAX Processing Completed
```

---

## ‚úÖ V√©rifier les r√©sultats (local)

S'assurer que les donn√©es ont √©t√© ing√©r√©es et que les logs sont g√©n√©r√©s.

```bash
ls data/output/
```

Vous devriez voir :
```
logs_execution/
logs_quality/
wax_table_all/       # Tables Parquet g√©n√©r√©es
wax_table_last/
```

---

## üîê Authentifier Databricks CLI

Connecter votre CLI au workspace Databricks pour pouvoir d√©ployer. Remplacer `your-workspace` par votre URL workspace.

```bash
databricks auth login --host https://your-workspace.cloud.databricks.com
```

Vous devriez voir :
```
‚úì Successfully logged in to Databricks
Profile: DEFAULT
```

---

## ‚úîÔ∏è Valider le bundle

V√©rifier que la configuration YAML (databricks.yml, resources/) est correcte avant de d√©ployer.

```bash
databricks bundle validate
```

Vous devriez voir :
```
‚úì Configuration is valid
‚úì All resources are correctly defined
```

---

## üöÄ D√©ployer en environnement DEV

D√©ployer le pipeline sur Databricks en mode d√©veloppement pour tester. Les notebooks seront upload√©s, le job cr√©√© avec cluster et schedule (2h du matin), et le pipeline DLT cr√©√©.

```bash
databricks bundle deploy -t dev
```

Vous devriez voir :
```
‚úì Uploading notebooks to /Workspace/Users/you/.bundle/wax_pipeline/dev/files
‚úì Creating job: WAX ETL Pipeline - dev
‚úì Creating pipeline: WAX Delta Live Tables - dev
‚úì Deployment complete
```

---

## üéØ Ex√©cuter le pipeline sur Databricks

Lancer manuellement le job pour tester l'ex√©cution sur Databricks.

```bash
databricks bundle run wax_etl_pipeline -t dev
```

Vous devriez voir :
```
‚úì Job run started (run_id: 12345)
‚úì Job is running...
‚úì Job completed successfully
Duration: 5m 23s
```

---

## üìä V√©rifier les logs d'ex√©cution

Consulter les logs pour s'assurer que tout s'est bien pass√©. Remplacer `<job_id>` par l'ID de votre job.

```bash
databricks jobs list-runs --job-id <job_id> --limit 1
```

Vous devriez voir :
```json
{
  "run_id": 12345,
  "state": {
    "life_cycle_state": "TERMINATED",
    "result_state": "SUCCESS"
  },
  "duration": 323000
}
```

---

## üóÑÔ∏è Consulter les tables Delta cr√©√©es

V√©rifier que les donn√©es ont bien √©t√© ing√©r√©es dans Delta Lake. Ex√©cuter dans un notebook Databricks :

```sql
SELECT * FROM delta.`/mnt/wax/dev/internal/v1/wax_table_last`
LIMIT 10;
```

Vous devriez voir une table avec vos donn√©es ing√©r√©es, incluant les colonnes `FILE_NAME_RECEIVED`, `FILE_PROCESS_DATE`, `yyyy`, `mm`, `dd`, et vos colonnes m√©tier.

---

## üè≠ D√©ployer en PRODUCTION

Une fois test√© en dev, d√©ployer en production avec isolation et permissions configur√©es pour les groupes data_engineers (MANAGE) et data_analysts (RUN).

```bash
databricks bundle deploy -t prod
```

Vous devriez voir :
```
‚úì Uploading to /Workspace/Shared/.bundle/wax_pipeline
‚úì Job created with schedule (daily at 2 AM)
‚úì Permissions applied: data_engineers (MANAGE), data_analysts (RUN)
‚úì Production deployment complete
```

---

## üîÑ Mode d'ingestion : FULL_SNAPSHOT

√âcrase compl√®tement la table `_last` √† chaque ex√©cution. Utilis√© pour des snapshots complets. La table `_last` contient toujours la derni√®re version compl√®te du fichier.

Configurer dans le fichier Excel, colonne "Ingestion mode" :
```
FULL_SNAPSHOT
```

---

## üîÑ Mode d'ingestion : DELTA_FROM_LOT

Ajoute les nouvelles donn√©es sans √©craser les anciennes (append). La table `_last` accumule toutes les donn√©es historiques.

Configurer dans le fichier Excel, colonne "Ingestion mode" :
```
DELTA_FROM_LOT
```

---

## üîÑ Mode d'ingestion : DELTA_FROM_NON_HISTORIZED

Fait un merge (upsert) bas√© sur les cl√©s de merge d√©finies dans Excel. Met √† jour les lignes existantes, ins√®re les nouvelles. La table `_last` contient toujours la version la plus r√©cente de chaque enregistrement.

Configurer dans le fichier Excel, colonne "Ingestion mode" :
```
DELTA_FROM_NON_HISTORIZED
```

D√©finir les cl√©s de merge dans Excel, colonne "isMergeKey" : mettre `1` pour les colonnes cl√©s.

---

## üîÑ Mode d'ingestion : FULL_KEY_REPLACE

Supprime toutes les lignes correspondant aux cl√©s de merge pr√©sentes dans le fichier, puis ins√®re les nouvelles. Permet un remplacement complet par cl√©s.

Configurer dans le fichier Excel, colonne "Ingestion mode" :
```
FULL_KEY_REPLACE
```

D√©finir les cl√©s de merge dans Excel, colonne "isMergeKey" : mettre `1` pour les colonnes cl√©s.

---

## üìù Configuration Excel : Feuille "Field-Column"

Cette feuille d√©finit les m√©tadonn√©es de chaque colonne (type, validation, transformation). Voici les colonnes importantes :

- `Delta Table Name` : Nom de la table Delta
- `Column Name` : Nom de la colonne
- `Field type` : Type (STRING, INTEGER, DATE, TIMESTAMP, FLOAT, etc.)
- `Is Nullable` : TRUE si la colonne peut √™tre NULL, FALSE sinon
- `Transformation Type` : uppercase, lowercase, regex, email, address, enumeration
- `Transformation Pattern` : Pattern regex pour les transformations
- `Enumeration Values` : Valeurs autoris√©es (s√©par√©es par virgule)
- `Default Value when Invalid` : Valeur par d√©faut si invalide
- `isMergeKey` : 1 si c'est une cl√© de merge, 0 sinon
- `isExtractValidity` : 1 si c'est la colonne de validit√© temporelle

---

## üìù Configuration Excel : Feuille "File-Table"

Cette feuille d√©finit la configuration de traitement de chaque fichier. Voici les colonnes importantes :

- `Delta Table Name` : Nom de la table cible
- `Source Table` : Nom de la table source
- `Filename Pattern` : Pattern du nom de fichier (ex: `wax_{yyyy}_{mm}_{dd}.csv`)
- `Input Format` : csv, excel, etc.
- `Ingestion mode` : FULL_SNAPSHOT, DELTA_FROM_LOT, DELTA_FROM_NON_HISTORIZED, FULL_KEY_REPLACE
- `Input delimiter` : D√©limiteur (`;`, `,`, `|`, etc.)
- `Input Header` : HEADER_USE (utilise le header), FIRST_LINE (premi√®re ligne uniquement), ou vide
- `Input charset` : UTF-8, ANSI, Latin1
- `Trim` : TRUE pour nettoyer les espaces, FALSE sinon
- `Merge concordant file` : TRUE pour merger plusieurs fichiers, FALSE pour un seul
- `Rejected line per file tolerance` : Tol√©rance d'erreur (ex: "10%" ou "5")

---

## üìä Consulter les jobs d√©ploy√©s

Lister tous les jobs WAX d√©ploy√©s sur Databricks avec leurs d√©tails.

```bash
databricks jobs list --output json | jq '.jobs[] | select(.settings.name | contains("WAX"))'
```

Vous devriez voir :
```json
{
  "job_id": 123,
  "settings": {
    "name": "WAX ETL Pipeline - dev",
    "schedule": {
      "quartz_cron_expression": "0 0 2 * * ?",
      "timezone_id": "Europe/Paris"
    }
  }
}
```

---

## üìä Consulter les derni√®res ex√©cutions

Afficher les 10 derni√®res ex√©cutions d'un job avec leur statut, dur√©e, et timestamp.

```bash
databricks jobs list-runs --job-id 123 --limit 10
```

Vous devriez voir une liste des ex√©cutions avec leur statut (SUCCESS, FAILED), dur√©e en millisecondes, et timestamp.

---

## üìä Consulter les logs de qualit√©

Consulter les erreurs de qualit√© d√©tect√©es (NULL_KEY, DUPLICATE_KEY, INVALID_DATE, etc.) dans un notebook Databricks.

```sql
SELECT table_name, filename, column_name, error_message, error_count, log_ts
FROM delta.`/mnt/logs/wax_data_quality_errors`
ORDER BY log_ts DESC
LIMIT 50;
```

Vous devriez voir une table avec les erreurs d√©tect√©es lors du traitement des fichiers.

---

## üìä Consulter les logs d'ex√©cution

Consulter l'historique des ex√©cutions avec statut, dur√©e, et nombre d'erreurs dans un notebook Databricks.

```sql
SELECT tableName, fileName, ingestionMode, status, errorCount, duration, ts
FROM delta.`/mnt/logs/wax_execution_logs_delta`
ORDER BY ts DESC
LIMIT 50;
```

Vous devriez voir l'historique complet des ex√©cutions du pipeline avec les m√©triques de performance.

---

## üîß Personnaliser les variables par environnement

D√©finir des variables sp√©cifiques pour dev vs prod (log level, retention, etc.) dans le fichier `databricks.yml`.

```yaml
targets:
  prod:
    variables:
      log_level: "ERROR"
      retention_days: 90
```

Utiliser dans un notebook :
```python
log_level = spark.conf.get("bundle.log_level", "INFO")
```

---

## üîê Utiliser des Secrets Databricks

Stocker des informations sensibles (tokens, mots de passe) dans Databricks Secrets au lieu de les mettre en clair dans `resources/wax_job.yml`.

```yaml
tasks:
  - task_key: wax_ingestion
    notebook_task:
      base_parameters:
        jfrog_token: "{{secrets/wax_pipeline/jfrog_token}}"
```

Cr√©er le secret :
```bash
databricks secrets create-scope wax_pipeline
databricks secrets put-secret wax_pipeline jfrog_token
```

---

## üß™ Tests et Validation

Le projet dispose d'une suite compl√®te de tests pour garantir la qualit√© et la non-r√©gression.

### Structure des Tests

```
tests/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ test_non_regression.py         # Tests de non-r√©gression (~45 tests)
‚îú‚îÄ‚îÄ test_calculation_accuracy.py   # Tests de validation des calculs (~25 tests)
‚îú‚îÄ‚îÄ test_transformations.py        # Tests des transformations (~30 tests)
‚îî‚îÄ‚îÄ test_compare_results.py        # Comparaison notebook original vs refactoris√© (~10 tests)

Total: ~110 tests avec ~80% de couverture
```

### Installation des d√©pendances de test

```bash
pip install pytest pytest-cov
```

### Ex√©cution rapide des tests

```bash
# Tous les tests
pytest tests/ -v

# Tests rapides uniquement (sans les tests de performance)
pytest tests/ -v -m "not slow"

# Avec rapport de couverture
pytest tests/ --cov=src --cov-report=html

# Un test sp√©cifique
pytest tests/test_non_regression.py::TestConfig::test_get_local_config -v
```

### Types de tests disponibles

#### 1. Tests de Non-R√©gression (`test_non_regression.py`)

V√©rifient que les fonctionnalit√©s existantes continuent de fonctionner :

- ‚úÖ Configuration (local/databricks)
- ‚úÖ Traitement de fichiers (ZIP, CSV, Excel)
- ‚úÖ Validation de donn√©es (types, dates, qualit√©)
- ‚úÖ Modes d'ingestion (APPEND, MERGE, OVERWRITE)
- ‚úÖ Gestion des cas limites (fichiers vides, caract√®res sp√©ciaux, gros fichiers)
- ‚úÖ Performance (temps de traitement < 30s pour 50K lignes)

**Exemple :**
```bash
# Tous les tests de non-r√©gression
pytest tests/test_non_regression.py -v

# Tests de configuration uniquement
pytest tests/test_non_regression.py::TestConfig -v
```

#### 2. Tests de Validation des Calculs (`test_calculation_accuracy.py`)

V√©rifient que les calculs sont exacts :

- ‚úÖ Calculs de base (somme, moyenne, comptage)
- ‚úÖ Statistiques (√©cart-type, spread, percentiles)
- ‚úÖ Tol√©rance (%, absolue)
- ‚úÖ Pr√©cision d√©cimale
- ‚úÖ Calculs de dates
- ‚úÖ Formules m√©tier (revenu, pourcentage, croissance)

**Exemples de calculs test√©s :**
```python
# Somme : [1,2,3,4,5] ‚Üí 15
# Moyenne : [10,20,30,40,50] ‚Üí 30.0
# Spread : [10,20,30,40,50] ‚Üí 40 (max - min)
# Tol√©rance : 10% de 100 ‚Üí 10
```

**Exemple :**
```bash
# Tous les tests de calculs
pytest tests/test_calculation_accuracy.py -v

# Tests statistiques uniquement
pytest tests/test_calculation_accuracy.py::TestStatisticalCalculations -v
```

#### 3. Tests des Transformations (`test_transformations.py`)

V√©rifient toutes les transformations de colonnes :

| Type | Input | Transformation | Output |
|------|-------|----------------|--------|
| Uppercase | `"alice"` | `uppercase` | `"ALICE"` |
| Lowercase | `"BOB"` | `lowercase` | `"bob"` |
| Trim | `"  alice  "` | `trim` | `"alice"` |
| Regex extract | `"user123"` | `user(\d+)` | `"123"` |
| Substring | `"ABCDEFGH"` | `1,4` | `"ABCD"` |
| Lpad | `"1"` | `5,0` | `"00001"` |
| Round | `3.14159` | `2 decimals` | `3.14` |
| Date format | `2025-01-01` | `dd/MM/yyyy` | `"01/01/2025"` |

**Exemple :**
```bash
# Tous les tests de transformations
pytest tests/test_transformations.py -v

# Tests de casse uniquement
pytest tests/test_transformations.py::TestCaseTransformations -v
```

#### 4. Tests de Comparaison avec le Notebook Original (`test_compare_results.py`)

**V√©rifient que le code refactoris√© produit exactement les m√™mes r√©sultats que le notebook original** :

- ‚úÖ M√™me nombre de lignes trait√©es
- ‚úÖ M√™mes transformations appliqu√©es (uppercase, capitalize, etc.)
- ‚úÖ M√™me pr√©cision num√©rique (d√©cimales conserv√©es)
- ‚úÖ M√™me parsing de dates
- ‚úÖ M√™mes contr√¥les qualit√© d√©tect√©s
- ‚úÖ M√™mes agr√©gations (moyennes, sommes)
- ‚úÖ Ordre des colonnes pr√©serv√©

**Exemples de comparaisons :**
```python
# Transformation: "alice dupont" ‚Üí "ALICE DUPONT" (comme le notebook original)
# Pr√©cision: 50000.50 reste 50000.50 (pas d'arrondi)
# Moyenne salaires: 51800.30 (exactement comme l'original)
```

**Exemple :**
```bash
# Tous les tests de comparaison
pytest tests/test_compare_results.py -v

# Test de comparaison des transformations
pytest tests/test_compare_results.py::TestResultsComparison::test_transformations_match_original -v
```

### Workflow recommand√©

#### Avant chaque commit
```bash
# Tests rapides
pytest tests/ -v -m "not slow"

# Si OK, commit
git add .
git commit -m "Feature: nouvelle fonctionnalit√©"
```

#### Avant chaque merge
```bash
# Tests complets + couverture
pytest tests/ --cov=src --cov-report=html

# V√©rifier > 80% de couverture
open htmlcov/index.html
```

### R√©solution de probl√®mes

**Probl√®me : Version Python diff√©rente pour Spark**
```
PySparkRuntimeError: [PYTHON_VERSION_MISMATCH]
```

**Solution :**
```bash
export PYSPARK_PYTHON=python3
export PYSPARK_DRIVER_PYTHON=python3
pytest tests/
```

**Probl√®me : pytest non trouv√©**
```bash
pip install pytest pytest-cov pytest-mock
```

**Probl√®me : Java non trouv√©**
```bash
sudo apt-get install openjdk-17-jdk
export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
```

---

## üõ†Ô∏è Workflow CI/CD : D√©veloppement local

Cr√©er une branche, modifier le code dans `src/` ou `notebooks/`, et tester en local.

```bash
git checkout -b feature/new-transformation
# Modifier le code
python3 src/main.py
```

---

## üõ†Ô∏è Workflow CI/CD : Validation

Valider le bundle avant de d√©ployer.

```bash
databricks bundle validate
```

---

## üõ†Ô∏è Workflow CI/CD : D√©ploiement en dev

D√©ployer en dev et ex√©cuter le job pour tester.

```bash
databricks bundle deploy -t dev
databricks bundle run wax_etl_pipeline -t dev
```

---

## üõ†Ô∏è Workflow CI/CD : Tests et validation

V√©rifier les r√©sultats de l'ex√©cution et consulter les tables Delta cr√©√©es.

```bash
databricks jobs list-runs --job-id <job_id>
```

---

## üõ†Ô∏è Workflow CI/CD : Merge et production

Merger la branche dans main et d√©ployer en production.

```bash
git checkout main
git merge feature/new-transformation
databricks bundle deploy -t prod
```

---

## üêõ Erreur : Java non trouv√©

Si vous voyez `JAVA_HOME is not set`, exporter la variable d'environnement et l'ajouter au fichier bashrc.

```bash
export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
echo 'export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64' >> ~/.bashrc
```

---

## üêõ Erreur : Module pyspark non trouv√©

Si vous voyez `ModuleNotFoundError: No module named 'pyspark'`, installer pyspark.

```bash
pip install pyspark
```

---

## üêõ Erreur : databricks CLI non trouv√©

Si vous voyez `command not found: databricks`, installer la CLI et relancer le terminal.

```bash
curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh | sh
# Relancer le terminal
```

---

## ‚úÖ Checklist premi√®re fois sur Databricks

- [ ] Installer Databricks CLI
- [ ] S'authentifier : `databricks auth login`
- [ ] Adapter `databricks.yml` (workspace URL)
- [ ] Adapter `resources/wax_job.yml` (cluster, schedule, emails)
- [ ] Uploader les fichiers source sur DBFS : `dbfs:/FileStore/tables/`
- [ ] Valider : `databricks bundle validate`
- [ ] D√©ployer en dev : `databricks bundle deploy -t dev`
- [ ] Tester : `databricks bundle run wax_etl_pipeline -t dev`
- [ ] V√©rifier les r√©sultats (tables Delta, logs)
- [ ] D√©ployer en prod : `databricks bundle deploy -t prod`
- [ ] Configurer les permissions (groupes data_engineers, data_analysts)

