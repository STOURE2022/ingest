# =========================================================
# Cr√©ation des widgets
# =========================================================

# landing = zone de d√©p√¥t des fichiers, actuellement dbfs (POC) apr√®s (ADLS Azure / data lake storage)
dbutils.widgets.text("zip_path", "dbfs:/FileStore/tables/wax_delta_from_historized.zip", "üì¶ ZIP Source")
dbutils.widgets.text("excel_path", "dbfs:/FileStore/tables/custom_test2_secret_conf.xlsx", "üìë Excel Config")

# transient/staging = espace de travail temporaire pour le d√©zippage et lister les fichiers
dbutils.widgets.text("extract_dir", "dbfs:/tmp/unzipped_wax_csvs", "üìÇ Dossier Extraction ZIP")

# les logs d'ex√©cution
dbutils.widgets.text("log_exec_path", "/mnt/logs/wax_execution_logs_delta", "üìù Logs Ex√©cution (Delta)")
dbutils.widgets.text("log_quality_path", "/mnt/logs/wax_data_quality_errors_delta", "üö¶ Log Qualit√© (Delta)")

# les param√®tres de notre pipeline
dbutils.widgets.text("env", "dev", "üåç Environnement")
dbutils.widgets.text("version", "v1", "‚öôÔ∏è Version Pipeline")

# Lecture de nos widgets
PARAMS = {k: dbutils.widgets.get(k) for k in [
    "zip_path", "excel_path", "extract_dir",
    "log_exec_path", "log_quality_path", "env", "version"
]}

print("‚úÖ Les param√®tres charg√©s :")
for k, v in PARAMS.items():
    print(f"{k}: {v}")


# =========================================================
# Gestion de nos imports n√©cessaires
# =========================================================

import os, re, zipfile, subprocess
from pyspark.sql import functions as F
from pyspark.sql import DataFrame, Window
from pyspark.sql import SparkSession
from functools import reduce

from pyspark.sql import types as T
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType, DateType, TimestampType, LongType, FloatType, DecimalType
from pyspark.sql.functions import lit, col, count, when, monotonically_increasing_id, coalesce, row_number, expr
from pyspark.sql.window import Window

from delta.tables import DeltaTable
from collections import Counter
from py4j.protocol import Py4JJavaError
import pandas as pd
import subprocess
import sys, time
from datetime import datetime
from pyspark.sql import Row

# =========================================================
# Installer openpyxl pour lecture Excel
# =========================================================
try:
    import openpyxl
except ImportError:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "openpyxl"])
import zipfile


# =========================================================
# Gestion des contraintes m√©tiers
# =========================================================

# Le bool dans le fichier excel
def parse_bool(x, default=False):
    if x is None:
        return default
    s = str(x).strip().lower()
    if s in ["true", "1", "yes", "y", "oui"]:
        return True
    if s in ["false", "0", "no", "n", "non"]:
        return False
    return default


# Le d√©limiteur dans le fichier excel
def normalize_delimiter(raw) -> str:
    """
    Normalise le d√©limiteur issu du fichier Excel.
    Doit √™tre un caract√®re unique (sinon -> erreur).
    """
    if raw is None or str(raw).strip() == "":
        return ","
    s = str(raw).strip().lower()
    if len(s) == 1:
        return s
    raise ValueError(f"Input delimiter '{raw}' non support√© (Spark CSV attend 1 char)")

# Le header dans le fichier excel
def parse_header_mode(x) -> tuple:
    if x is None:
        return False, False
    s = str(x).strip().upper()
    if s == "HEADER USE":
        return True, True
    if s == "FIRST LINE":
        return True, False
    return False, False


# Gestion des tol√©rances aux erreurs dans fichier excel
def parse_tolerance(raw, total_rows: int, default=0.0) -> float:
    # Si la valeur est vide, None ou invalide ‚Üí on utilise la valeur par d√©faut
    if raw is None or str(raw).strip().lower() in ["", "nan", "n/a", "none"]:
        return default

    s = str(raw).strip().lower().replace(",", ".").replace("%", "").replace(" ", "")

    # Cas pourcentage (ex: "10%", "0.5%")
    m1 = re.search(r"^(\d+(?:\.\d+)?)%?$", s)
    if m1:
        val = float(m1.group(1))
        if "%" in str(raw):
            return val / 100.0
        # Cas valeur absolue
        if total_rows <= 0:
            return 0.0
        return val / total_rows

    # Si rien ne correspond, retourne 0.0
    return 0.0

# =========================================================
# Gestion du type Field type dans le fichier Excel
# =========================================================
def spark_type_from_config(row):
    t = str(row.get("Field type")).strip().upper()

    if t == "STRING":    return StringType()
    if t in ["INT", "INTEGER"]: return IntegerType()
    if t == "LONG":      return LongType()
    if t == "FLOAT":     return FloatType()
    if t == "DOUBLE":    return DoubleType()
    if t == "BOOLEAN":   return BooleanType()
    if t == "DATE":      return DateType()
    if t == "TIMESTAMP": return TimestampType()
    if t == "DECIMAL":
        prec = int(row.get("Decimal precision", 38) or 38)
        scale = int(row.get("Decimal scale", 18) or 18)
        return DecimalType(prec, scale)

    return StringType()


# =========================================================
# R√©sum√© des erreurs qualit√©
# =========================================================
def print_summary(table_name: str, filename: str, total_rows: tuple, corrupt_rows: int, anomalies_total: int, cleaned_rows: int, errors_df):
    print("\n" + "=" * 80)
    print(f"üìä Report Ingestion App | Table={table_name}, File={filename}")
    if isinstance(total_rows, tuple):
        print(f"Total rows: {total_rows[0]} -> {total_rows[1]}, rejected (corrupt_rows): {corrupt_rows}, anomalies possibles: {anomalies_total}, cleaned={cleaned_rows}")
    else:
        print(f"Total rows: {total_rows}, rejected (corrupt_rows): {corrupt_rows}, anomalies possibles: {anomalies_total}, cleaned={cleaned_rows}")
    print("=" * 80)

    if errors_df is not None and not errors_df.rdd.isEmpty():
        print("‚ö†Ô∏è Data quality issues detected")
        error_counter = Counter()
        null_counter = Counter()

        for r in errors_df.collect():
            rd = r.asDict(recursive=True)
            em = rd.get("error_message") or rd.get("Error") or rd.get("error") or "message_inconnu"
            ec = rd.get("error_count") or rd.get("Count") or 1

            try:
                ec = int(ec)
            except Exception:
                ec = 1

            # S√©paration des erreurs nulles et autres
            if "null value" in str(em).lower() or "NULL" in str(em):
                null_counter[em] += ec
            else:
                error_counter[em] += ec

        # Affichage des erreurs non nulles
        if error_counter:
            print("\nErreurs de typage ou de format :")
            for em, total in error_counter.items():
                print(f" - {em}: {total} erreurs")

        # Affichage des valeurs nulles
        if null_counter:
            print("\nValeurs nulles d√©tect√©es :")
            for em, total in null_counter.items():
                print(f" - {em}: {total} cas")

    else:
        print("\n‚úÖ No data quality issues")
    print("=" * 80 + "\n")

# =========================================================
# Fonctions de logs d'ex√©cution (Delta + chemin fixe + partitions)
# =========================================================
def log_execution(table_name: str, filename: str, input_format: str, ingestion_mode: str,
                  output_zone: str, row_count: int = 0, column_count: int = 0, masking_applied: bool = False,
                  error_count: int = 0, error_msg: str = None,
                  status: str = "SUCCESS", start_time: float = None,
                  env: str = None, log_path: str = None):
    
    if env is None:
        env = PARAMS["env"]
    if log_path is None:
        log_path = PARAMS["log_exec_path"]

    today = datetime.today()
    duration = None
    if start_time:
        duration = round(time.time() - start_time, 2)

    # Sch√©ma explicite pour stabilit√©
    schema = StructType([
        StructField("table_name", StringType(), True),
        StructField("filename", StringType(), True),
        StructField("input_format", StringType(), True),
        StructField("ingestion_mode", StringType(), True),
        StructField("output_zone", StringType(), True),
        StructField("row_count", IntegerType(), True),
        StructField("column_count", IntegerType(), True),
        StructField("masking_applied", BooleanType(), True),
        StructField("error_count", IntegerType(), True),
        StructField("error_message", StringType(), True),
        StructField("status", StringType(), True),
        StructField("duration", DoubleType(), True),
        StructField("env", StringType(), True),
        StructField("log_ts", TimestampType(), True),
        StructField("yyyy", IntegerType(), True),
        StructField("mm", IntegerType(), True),
        StructField("dd", IntegerType(), True)
    ])

    row = [(str(table_name), str(filename), str(input_format), str(ingestion_mode),
            str(output_zone), int(row_count or 0), int(column_count or 0), bool(masking_applied),
            int(error_count or 0), str(error_msg or ""), str(status),
            float(duration or 0), str(env), datetime.now(),
            today.year, today.month, today.day)]

    df_log = spark.createDataFrame(row, schema=schema)

    # √âcriture Delta partitionn√©e par date
    df_log.write.format("delta") \
        .mode("append") \
        .option("mergeSchema", "true") \
        .partitionBy("yyyy", "mm", "dd") \
        .save(log_path)

# =========================================================
# Log des erreurs de qualit√© dans Delta (chemin fixe pour le moment)
# =========================================================
def write_quality_errors(df_errors: DataFrame, table_name: str, zone: str = "internal",
                         base_path: str = None,
                         env: str = None):
    
    if base_path is None:
        base_path = PARAMS["log_quality_path"]
    if env is None:
        env = PARAMS["env"]

    if df_errors is None or df_errors.rdd.isEmpty():
        return

    today = datetime.today()

    # Suppression des doublons de colonnes
    seen = set()
    cols = []
    for c in df_errors.columns:
        c1 = c.lower()
        if c1 not in seen:
            cols.append(c)
            seen.add(c1)

    df_errors = df_errors.select(*cols)

    # Normalisation de la colonne raw_value
    if "raw_value" in df_errors.columns:
        df_errors = df_errors.withColumn("raw_value", F.col("raw_value").cast("string"))
    else:
        df_errors = df_errors.withColumn("raw_value", F.lit(None).cast("string"))

    df_log = (
        df_errors
        .withColumn("table_name", F.coalesce(F.col("table_name"), F.lit(table_name)))
        .withColumn("Zone", F.lit(zone))
        .withColumn("Env", F.lit(env))
        .withColumn("log_ts", F.lit(datetime.now()))
        .withColumn("yyyy", F.lit(today.year))
        .withColumn("mm", F.lit(today.month))
        .withColumn("dd", F.lit(today.day))
    )

    # Create the directory if it does not exist
    try:
        dbutils.fs.ls(base_path)
    except Exception:
        dbutils.fs.mkdirs(base_path)

    # Check if the Delta table exists
    try:
        spark.read.format("delta").load(base_path)
    except Exception:
        # If the Delta table does not exist, create it
        df_log.write.format("delta") \
            .mode("overwrite") \
            .partitionBy("yyyy", "mm", "dd") \
            .save(base_path)
        print(f"‚úÖ Delta table created at {base_path}")

    # Append to the existing Delta table
    df_log.write.format("delta") \
        .mode("append") \
        .option("mergeSchema", "true") \
        .save(base_path)

# =========================================================
# Mise en place de chemin racine sans partitions
# =========================================================
def build_output_path(env: str, zone: str, table_name: str, version: str, parts: dict = None):
    return f"/mnt/wax/{env}/{zone}/{version}/{table_name}"


# =========================================================
# Extraction des parties yyyy/mm/dd √† partir du nom de fichier
# =========================================================
def extract_parts_from_filename(fname: str) -> dict:
    """
    Essaie d'extraire yyyy/mm/dd √† partir du nom de fichier
    Retourne {} si rien trouv√©
    """
    base = os.path.basename(fname)
    m = re.search(r"(?P<yyyy>\d{4})(?P<mm>\d{2})(?P<dd>\d{2})?", base)
    if not m:
        return {}

    parts = {}
    if m.group("yyyy"):
        parts["yyyy"] = int(m.group("yyyy"))
    if m.group("mm"):
        parts["mm"] = int(m.group("mm"))
    if m.group("dd"):
        parts["dd"] = int(m.group("dd"))

    return parts

# =========================================================
# Fonction de validation stricte du nom de fichier
# =========================================================
def validate_filename(fname: str, source_table: str, matched_uri: str, log_quality_path: str):
    """
    On v√©rifie que le nom de fichier contient une date valide.
    Extrait yyyy/mm/dd √† partir du nom de fichier.
    Retourne False si erreur, True si OK
    """

    base = os.path.basename(fname)
    print(f"Analyse du fichier : {base}")

    # Sch√©ma explicite pour le DataFrame d'erreur
    error_schema = StructType([
        StructField("table_name", StringType(), True),
        StructField("filename", StringType(), True),
        StructField("column_name", StringType(), True),
        StructField("line_id", IntegerType(), True),
        StructField("invalid_value", StringType(), True),
        StructField("error_message", StringType(), True),
        StructField("uri", StringType(), True),
    ])

    # Expression r√©guli√®re corrig√©e
    m = re.search(r"(?P<yyyy>\d{4})(?P<mm>\d{2})(?P<dd>\d{2})?", base)
    if not m:
        print(f"‚ùå Fichier rejet√© {base} (pattern de date manquant)")
        err_data = [(source_table, base, None, None, None,
                     "Missing / invalid date pattern in filename", matched_uri)]
        err_df = spark.createDataFrame(err_data, error_schema)
        err_df.write.format("delta").mode("append").save(log_quality_path)
        return False

    try:
        yyyy = int(m.group("yyyy"))
        mm = int(m.group("mm"))
        dd = int(m.group("dd")) if m.group("dd") else 1
        datetime(yyyy, mm, dd)  # validation date r√©elle
        print(f"‚úÖ Fichier accept√© {base} : date valide {yyyy}-{mm:02d}-{dd:02d}")
        return True
    except (ValueError, TypeError):
        print(f"‚ùå Fichier rejet√© {base} (date invalide)")
        err_data = [(source_table, base, "filename", None, f"{yyyy}-{mm:02d}-{dd:02d}",
                     "INVALID DATE in filename", matched_uri)]
        err_df = spark.createDataFrame(err_data, error_schema)
        err_df.write.format("delta").mode("append").save(log_quality_path)
        return False

# =========================================================
# √âcriture Delta √† la racine avec partition yyyy/mm/dd
# =========================================================
def save_delta(
    df,
    path: str,
    mode: str = "append",
    add_ts: bool = False,
    parts: dict = None,
    file_name_received: str = None
):
    """
    Sauvegarde un DataFrame en Delta Lake avec partition yyyy/mm/dd.
    """

    today = datetime.today()
    y = int((parts or {}).get("yyyy", today.year))
    m = int((parts or {}).get("mm", today.month))
    d = int((parts or {}).get("dd", today.day))

    # Ajout d'un timestamp de traitement
    if add_ts:
        df = df.withColumn("FILE_PROCESS_DATE", F.current_timestamp())

    # Ajout du nom de fichier sans extension
    if file_name_received:
        base_name = os.path.splitext(os.path.basename(file_name_received))[0]
        df = df.withColumn("FILE_NAME_RECEIVED", F.lit(base_name))

    # Forcer l'ordre des colonnes pour avoir FILE_NAME_RECEIVED et FILE_PROCESS_DATE en premier
    ordered_cols = []
    if "FILE_NAME_RECEIVED" in df.columns:
        ordered_cols.append("FILE_NAME_RECEIVED")
    if "FILE_PROCESS_DATE" in df.columns:
        ordered_cols.append("FILE_PROCESS_DATE")
    other_cols = [c for c in df.columns if c not in ordered_cols]
    df = df.select(ordered_cols + other_cols)

    # Supprimer les colonnes dupliqu√©es (case-insensitive)
    seen, cols = set(), []
    for c in df.columns:
        if c.lower() not in seen:
            cols.append(c)
            seen.add(c.lower())
    df = df.select(*cols)

    # D√©terminer les types attendus pour les partitions
    if DeltaTable.isDeltaTable(spark, path):
        schema = spark.read.format("delta").load(path).schema
        type_map = {f.name: f.dataType.simpleString() for f in schema.fields}
        yyyy_type = type_map.get("yyyy", "int")
        mm_type = type_map.get("mm", "int")
        dd_type = type_map.get("dd", "int")
    else:
        yyyy_type, mm_type, dd_type = "int", "int", "int"

    # Ajout / cast des colonnes de partition
    df = (
        df.withColumn("yyyy", F.lit(y).cast(yyyy_type))
          .withColumn("mm", F.lit(m).cast(mm_type))
          .withColumn("dd", F.lit(d).cast(dd_type))
    )

    # √âcriture finale en Delta Lake
    (
        df.write.format("delta")
        .option("mergeSchema", "true")
        .mode(mode)
        .partitionBy("yyyy", "mm", "dd")
        .save(path)
    )

    print(f"‚úÖ Delta saved at {path} (mode={mode}, partitions={y}-{m:02d}-{d:02d})")


# =========================================================
# Enregistrement de table Delta dans le metastore Hive
# =========================================================
def register_table_in_metastore(spark, table_name: str, path: str, database: str = "wax_obs", if_exists: str = "ignore"):
    """
    Enregistre la table Delta dans le metastore Hive.
    if_exists peut √™tre :
      - "overwrite"   : √©crase la table existante
      - "ignore"      : ne fait rien si la table existe d√©j√†
      - "errorexists" : l√®ve une erreur si la table existe d√©j√†
      - "append"      : ajoute les nouvelles donn√©es √† la table existante
    """

    full_name = f"{database}.{table_name}"
    exists = any(t.name == table_name for t in spark.catalog.listTables(database))

    if exists and if_exists == "ignore":
        print(f"‚ö†Ô∏è Table {full_name} existe d√©j√† -> ignor√©e")
        return
    elif exists and if_exists == "errorexists":
        raise Exception(f"‚ùå Table {full_name} existe d√©j√† -> errorexists")
    elif exists and if_exists == "overwrite":
        print(f"‚ôªÔ∏è Table {full_name} existe d√©j√† -> supprim√©e avant recr√©ation")
        spark.sql(f"DROP TABLE IF EXISTS {full_name}")
    elif exists and if_exists == "append":
        print(f"‚ûï Table {full_name} existe d√©j√† -> append (aucun changement metastore)")
        return

    # Cr√©ation de la table Delta
    spark.sql(f"""
        CREATE TABLE IF NOT EXISTS {full_name}
        USING DELTA
        LOCATION '{path}'
    """)
    print(f"‚úÖ Table {full_name} enregistr√©e sur {path}")

def apply_ingestion_mode(df_raw: DataFrame, column_defs: pd.DataFrame,
                         table_name: str, ingestion_mode: str,
                         env: str = None, zone: str = "internal",
                         version: str = None, parts=None,
                         FILE_NAME_RECEIVED=None):
    """
    Applique le mode d'ingestion (overwrite, merge, append).
    """
    if env is None:
        env = PARAMS["env"]
    if version is None:
        version = PARAMS["version"]
    
    path_all = build_output_path(env, zone, f"{table_name}_all", version, parts)
    path_last = build_output_path(env, zone, f"{table_name}_last", version, parts)

    specials = column_defs.copy()
    specials["Is Special lower"] = specials["Is Special"].astype(str).str.lower()
    merge_keys = specials[specials["Is Special lower"] == "ismergekey"]["Column Name"].tolist()
    update_cols = specials[specials["Is Special lower"] == "isstartvalidity"]["Column Name"].tolist()
    update_col = update_cols[0] if update_cols else None

    imode = (ingestion_mode or "").strip().upper()
    print(f"Ingestion mode : {imode}")

    save_delta(df_raw, path_all, mode="append", add_ts=True,
               parts=parts, file_name_received=FILE_NAME_RECEIVED)
    register_table_in_metastore(spark, f"{table_name}_all", path_all, if_exists="ignore")

    if imode == "FULL_SNAPSHOT":
        save_delta(df_raw, path_last, mode="overwrite",
                   parts=parts, file_name_received=FILE_NAME_RECEIVED)
        register_table_in_metastore(spark, f"{table_name}_last", path_last, if_exists="overwrite")

    elif imode == "DELTA_FROM_FLOW":
        save_delta(df_raw, path_last, mode="append", add_ts=True,
                   parts=parts, file_name_received=FILE_NAME_RECEIVED)
        register_table_in_metastore(spark, f"{table_name}_last", path_last, if_exists="append")

    elif imode == "DELTA_FROM_NON_HISTORIZED":
        
        if not merge_keys:
            print("‚ö†Ô∏è Pas de cl√© de jointure - fallback overwrite")
            save_delta(df_raw, path_last, mode="overwrite", add_ts=True, 
                      parts=parts, file_name_received=FILE_NAME_RECEIVED)
            register_table_in_metastore(spark, f"{table_name}_last", path_last, if_exists="overwrite")
            return

        fallback_col = "FILE_PROCESS_DATE"
        compare_col = update_col if update_col else fallback_col
        print(f"Colonne de comparaison temporelle : {compare_col}")
            
        auto_cols = ["FILE_PROCESS_DATE", "yyyy", "mm", "dd"]
        
        # Cast si n√©cessaire
        if compare_col in df_raw.columns:
            compare_dtype = str(df_raw.schema[compare_col].dataType)
            if compare_dtype == "StringType":
                df_raw = df_raw.withColumn(compare_col, F.to_timestamp(compare_col))
        
        df_raw = (
            df_raw
            .withColumn("FILE_PROCESS_DATE", F.current_timestamp())
            .withColumn("yyyy", F.lit(parts.get("yyyy", datetime.today().year)).cast("int"))
            .withColumn("mm", F.lit(parts.get("mm", datetime.today().month)).cast("int"))
            .withColumn("dd", F.lit(parts.get("dd", datetime.today().day)).cast("int"))
        )

        updates = df_raw.alias("updates")

        if DeltaTable.isDeltaTable(spark, path_last):
            target = DeltaTable.forPath(spark, path_last)
            target_cols = [f.name for f in target.toDF().schema.fields]
        else:
            target_cols = df_raw.columns

        # colonnes √† mettre √† jour (hors merge key et colonnes auto)
        update_cols_clean = [c for c in df_raw.columns if c in target_cols and c not in merge_keys and c not in auto_cols]
        insert_cols_clean = [c for c in df_raw.columns if c in target_cols and c not in auto_cols]

        update_expr = {c: f"updates.{c}" for c in update_cols_clean}
        insert_expr = {c: f"updates.{c}" for c in insert_cols_clean}

        cond = " AND ".join([f"target.{k}=updates.{k}" for k in merge_keys])

        if DeltaTable.isDeltaTable(spark, path_last):   
            (target.alias("target")
             .merge(updates, cond)
             .whenMatchedUpdate(condition=f"updates.{compare_col} > target.{compare_col}", set=update_expr)
             .whenNotMatchedInsert(values=insert_expr)
             .execute())
            print(f"‚úÖ Merge avec comparaison sur {compare_col} et cl√©s {merge_keys}")
            register_table_in_metastore(spark, f"{table_name}_last", path_last, if_exists="append")
        else:
            print(f"Cr√©ation table (pas encore Delta). OVERWRITE {compare_col}")
            save_delta(df_raw, path_last, mode="overwrite", add_ts=True, 
                      parts=parts, file_name_received=FILE_NAME_RECEIVED)
            register_table_in_metastore(spark, f"{table_name}_last", path_last, if_exists="overwrite")

    elif imode == "DELTA_FROM_HISTORIZED":  
        save_delta(df_raw, path_last, mode="append", add_ts=True, 
                  parts=parts, file_name_received=FILE_NAME_RECEIVED)
        register_table_in_metastore(spark, f"{table_name}_last", path_last, if_exists="append")

    elif imode == "FULL_KEY_REPLACE":
        if not merge_keys:
            raise Exception(f"Mode FULL KEY REPLACE pour table {table_name} ‚Üí merge_keys obligatoire.")

        if DeltaTable.isDeltaTable(spark, path_last):
            target = DeltaTable.forPath(spark, path_last)
            # construire la condition de suppression
            conditions = []
            for k in merge_keys:
                values = df_raw.select(k).distinct().rdd.flatMap(lambda x: x).collect()
                values_str = ','.join([f"'{str(x)}'" for x in values])
                conditions.append(f"{k} IN ({values_str})")
            cond = " OR ".join(conditions)
            
            print(f"üîÑ Suppression des lignes existantes sur cl√©s = {merge_keys}")
            target.delete(condition=cond)

            print(f"‚ûï Insertion des nouvelles lignes")
            save_delta(df_raw, path_last, mode="append", add_ts=True, 
                      parts=parts, file_name_received=FILE_NAME_RECEIVED)
            register_table_in_metastore(spark, f"{table_name}_last", path_last, if_exists="append")
        else:
            print(f"‚ö†Ô∏è Delta table {table_name}_last inexistante ‚Üí cr√©ation")
            save_delta(df_raw, path_last, mode="overwrite", add_ts=True, 
                      parts=parts, file_name_received=FILE_NAME_RECEIVED)
            register_table_in_metastore(spark, f"{table_name}_last", path_last, if_exists="overwrite")
    else:
        print(f"‚ùå Mode ingestion inconnu : {imode}")
        save_delta(df_raw, path_last, mode="append", add_ts=True, 
                  parts=parts, file_name_received=FILE_NAME_RECEIVED)
        register_table_in_metastore(spark, f"{table_name}_last", path_last, if_exists="append")
        
def check_data_quality(df: DataFrame, table_name: str,
                       merge_keys: list, filename: str = None, column_defs: pd.DataFrame = None) -> DataFrame:
    """
    V√©rifie la qualit√© des donn√©es (nulls, doublons, colonnes vides).
    """
    if "line_id" not in df.columns:
        df = df.withColumn("line_id", row_number().over(Window.orderBy(monotonically_increasing_id())))

    schema = StructType([
        StructField("table_name", StringType(), True),
        StructField("filename", StringType(), True),
        StructField("line_id", IntegerType(), True),
        StructField("column_name", StringType(), True),
        StructField("error_message", StringType(), True),
        StructField("error_count", IntegerType(), True)
    ])
    
    errors_df = spark.createDataFrame([], schema)

    data_columns = [c for c in df.columns if c not in ["line_id", "yyyy", "mm", "dd", "FILE_PROCESS_DATE", "FILE_NAME_RECEIVED"]]
    
    if not data_columns:
        return errors_df
    
    all_null = all(df.filter(col(c).isNotNull()).count() == 0 for c in data_columns)
    
    if all_null:
        return spark.createDataFrame(
            [(table_name, filename, None, "ALL_COLUMNS", "FILE_EMPTY_OR_ALL_NULL", 1)],
            schema
        )
    
    # cl√©s nulles
    for key in merge_keys or []:
        if key in df.columns:
            null_key_count = df.filter(col(key).isNull()).count()
            if null_key_count > 0:
                errs = spark.createDataFrame([(table_name, filename, None, key, "NULL_KEY", null_key_count)], schema)
                errors_df = errors_df.unionByName(errs, allowMissingColumns=True)
            
    # doublons sur les cl√©s
    if merge_keys:
        valid_keys = [k for k in merge_keys if k in df.columns]
        if valid_keys:
            dup_df = (
                df.groupBy(*valid_keys).count().filter(col("count") > 1)
                  .select(
                      lit(table_name).alias("table_name"),
                      lit(filename).alias("filename"),
                      lit(None).cast("int").alias("line_id"),
                      lit(','.join(valid_keys)).alias("column_name"),
                      lit("DUPLICATE_KEY").alias("error_message"),
                      col("count").alias("error_count")
                  )
            )
            errors_df = errors_df.unionByName(dup_df, allowMissingColumns=True)

    # Colonnes enti√®rement nulles ou nullabilit√©
    if column_defs is not None:
        subset = column_defs[column_defs["Delta Table Name"] == table_name]
        total_rows = df.count()
        
        for idx, crow in subset.iterrows():
            cname = crow["Column Name"]
            is_nullable = str(crow.get("Is Nullable", "true")).strip().lower() == "true"
            
            if cname in df.columns:
                non_null_count = df.filter(col(cname).isNotNull()).count()
                
                # Colonne enti√®rement nulle et non nullable
                if non_null_count == 0 and not is_nullable:
                    errs = spark.createDataFrame([(table_name, filename, None, cname, "COLUMN_ALL_NULL", total_rows)], schema)
                    errors_df = errors_df.unionByName(errs, allowMissingColumns=True)
                
                # Valeurs nulles ligne par ligne (sauf colonnes enti√®rement nulles)
                elif non_null_count > 0 and not is_nullable:
                    null_count = df.filter(col(cname).isNull()).count()
                    if null_count > 0:
                        null_df = (
                            df.filter(col(cname).isNull())
                              .withColumn("error_message", lit("NULL_VALUE"))
                              .withColumn("error_count", lit(1))
                              .withColumn("table_name", lit(table_name))
                              .withColumn("column_name", lit(cname))
                              .withColumn("filename", lit(filename))
                              .select("table_name", "filename", "line_id", "column_name", "error_message", "error_count")
                        )
                        errors_df = errors_df.unionByName(null_df, allowMissingColumns=True)

    return errors_df

# =========================================================
# WAX POC ‚Äì Partie 5 : Pr√©paration du ZIP & Lecture Excel
# =========================================================

print("üì¶ Extraction ZIP ...")
dbutils.fs.mkdirs(PARAMS["extract_dir"])

# Convertir le chemin dbfs:/ en chemin local pour extraction
extract_dir_local = PARAMS["extract_dir"].replace("dbfs:", "/dbfs")
os.makedirs(extract_dir_local, exist_ok=True)

with zipfile.ZipFile(PARAMS["zip_path"].replace("dbfs:", "/dbfs"), 'r') as zip_ref:
    zip_ref.extractall(extract_dir_local)

# Lecture du fichier Excel (config WAX)
excel_path = dbutils.widgets.get("excel_path")
excel_path_local = excel_path.replace("dbfs:", "/dbfs") if excel_path.startswith("dbfs:") else excel_path

# Chargement des d√©finitions de colonnes et tables
print(f"üìë Lecture Excel : {excel_path}")
file_columns_df = pd.read_excel(excel_path_local, sheet_name="Field-Column")
file_tables_df = pd.read_excel(excel_path_local, sheet_name="File-Table")

# =========================================================
# WAX POC ‚Äì Partie 6 : Parsing des dates & Mapping des types
# =========================================================

def parse_date_with_logs(df: DataFrame, cname: str, patterns: list,
                          table_name: str, filename: str,
                          default_date=None):
    """
    Parse une colonne de type date/timestamp avec plusieurs patterns.
    G√©n√®re des logs d√©taill√©s :
    - EMPTY_DATE
    - INVALID_DATE
    """
    raw_col = F.col(cname)

    # Champs vides -> None
    col_expr = F.when(F.length(F.trim(raw_col)) == 0, F.lit(None)).otherwise(raw_col)

    ts_col = None
    for p in patterns:
        cand = F.expr(f"try_to_timestamp({cname}, '{p}')")
        ts_col = cand if ts_col is None else F.coalesce(ts_col, cand)

    parsed = F.to_date(ts_col)
    parsed_with_default = F.when(parsed.isNull(), F.lit(default_date)).otherwise(parsed)

    # Ajout de la colonne pars√©e
    df_raw = df.withColumn(cname, parsed_with_default)

    # Logs d√©taill√©s
    errs = (
        df.withColumn("line_id", F.monotonically_increasing_id() + 1)
          .select(
              F.lit(table_name).alias("table_name"),
              F.lit(filename).alias("filename"),
              F.lit(cname).alias("column_name"),
              F.col("line_id"),
              raw_col.cast("string").alias("raw_value"),
              F.when(parsed.isNull() & (F.trim(raw_col) == ""), F.lit("EMPTY_DATE"))
               .when(parsed.isNull() & (F.trim(raw_col) != ""), F.lit("INVALID_DATE"))
               .otherwise(F.lit(None)).alias("error_type"),
              F.lit(default_date).alias("default_applied")
          )
          .where(F.col("error_type").isNotNull())
    )

    errs = errs.dropDuplicates(
        ["filename", "table_name", "column_name", "line_id", "raw_value", "error_type"]
    )

    return df_raw, errs


# ---------- Type mapping ----------
type_mapping = {
    "STRING": StringType(),
    "INTEGER": IntegerType(),
    "INT": IntegerType(),
    "FLOAT": FloatType(),
    "DOUBLE": DoubleType(),
    "BOOLEAN": BooleanType(),
    "DATE": DateType(),
}

# =========================================================
# Boucle principale de traitement des tables
# =========================================================

for _, trow in file_tables_df.iterrows():
    start_table_time = time.time()
    source_table = trow["Delta Table Name"]
    filename_pattern = str(trow.get("Filename Pattern", "")).strip()
    input_format = str(trow.get("Input Format", "csv")).strip().lower()
    output_zone = str(trow.get("Output Zone", "internal")).strip().lower()
    ingestion_mode = str(trow.get("Ingestion mode", "")).strip()
    print(f"\n{'='*60}\nüìã Table: {source_table}\n{'='*60}")

    # Options du File Table
    trim_flag = parse_bool(trow.get("Trim", True), True)
    delimiter_raw = str(trow.get("Input delimiter", ","))
    
    del_cols_allowed = parse_bool(trow.get("Delete Columns Allowed", False), False)
    ignore_empty = parse_bool(trow.get("Ignore empty Files", True), True)
    merge_files_flag = parse_bool(trow.get("Merge concomitant file", False), False)
    charset = str(trow.get("Input charset", "UTF-8")).strip() if pd.notnull(trow.get("Input charset")) else "UTF-8"
    if charset.lower() in ["nan", ""]:
        charset = "UTF-8"

    invalid_gen = parse_bool(trow.get("Invalid Lines Generate", False), False)
    
    # RegEx motif et capture date/heure
    rx = re.escape(filename_pattern)
    rx_yes_time = rx.replace(r"\<yyyy\>", r"(\d{4})").replace(r"\<mm\>", r"(\d{2})").replace(r"\<dd\>", r"(\d{2})").replace(r"\<hhmmss\>", r"(\d{6})")
    rx_yes_time = f"^{rx_yes_time}$"
    
    pattern_no_time = rx.replace(r"\<yyyy\>", r"(\d{4})").replace(r"\<mm\>", r"(\d{2})").replace(r"\<dd\>", r"(\d{2})").replace(r"_\<hhmmss\>", "").replace(r"\<hhmmss\>", "")
    pattern_no_time = f"^{pattern_no_time}$"

    # Collecte toutes les correspondances dans une liste
    try:
        all_files = dbutils.fs.ls(PARAMS["extract_dir"])
    except Exception as e:
        print(f"‚ùå Impossible de lister {PARAMS['extract_dir']}: {e}")
        log_execution(table_name=source_table, filename="N/A", input_format=input_format, 
                     ingestion_mode=ingestion_mode, output_zone=output_zone, 
                     error_msg=f"Directory error: {e}", status="FAILED", start_time=start_table_time)
        continue
    
    matched = [fi for fi in all_files if re.match(rx_yes_time, fi.name) or re.match(pattern_no_time, fi.name)]

    if len(matched) == 0:
        print(f"‚ö†Ô∏è Aucun fichier trouv√© correspondant au pattern: {filename_pattern}")
        log_execution(table_name=source_table, filename="N/A", input_format=input_format, 
                     ingestion_mode=ingestion_mode, output_zone=output_zone, row_count=0, column_count=0, 
                     masking_applied=(output_zone=="secret"),
                     error_msg=f"No file matching {filename_pattern}", status="FAILED", start_time=start_table_time)
        continue

    # On va produire (uri, parts) o√π parts = {yyyy, mm, dd}
    def to_tuple(fi):
        return (fi.path, extract_parts_from_filename(fi.name))

    if merge_files_flag:
        files_to_read = [to_tuple(fi) for fi in matched]
    else:
        files_to_read = [to_tuple(matched[0])]

    # D√©limiteur et header
    try:
        sep_char = normalize_delimiter(delimiter_raw)
    except Exception as e:
        log_execution(table_name=source_table, filename="N/A", input_format=input_format, 
                     ingestion_mode=ingestion_mode, output_zone=output_zone, row_count=0, column_count=0, 
                     masking_applied=(output_zone=="secret"),
                     error_msg=f"‚ùå Delimiter error: {e}", status="FAILED", start_time=start_table_time)
        continue
    
    # badRecordsPath
    bad_records = None
    if invalid_gen:
        bad_records = f"/mnt/logs/badrecords/{PARAMS['env']}/{source_table}"
        dbutils.fs.mkdirs(bad_records)

    header_mode = str(trow.get("Input header", ""))
    user_header, first_line_only = parse_header_mode(header_mode)

    # colonnes attendues
    expected_cols = file_columns_df[file_columns_df["Delta Table Name"] == source_table]["Column Name"].tolist()

    # imposed schema optionnel
    imposed_schema = None
    try:
        subset = file_columns_df[file_columns_df["Delta Table Name"] == source_table].copy()
        if not subset.empty and "Field Order" in subset.columns:
            subset = subset.sort_values(by=["Field Order"])
            fields = [StructField(r["Column Name"], spark_type_from_config(r), True) for _, r in subset.iterrows()]
            imposed_schema = StructType(fields)
    except Exception as e:
        print(f"‚ö†Ô∏è Impossible de cr√©er imposed schema: {e}")
        imposed_schema = None
    
    df_raw_list = []
    
    for matched_uri, parts in files_to_read:
        print(f"üìÑ Lecture du fichier : {os.path.basename(matched_uri)}, partitions : {parts}")
        
        reader = (spark.read
                  .option("sep", sep_char)
                  .option("header", str(user_header).lower())
                  .option("encoding", charset)
                  .option("ignoreEmptyFiles", str(ignore_empty).lower())
                  .option("mode", "PERMISSIVE")
                  .option("enforceSchema", "false")
                  .option("columnNameOfCorruptRecord", "_corrupt_record"))

        if bad_records:
            reader = reader.option("badRecordsPath", bad_records)

        if input_format in ["csv", "csv_quote", "csv_quote_ml", "csv_deprecated"]:
            if input_format in ["csv_quote", "csv_quote_ml"]:
                reader = reader.option("quote", '"').option("escape", "\\")
            if input_format == "csv_quote_ml":
                reader = reader.option("multiline", "true")

            # gestion de nos headers
            if imposed_schema is not None:
                df_file = reader.schema(imposed_schema).csv(matched_uri)
            elif user_header and not first_line_only:
                # HEADER_USE
                df_file = reader.option("header", "true").csv(matched_uri)
            elif user_header and first_line_only:
                # FIRST_LINE
                tmp_df = reader.option("header", "false").csv(matched_uri)
                tmp_df = tmp_df.withColumn("_rn", F.row_number().over(Window.orderBy(monotonically_increasing_id()))).filter(F.col("_rn") > 1).drop("_rn")
           
                if len(expected_cols) != len(tmp_df.columns):
                    raise Exception(f"The expected number of columns ({len(expected_cols)}) "
                                    f"does not match the number of columns found ({len(tmp_df.columns)})")

                df_file = tmp_df.toDF(*expected_cols)
            else:
                # Pas de header
                df_file = reader.option("header", "false").csv(matched_uri)
                if len(expected_cols) == len(df_file.columns):
                    df_file = df_file.toDF(*expected_cols)

        elif input_format == "fixed":
            # fixed-width √† partir de Column Size + Field Order
            text_df = spark.read.text(matched_uri)
            pos = 1
            exprs = []
            subset_fixed = file_columns_df[file_columns_df["Delta Table Name"] == source_table].sort_values(by=["Field Order"])
            for _, crow in subset_fixed.iterrows():
                cname = crow["Column Name"]
                size = int(crow.get("Column Size", 0) or 0)
                if size <= 0: 
                    size = 1
                exprs.append(F.expr(f"substring(value, {pos}, {size})").alias(cname))
                pos += size
            df_file = text_df.select(*exprs)

        else:
            log_execution(
                table_name=source_table,
                filename="N/A",
                input_format=input_format,
                ingestion_mode=ingestion_mode,
                output_zone=output_zone,
                row_count=0,
                column_count=0,
                masking_applied=(output_zone == "secret"),
                error_msg=f"Unsupported format {input_format}",
                status="FAILED",
                start_time=start_table_time
            )
            continue

        df_raw = df_file

        if not del_cols_allowed and expected_cols:
            missing = [c for c in expected_cols if c not in df_raw.columns]
            if missing:
                # Log des colonnes manquantes
                df_missing = spark.createDataFrame(
                    [(os.path.basename(matched_uri), m, "MISSING_COLUMN") for m in missing],
                    "filename STRING, column_name STRING, error_type STRING"
                )
                write_quality_errors(df_missing, source_table, zone=output_zone)
                log_execution(
                    table_name=source_table,
                    filename=os.path.basename(matched_uri),
                    input_format=input_format,
                    ingestion_mode=ingestion_mode,
                    output_zone=output_zone,
                    row_count=0,
                    column_count=len(df_raw.columns),
                    masking_applied=(output_zone == "secret"),
                    error_msg=f"Missing columns {missing} in your file",
                    status="FAILED",
                    start_time=start_table_time
                )
                continue

        # Ajouter colonnes manquantes si autoris√© / ou normaliser
        if expected_cols:
            for c in expected_cols:
                if c not in df_raw.columns:
                    df_raw = df_raw.withColumn(c, lit(None).cast(StringType()))
                else:
                    df_raw = df_raw.withColumn(c, df_raw[c].cast(StringType()))

        # Tol√©rance lignes corrompues
        total_rows = df_raw.count()
        corrupt_rows = df_raw.filter(col("_corrupt_record").isNotNull()).count() if "_corrupt_record" in df_raw.columns else 0

        rej_tol = parse_tolerance(trow.get("Rejected line per file tolerance", "10%"), total_rows)

        if corrupt_rows > rej_tol * total_rows:
            log_execution(
                table_name=source_table,
                filename=os.path.basename(matched_uri),
                input_format=input_format,
                ingestion_mode=ingestion_mode,
                output_zone=output_zone,
                row_count=total_rows,
                column_count=len(df_raw.columns),
                masking_applied=(output_zone == "secret"),
                error_msg=f"Too many corrupted lines ({corrupt_rows} > {rej_tol * total_rows})",
                status="FAILED",
                start_time=start_table_time
            )
            continue

        if "_corrupt_record" in df_raw.columns:
            df_raw = df_raw.drop("_corrupt_record")

        # Trim global table
        if trim_flag:
            for c in df_raw.columns:
                df_raw = df_raw.withColumn(c, F.trim(df_raw[c]))

        # D√©but : Typage + transformation + validation
        invalid_flags = []

        for _, crow in file_columns_df[file_columns_df["Delta Table Name"] == source_table].iterrows():
            cname = crow["Column Name"]
            if cname not in df_raw.columns:
                continue

            stype = spark_type_from_config(crow)
            tr_type = str(crow.get("Transformation Type", "")).strip().lower()
            tr_patt = str(crow.get("Transformation pattern", "")).strip()
            regex_repl = str(crow.get("Regex replacement", "")).strip()
            is_nullable = parse_bool(crow.get("Is Nullable", "True"), True)
            err_action = str(crow.get("Error action", "ICT_DRIVEN")).strip().upper()
            if err_action in ["", "NAN", "NONE", "NULL"]:
                err_action = "ICT_DRIVEN"

            ftype = str(crow.get("Field type", "")).strip().upper()
            default_inv = str(crow.get("Default when invalid", "")).strip()

            # Transformations simples
            if tr_type == "uppercase":
                df_raw = df_raw.withColumn(cname, F.upper(col(cname)))
            elif tr_type == "lowercase":
                df_raw = df_raw.withColumn(cname, F.lower(col(cname)))
            elif tr_type == "regex" and tr_patt:
                df_raw = df_raw.withColumn(cname, F.regexp_replace(F.col(cname), tr_patt, regex_repl if regex_repl else ""))

            # Cast s√©curis√©
            df_raw = df_raw.withColumn(f"{cname}_cast", F.expr(f"try_cast({cname} as {stype.simpleString()})"))

            # Remplacement par NULL si cast √©chou√©
            df_raw = df_raw.withColumn(
                cname,
                F.when(F.col(f"{cname}_cast").isNotNull(), F.col(f"{cname}_cast")).otherwise(F.lit(None))
            ).drop(f"{cname}_cast")

            # D√©tection des erreurs de type (valeurs devenues NULL apr√®s cast)
            if not is_nullable:
                invalid_cond = F.col(cname).isNull()
                invalid_count = df_raw.filter(invalid_cond).count()

                # Application de l'action d'erreur
                tolerance = parse_tolerance(crow.get("Rejected line per file tolerance", "10%"), total_rows)

                if err_action == "REJECT":
                    if invalid_count > 0:
                        errs = spark.createDataFrame(
                            [(os.path.basename(matched_uri), source_table, cname, invalid_count, "REJECT")],
                            "filename STRING, table_name STRING, column_name STRING, error_count INT, error_type STRING"
                        )
                        write_quality_errors(errs, source_table, zone=output_zone)
                        df_raw = df_raw.filter(~invalid_cond)
                        print(f"{invalid_count} lignes rejet√©es pour {cname}")

                elif err_action == "ICT_DRIVEN":
                    # Ajout d'un flag d'erreur pour cette colonne
                    flag_col = f"{cname}_invalid"
                    df_raw = df_raw.withColumn(flag_col, F.when(invalid_cond, F.lit(1)).otherwise(F.lit(0)))
                    invalid_flags.append(flag_col)

                    if total_rows > 0 and (invalid_count / float(total_rows)) > tolerance:
                        print(f"Trop d'erreurs sur {cname} -> abort ICT_DRIVEN")
                        errs_summary = spark.createDataFrame(
                            [(os.path.basename(matched_uri), source_table, cname, invalid_count, "ICT_DRIVEN_ABORT")],
                            "filename STRING, table_name STRING, column_name STRING, error_count INT, error_type STRING"
                        )
                        write_quality_errors(errs_summary, source_table, zone=output_zone)
                        df_raw = spark.createDataFrame([], df_raw.schema)
                        break

                    elif invalid_count > 0:
                        errs_detailed = df_raw.filter(invalid_cond).withColumn("line_id", monotonically_increasing_id()).select(
                            F.lit(os.path.basename(matched_uri)).alias("filename"),
                            F.lit(source_table).alias("table_name"),
                            F.lit(cname).alias("column_name"),
                            F.col("line_id"),
                            F.col(cname).alias("raw_value"),
                            F.lit("ICT_DRIVEN").alias("error_type")
                        )
                        write_quality_errors(errs_detailed, source_table, zone=output_zone)

                elif err_action == "LOG_ONLY":
                    if invalid_count > 0:
                        errs = spark.createDataFrame(
                            [(os.path.basename(matched_uri), source_table, cname, invalid_count, "LOG_ONLY")],
                            "filename STRING, table_name STRING, column_name STRING, error_count INT, error_type STRING"
                        )
                        write_quality_errors(errs, source_table, zone=output_zone)

            # Parsing Date/timestamp
            if isinstance(stype, (DateType, TimestampType)):
                patterns = []
                if tr_patt:
                    patterns.append(tr_patt)
                for p in ["dd/MM/yyyy HH:mm:ss", "dd/MM/yyyy", "yyyy-MM-dd HH:mm:ss", "yyyyMMddHHmmss",
                          "yyyyMMdd", "yyyy-MM-dd'T'HH:mm:ss"]:
                    if p not in patterns:
                        patterns.append(p)

                df_raw, errs = parse_date_with_logs(
                    df_raw, cname, [p for p in patterns if 'T' not in p],
                    source_table, os.path.basename(matched_uri),
                    default_date=default_inv if default_inv else None
                )
                # Enregistrer les erreurs si pr√©sentes
                if not errs.rdd.isEmpty():
                    write_quality_errors(errs, source_table, zone=output_zone)

            else:
                # Remplacement virgule d√©cimale par point
                df_raw = df_raw.withColumn(cname, F.regexp_replace(F.col(cname), ",", "."))

        # Rejet ligne par ligne pour ICT_DRIVEN (apr√®s la boucle)
        if invalid_flags:
            df_raw = df_raw.withColumn("invalid_column_count", sum([F.col(c) for c in invalid_flags]))
            max_invalid_per_line = max(1, int(len(invalid_flags) * 0.1))

            df_raw_valid = df_raw.filter(F.col("invalid_column_count") <= max_invalid_per_line)
            df_raw_invalid = df_raw.filter(F.col("invalid_column_count") > max_invalid_per_line)

            if not df_raw_invalid.rdd.isEmpty():
                df_raw_invalid = df_raw_invalid.withColumn("line_id", monotonically_increasing_id())
                write_quality_errors(df_raw_invalid, source_table, zone=output_zone)

            df_raw = df_raw_valid.drop("invalid_column_count", *invalid_flags)

        # Ajout ligne_id si absente
        if "line_id" not in df_raw.columns:
            df_raw = df_raw.withColumn("line_id", row_number().over(Window.orderBy(monotonically_increasing_id())))
        
        df_raw_list.append(df_raw)

    # =============================
    # Fusion des DataFrames si merge activ√©
    # =============================
    if not df_raw_list:
        print(f"‚ö†Ô∏è Aucun DataFrame √† traiter pour {source_table}")
        continue
    
    if len(df_raw_list) > 1:
        df_raw = reduce(DataFrame.unionByName, df_raw_list)
    else:
        df_raw = df_raw_list[0]

    # =============================
    # Data quality global (merge key)
    # =============================
    specials = file_columns_df[file_columns_df["Delta Table Name"] == source_table].copy()

    if "Is Special" in specials.columns:
        specials["Is Special lower"] = specials["Is Special"].astype(str).str.lower()
        merge_keys = specials[specials["Is Special lower"] == "ismergekey"]["Column Name"].tolist()
    else:
        merge_keys = []

    df_err_global = check_data_quality(
        df_raw,
        source_table,
        merge_keys,
        filename=f"merged_{len(df_raw_list)}_files" if len(df_raw_list) > 1 else os.path.basename(files_to_read[0][0]),
        column_defs=file_columns_df
    )

    column_errors = {}
    for _, col_def in file_columns_df[file_columns_df["Delta Table Name"] == source_table].iterrows():
        cname = str(col_def.get("Column Name")).strip()
        expected_type = str(col_def.get("Field type")).strip().upper()
        is_nullable = str(col_def.get("Is Nullable", "true")).strip().lower() == "true"

        if cname in df_raw.columns and expected_type in type_mapping:
            # Cast s√©curis√©
            safe_cast = df_raw.withColumn(f"{cname}_cast", expr(f"try_cast({cname} as {expected_type})"))

            # D√©tection des erreurs de type
            invalid_rows = safe_cast.filter(F.col(f"{cname}_cast").isNull() & F.col(cname).isNotNull() & (F.col(cname) != ""))

            if invalid_rows.count() > 0:
                if "line_id" not in invalid_rows.columns:
                    window = Window.orderBy(F.monotonically_increasing_id())
                    invalid_rows = invalid_rows.withColumn("line_id", F.row_number().over(window))

                invalid_samples_df = invalid_rows.select("line_id", cname)
                invalid_samples = invalid_samples_df.limit(100).collect()

                # Ajout au DataFrame global ligne par ligne
                err = spark.createDataFrame(
                    [(source_table, f"merged_{len(df_raw_list)}_files" if len(df_raw_list) > 1 else os.path.basename(files_to_read[0][0]), 
                      r["line_id"], cname,
                      f"TYPE MISMATCH : Expected type {expected_type}, but found : [line {r['line_id']}, value={r[cname]}]", 1)
                     for r in invalid_samples],
                    ["table_name", "filename", "line_id", "column_name", "error_message", "error_count"]
                )
                df_err_global = df_err_global.unionByName(err, allowMissingColumns=True)

            # Remplacement de la colonne originale par la colonne cast√©e
            df_raw = safe_cast.drop(cname).withColumnRenamed(f"{cname}_cast", cname)

            # ============================
            # Contr√¥le de la nullabilit√©
            # ============================
            if not is_nullable:
                null_rows = df_raw.filter(F.col(cname).isNull())
                if null_rows.count() > 0:
                    if "line_id" not in null_rows.columns:
                        window = Window.orderBy(F.monotonically_increasing_id())
                        null_rows = null_rows.withColumn("line_id", F.row_number().over(window))

                    null_samples_df = null_rows.select("line_id", cname)
                    null_samples = null_samples_df.limit(100).collect()

                    # Ajout au dictionnaire regroup√©
                    if cname not in column_errors:
                        column_errors[cname] = []
                    
                    for r in null_samples:
                        column_errors[cname].append(f"[line {r['line_id']}, value={r[cname]}]")

                    # Ajout au DataFrame global ligne par ligne
                    err = spark.createDataFrame(
                        [
                            (
                                source_table,
                                f"merged_{len(df_raw_list)}_files" if len(df_raw_list) > 1 else os.path.basename(files_to_read[0][0]),
                                r["line_id"],
                                cname,
                                f"Null value detected [not authorized], ICT count is incremented : [line {r['line_id']}, value={r[cname]}]",
                                1
                            )
                            for r in null_samples
                        ],
                        ["table_name", "filename", "line_id", "column_name", "error_message", "error_count"]
                    )
                    df_err_global = df_err_global.unionByName(err, allowMissingColumns=True)

    if column_errors:
        print("\n‚ö†Ô∏è R√©sum√© des erreurs de nullabilit√© par colonne :")
        for col_name, errors_list in column_errors.items():
            print(f"  - {col_name}: {len(errors_list)} valeurs nulles d√©tect√©es")

    print("\nüìä Record of possible errors detected in the file :")
    if not df_err_global.rdd.isEmpty():
        display(df_err_global.limit(100))
    else:
        print("‚úÖ Aucune erreur d√©tect√©e")

    # ============================
    # √âcriture des logs qualit√©
    # ============================
    write_quality_errors(df_err_global, source_table, zone=output_zone)

    # ============================
    # Ingestion (wax all, wax last)
    # ============================
    apply_ingestion_mode(
        df_raw,
        table_name=source_table,
        ingestion_mode=str(ingestion_mode),
        column_defs=file_columns_df[file_columns_df["Delta Table Name"] == source_table],
        env=PARAMS["env"], zone=output_zone, version=PARAMS["version"],
        parts=files_to_read[0][1] if files_to_read else {},
        FILE_NAME_RECEIVED=os.path.basename(files_to_read[0][0]) if files_to_read else "unknown"
    )

    # ============================
    # Harmonisation des colonnes df_err_global
    # ============================
    if "error" in df_err_global.columns and "error_message" not in df_err_global.columns:
        df_err_global = df_err_global.withColumnRenamed("error", "error_message")
    if "count" in df_err_global.columns and "error_count" not in df_err_global.columns:
        df_err_global = df_err_global.withColumnRenamed("count", "error_count")

    # ============================
    # R√©sum√© console + log ex√©cution
    # ============================
    total_rows_after = df_raw.count()

    # On filtre uniquement les erreurs Invalid
    if "error_message" in df_err_global.columns:
        inv_only = df_err_global.filter(F.col("error_message").contains("Invalid") | F.col("error_message").contains("TYPE MISMATCH"))
    else:
        inv_only = df_err_global.filter(F.col("Error").contains("Invalid"))

    if "error_count" in df_err_global.columns:
        corrupt_rows_only = inv_only.agg(F.sum("error_count").alias("s"))
    else:
        corrupt_rows_only = inv_only.agg(F.sum("Count").alias("s"))

    # Valeur du nombre de lignes corrompues
    corrupt_rows_val = corrupt_rows_only.collect()[0]["s"] if corrupt_rows_only.count() > 0 else 0
    corrupt_rows_val = corrupt_rows_val if corrupt_rows_val else 0

    # Calcul des lignes nettoy√©es
    cleaned_rows = total_rows_after - corrupt_rows_val

    # R√©sum√© final + log
    print_summary(
        table_name=source_table,
        filename=f"merged_{len(df_raw_list)}_files" if len(df_raw_list) > 1 else os.path.basename(files_to_read[0][0]),
        total_rows=(total_rows, total_rows_after),
        corrupt_rows=corrupt_rows_val,
        anomalies_total=df_err_global.count(),
        cleaned_rows=cleaned_rows,
        errors_df=df_err_global
    )
    
    # ============================
    # Log ex√©cution
    # ============================
    log_execution(
        table_name=source_table,
        filename=f"merged_{len(df_raw_list)}_files" if len(df_raw_list) > 1 else os.path.basename(files_to_read[0][0]),
        input_format=input_format,
        ingestion_mode=ingestion_mode,
        output_zone=output_zone,
        row_count=total_rows_after,
        column_count=len(df_raw.columns),
        masking_applied=(output_zone == "secret"),
        error_msg=f"{df_err_global.count()} errors detected",
        status="SUCCESS",
        error_count=df_err_global.count(),
        start_time=start_table_time
    )

print("\n" + "="*80)
print("üìä OBSERVABILIT√â - DASHBOARDS")
print("="*80)

# ============================
# Vues sur le dashboard
# Observabilit√© des logs
# ============================
try:
    df_logs = spark.read.format("delta").load(PARAMS["log_exec_path"])
    df_logs.createOrReplaceTempView("wax_execution_logs_delta")
    print("\n‚úÖ Full collection of raw logs")
    display(spark.sql("""
        SELECT table_name, input_format, filename, row_count, column_count, status,
               error_count, env, duration, log_ts
        FROM wax_execution_logs_delta
        ORDER BY log_ts DESC
        LIMIT 20
    """))
except Exception as e:
    print(f"‚ö†Ô∏è Impossible de charger les logs d'ex√©cution: {e}")

# ============================
# V√©rification existence logs qualit√©
# ============================
try:
    dbutils.fs.ls(PARAMS["log_quality_path"])
    path_exists = True
except Exception:
    path_exists = False
    print(f"‚ö†Ô∏è Le chemin {PARAMS['log_quality_path']} n'existe pas encore.")

if path_exists:
    try:
        df_quality = spark.read.format("delta").load(PARAMS["log_quality_path"])
        df_quality.createOrReplaceTempView("wax_data_quality_errors")
        print("\n‚úÖ File structure quality log")
        display(spark.sql("""
            SELECT table_name, filename, column_name, error_message, error_count, Zone, Env, log_ts
            FROM wax_data_quality_errors
            WHERE error_message IS NOT NULL
            ORDER BY log_ts DESC
            LIMIT 100
        """))
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur lors de la lecture des logs qualit√©: {e}")
else:
    print("‚ÑπÔ∏è Aucun log qualit√© disponible pour le moment.")

print("\n" + "="*80)
print("‚úÖ WAX PROCESSING COMPLETED")
print("="*80)
