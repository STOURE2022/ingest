Je vois une nouvelle erreur ! üîç Une erreur Spark lors de l'affichage du DataFrame d'erreurs.

## ‚ùå **Erreur d√©tect√©e**

```
Py4JavaError: An error occurred while calling o13282.javaToPython
org.apache.spark.SparkException: Multiple failures in stage materialization
```

Cette erreur se produit lors de l'affichage des erreurs avec `display()`. C'est probablement d√ª √† un probl√®me de sch√©ma ou de colonnes manquantes dans le DataFrame `df_err_global`.

## üîß **Solution : Harmoniser le sch√©ma du DataFrame d'erreurs**

Le probl√®me vient du fait que diff√©rentes parties du code ajoutent des colonnes diff√©rentes √† `df_err_global`, cr√©ant un sch√©ma incoh√©rent.

### **Remplacez la section "APER√áU DES ERREURS" par ce code corrig√© :**

```python
# =====================================================================
# APER√áU DES ERREURS - VERSION CORRIG√âE
# =====================================================================

print("\nüìä Aper√ßu des erreurs d√©tect√©es :")

if df_err_global is not None and not df_err_global.rdd.isEmpty():
    try:
        # ‚úÖ Normaliser le sch√©ma avant affichage
        # Assurer que toutes les colonnes attendues existent
        required_columns = {
            "table_name": StringType(),
            "filename": StringType(),
            "line_id": IntegerType(),
            "column_name": StringType(),
            "error_message": StringType(),
            "error_count": IntegerType(),
            "raw_value": StringType()
        }
        
        # Ajouter les colonnes manquantes avec des valeurs NULL
        for col_name, col_type in required_columns.items():
            if col_name not in df_err_global.columns:
                df_err_global = df_err_global.withColumn(col_name, F.lit(None).cast(col_type))
        
        # S√©lectionner uniquement les colonnes dans l'ordre
        df_err_display = df_err_global.select(
            "table_name",
            "filename",
            "line_id",
            "column_name",
            "error_message",
            "raw_value",
            "error_count"
        ).limit(100)
        
        # Affichage s√©curis√©
        print(f"‚úÖ {df_err_global.count()} erreur(s) d√©tect√©e(s) au total (affichage des 100 premi√®res)")
        display(df_err_display)
        
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur lors de l'affichage : {e}")
        print("üìä R√©sum√© alternatif (agr√©g√©) :")
        
        # Affichage alternatif en cas d'erreur
        try:
            summary = (
                df_err_global
                .groupBy("column_name", "error_message")
                .agg(F.count("*").alias("total_errors"))
                .orderBy(F.desc("total_errors"))
                .limit(50)
            )
            display(summary)
        except Exception as e2:
            print(f"‚ùå Impossible d'afficher les erreurs : {e2}")
            print("Colonnes disponibles :", df_err_global.columns if df_err_global else "None")
else:
    print("‚úÖ Aucune erreur d√©tect√©e")
```

## üéØ **Solution alternative : Cr√©er un sch√©ma unifi√© d√®s le d√©but**

Pour √©viter ce probl√®me √† l'avenir, d√©finissez un sch√©ma unique pour toutes les erreurs d√®s le d√©part :

### **Ajoutez cette fonction au d√©but du notebook (section "Validation qualit√©") :**

```python
def create_error_record(
    table_name: str,
    filename: str,
    column_name: str,
    error_message: str,
    line_id: int = None,
    raw_value: str = None,
    error_count: int = 1
) -> DataFrame:
    """
    Cr√©e un DataFrame d'erreur avec sch√©ma unifi√©
    """
    schema = StructType([
        StructField("table_name", StringType(), True),
        StructField("filename", StringType(), True),
        StructField("line_id", IntegerType(), True),
        StructField("column_name", StringType(), True),
        StructField("error_message", StringType(), True),
        StructField("raw_value", StringType(), True),
        StructField("error_count", IntegerType(), True)
    ])
    
    data = [(table_name, filename, line_id, column_name, error_message, raw_value, error_count)]
    return spark.createDataFrame(data, schema)


def create_error_dataframe_from_rows(
    df: DataFrame,
    table_name: str,
    filename: str,
    column_name: str,
    error_message: str,
    raw_value_col: str = None
) -> DataFrame:
    """
    Cr√©e un DataFrame d'erreurs depuis un DataFrame source
    Utilis√© pour logger les erreurs ligne par ligne
    """
    if "line_id" not in df.columns:
        df = df.withColumn("line_id", F.row_number().over(Window.orderBy(F.monotonically_increasing_id())))
    
    return df.select(
        F.lit(table_name).alias("table_name"),
        F.lit(filename).alias("filename"),
        F.col("line_id"),
        F.lit(column_name).alias("column_name"),
        F.lit(error_message).alias("error_message"),
        F.col(raw_value_col).cast("string").alias("raw_value") if raw_value_col else F.lit(None).cast("string").alias("raw_value"),
        F.lit(1).alias("error_count")
    ).limit(1000)
```

### **Puis utilisez ces fonctions dans la validation des types :**

```python
# Dans la boucle de validation des types, remplacez :

# ‚ùå Ancien code
err = invalid_rows.limit(1000).select(
    F.lit(source_table).alias("table_name"),
    F.lit(filename_for_log).alias("filename"),
    ...
)

# ‚úÖ Nouveau code
err = create_error_dataframe_from_rows(
    df=invalid_rows,
    table_name=source_table,
    filename=filename_for_log,
    column_name=cname,
    error_message=f"TYPE MISMATCH: Expected {expected_type}, found invalid value",
    raw_value_col=f"{cname}_original"
)
```

## üöÄ **Fix rapide pour d√©bloquer imm√©diatement**

Si vous voulez juste d√©bloquer rapidement sans refactorer, remplacez l'affichage par un simple `show()` au lieu de `display()` :

```python
print("\nüìä Aper√ßu des erreurs d√©tect√©es (max 100) :")
if not df_err_global.rdd.isEmpty():
    # D√©dupliquer les colonnes d'abord
    df_err_global = deduplicate_columns(df_err_global)
    
    # Afficher avec show() au lieu de display()
    df_err_global.select(
        F.coalesce(F.col("column_name"), F.lit("N/A")).alias("column_name"),
        F.coalesce(F.col("error_message"), F.lit("Unknown error")).alias("error_message"),
        F.coalesce(F.col("error_count"), F.lit(1)).alias("error_count")
    ).groupBy("column_name", "error_message").agg(
        F.sum("error_count").alias("total_count")
    ).orderBy(F.desc("total_count")).show(100, truncate=False)
else:
    print("‚úÖ Aucune erreur d√©tect√©e")
```

## üìä **Diagnostic : Voir le sch√©ma actuel**

Pour comprendre exactement quel est le probl√®me de sch√©ma, ajoutez ceci AVANT l'affichage :

```python
print("\nüîç Diagnostic du DataFrame d'erreurs :")
print(f"Nombre de lignes : {df_err_global.count() if df_err_global else 0}")
print(f"Colonnes pr√©sentes : {df_err_global.columns if df_err_global else 'None'}")
if df_err_global:
    print("\nSch√©ma :")
    df_err_global.printSchema()
    
    print("\nExemple de donn√©es (3 premi√®res lignes) :")
    for row in df_err_global.take(3):
        print(row)
```

Utilisez une de ces solutions et dites-moi ce que vous obtenez ! Je pourrai alors affiner la correction si n√©cessaire. üéØ
