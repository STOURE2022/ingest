# ============================================================================
# WAX PIPELINE - STRUCTURE COMPLÃˆTE DU PROJET
# ============================================================================
# 
# Voici la structure complÃ¨te du projet WAX Pipeline industrialisÃ©.
# CrÃ©ez cette arborescence sur votre systÃ¨me :
#
# wax_pipeline/
# â”œâ”€â”€ README.md
# â”œâ”€â”€ requirements.txt
# â”œâ”€â”€ setup.py
# â”œâ”€â”€ .gitignore
# â”œâ”€â”€ data/
# â”‚   â”œâ”€â”€ input/           # Placez vos fichiers ZIP et Excel ici
# â”‚   â”œâ”€â”€ temp/            # Fichiers temporaires
# â”‚   â””â”€â”€ output/          # RÃ©sultats et logs
# â”œâ”€â”€ src/
# â”‚   â”œâ”€â”€ __init__.py
# â”‚   â”œâ”€â”€ config.py        # âœ… CorrigÃ©
# â”‚   â”œâ”€â”€ utils.py         # âœ… CorrigÃ©
# â”‚   â”œâ”€â”€ validators.py    # âœ… CorrigÃ©
# â”‚   â”œâ”€â”€ delta_manager.py # âœ… CorrigÃ©
# â”‚   â”œâ”€â”€ logging_manager.py # âœ… CorrigÃ©
# â”‚   â”œâ”€â”€ ingestion.py     # âœ… CorrigÃ©
# â”‚   â”œâ”€â”€ file_processor.py # âœ… CorrigÃ©
# â”‚   â””â”€â”€ main.py          # âœ… CorrigÃ©
# â”œâ”€â”€ tests/
# â”‚   â”œâ”€â”€ __init__.py
# â”‚   â”œâ”€â”€ test_utils.py
# â”‚   â””â”€â”€ test_validators.py
# â””â”€â”€ notebooks/
#     â””â”€â”€ wax_databricks.py  # Pour exÃ©cution Databricks
#
# ============================================================================

# ============================================================================
# FICHIER 1: README.md
# ============================================================================
"""
# WAX Pipeline - Data Ingestion Framework

Pipeline ETL industriel pour l'ingestion de donnÃ©es CSV/Excel vers Delta Lake.

## ğŸš€ FonctionnalitÃ©s

- âœ… Support multi-environnement (Local / Databricks)
- âœ… 4 modes d'ingestion (FULL_SNAPSHOT, DELTA_FROM_LOT, etc.)
- âœ… Validation de donnÃ©es complÃ¨te
- âœ… Logging et observabilitÃ©
- âœ… Gestion des erreurs et bad records
- âœ… OptimisÃ© pour gros volumes

## ğŸ“¦ Installation

### Mode Local (Test)

```bash
# Cloner le repository
git clone <your-repo>
cd wax_pipeline

# CrÃ©er environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\\Scripts\\activate  # Windows

# Installer les dÃ©pendances
pip install -r requirements.txt

# PrÃ©parer les donnÃ©es
mkdir -p data/input data/temp data/output
# Placez vos fichiers ZIP et Excel dans data/input/
```

### Mode Databricks (Production)

```bash
# Uploader le package sur Databricks
databricks fs cp -r src/ dbfs:/wax_pipeline/src/

# CrÃ©er un notebook avec le contenu de notebooks/wax_databricks.py
# Ou utiliser le notebook Databricks directement
```

## ğŸ¯ Usage

### ExÃ©cution Locale

```bash
# Mode interactif
python src/main.py

# Avec paramÃ¨tres personnalisÃ©s
python src/main.py --zip data/input/site.zip --excel data/input/config.xlsx
```

### ExÃ©cution Databricks

1. CrÃ©er un notebook Databricks
2. Copier le contenu de `notebooks/wax_databricks.py`
3. Configurer les widgets
4. ExÃ©cuter

## ğŸ“Š Configuration

Le fichier Excel de configuration doit contenir 2 sheets :

### Sheet 1: Field-Column
- Column Name
- Field Type (STRING, INTEGER, DATE, etc.)
- Is Nullable
- Transformation Type
- isMergeKey
- isExtractValidity

### Sheet 2: File-Table
- Delta Table Name
- Filename Pattern
- Input Format
- Input delimiter
- Ingestion mode
- Rejected line per file tolerance

## ğŸ”§ Modes d'Ingestion

1. **FULL_SNAPSHOT** : Ã‰crase complÃ¨tement la table
2. **DELTA_FROM_LOT** : Ajout simple (append)
3. **DELTA_FROM_NON_HISTORIZED** : Merge avec mise Ã  jour
4. **FULL_KEY_REPLACE** : Suppression puis insertion par clÃ©

## ğŸ“ Logs

Les logs sont gÃ©nÃ©rÃ©s dans :
- `/mnt/logs/wax_execution_logs_delta` (logs d'exÃ©cution)
- `/mnt/logs/wax_data_quality_errors_delta` (logs qualitÃ©)

## ğŸ§ª Tests

```bash
# ExÃ©cuter tous les tests
pytest tests/

# Avec coverage
pytest --cov=src tests/
```

## ğŸ“š Documentation

Voir la documentation complÃ¨te : [docs/](docs/)

## ğŸ¤ Contributing

1. Fork le projet
2. CrÃ©er une branche feature
3. Commiter vos changements
4. Pousser sur la branche
5. Ouvrir une Pull Request

## ğŸ“„ License

MIT License - voir LICENSE file

## ğŸ‘¥ Authors

WAX Team - Data Engineering

## ğŸ“ Support

Pour tout problÃ¨me : <support@example.com>
"""

# ============================================================================
# FICHIER 2: requirements.txt
# ============================================================================
"""
# Core dependencies
pyspark==3.5.0
pandas==2.1.4
openpyxl==3.1.2
delta-spark==3.0.0

# Databricks (optionnel pour local)
# databricks-connect==13.3.0

# Testing
pytest==7.4.3
pytest-cov==4.1.0
pytest-mock==3.12.0

# Code quality
black==23.12.1
flake8==6.1.0
mypy==1.7.1

# Development
ipython==8.18.1
jupyter==1.0.0
"""

# ============================================================================
# FICHIER 3: setup.py
# ============================================================================
"""
from setuptools import setup, find_packages

setup(
    name="wax-pipeline",
    version="1.0.0",
    description="Pipeline ETL pour ingestion de donnÃ©es dans Delta Lake",
    author="WAX Team",
    author_email="wax@example.com",
    packages=find_packages(where="src"),
    package_dir={"": "src"},
    install_requires=[
        "pyspark>=3.5.0",
        "pandas>=2.1.0",
        "openpyxl>=3.1.0",
        "delta-spark>=3.0.0",
    ],
    extras_require={
        "dev": [
            "pytest>=7.4.0",
            "pytest-cov>=4.1.0",
            "black>=23.12.0",
            "flake8>=6.1.0",
            "mypy>=1.7.0",
        ]
    },
    python_requires=">=3.9",
    entry_points={
        "console_scripts": [
            "wax-pipeline=main:main",
        ],
    },
    classifiers=[
        "Development Status :: 4 - Beta",
        "Intended Audience :: Developers",
        "Topic :: Software Development :: Libraries",
        "Programming Language :: Python :: 3.9",
        "Programming Language :: Python :: 3.10",
        "Programming Language :: Python :: 3.11",
    ],
)
"""

# ============================================================================
# FICHIER 4: .gitignore
# ============================================================================
"""
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual Environment
venv/
ENV/
env/

# IDEs
.vscode/
.idea/
*.swp
*.swo
*~

# Data
data/temp/*
data/output/*
*.csv
*.zip
*.parquet

# Logs
*.log
logs/

# Jupyter
.ipynb_checkpoints/

# OS
.DS_Store
Thumbs.db

# Spark
metastore_db/
derby.log
spark-warehouse/

# Keep folder structure
!data/input/.gitkeep
!data/temp/.gitkeep
!data/output/.gitkeep
"""

# ============================================================================
# FICHIER 5: src/__init__.py (CORRIGÃ‰)
# ============================================================================
"""
WAX Pipeline - Data Ingestion Framework
Version: 1.0.0
Description: Pipeline ETL pour l'ingestion de donnÃ©es dans Delta Lake
"""

__version__ = "1.0.0"
__author__ = "WAX Team"
__all__ = [
    "config",
    "utils",
    "validators",
    "delta_manager",
    "logging_manager",
    "ingestion",
    "file_processor",
    "main"
]

# ============================================================================
# Les autres fichiers suivent dans les prochains messages...
# Pour obtenir le ZIP complet, je vais crÃ©er chaque fichier corrigÃ©.
# ============================================================================
"""
-------------------------------------------->
-------------------------------------------->

"""
Configuration Management
GÃ¨re les paramÃ¨tres pour Databricks et mode local
"""

import os
from typing import Dict, Any, Optional


# ================================================================================
# ğŸ”§ CONFIGURATION POUR DATABRICKS
# ================================================================================

def get_databricks_config(dbutils) -> Dict[str, Any]:
    """
    RÃ©cupÃ¨re la configuration depuis les widgets Databricks

    Args:
        dbutils: L'objet dbutils de Databricks

    Returns:
        Dict contenant tous les paramÃ¨tres
    """
    # CrÃ©ation des widgets
    dbutils.widgets.text("zip_path", "dbfs:/FileStore/tables/site_20251201_120001.zip", "ğŸ“¦ Source ZIP")
    dbutils.widgets.text("excel_path", "dbfs:/FileStore/tables/waxsite_config.xlsx", "ğŸ“‘ Excel Config")
    dbutils.widgets.text("extract_dir", "dbfs:/tmp/unzipped_wax_csvs", "ğŸ“‚ Dossier Extraction ZIP")
    dbutils.widgets.text("log_exec_path", "/mnt/logs/wax_execution_logs_delta", "ğŸ“ Logs ExÃ©cution (Delta)")
    dbutils.widgets.text("log_quality_path", "/mnt/logs/wax_data_quality_errors_delta", "ğŸš¦ Logs QualitÃ© (Delta)")
    dbutils.widgets.text("env", "dev", "ğŸŒ Environnement")
    dbutils.widgets.text("version", "v1", "ğŸ“– Version Pipeline")

    # Lecture des paramÃ¨tres
    params = {
        k: dbutils.widgets.get(k) for k in [
            "zip_path",
            "excel_path",
            "extract_dir",
            "log_exec_path",
            "log_quality_path",
            "env",
            "version"
        ]
    }

    return params


# ================================================================================
# ğŸ’» CONFIGURATION POUR MODE LOCAL (TEST)
# ================================================================================

def get_local_config(base_path: Optional[str] = None) -> Dict[str, Any]:
    """
    Configuration pour les tests en local

    Args:
        base_path: Chemin de base du projet (auto-dÃ©tectÃ© si None)

    Returns:
        Dict contenant tous les paramÃ¨tres avec chemins locaux
    """
    if base_path is None:
        # Auto-dÃ©tection du chemin de base
        base_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

    params = {
        "zip_path": os.path.join(base_path, "data", "input", "site_20251201_120001.zip"),
        "excel_path": os.path.join(base_path, "data", "input", "waxsite_config.xlsx"),
        "extract_dir": os.path.join(base_path, "data", "temp", "unzipped"),
        "log_exec_path": os.path.join(base_path, "data", "output", "logs_execution"),
        "log_quality_path": os.path.join(base_path, "data", "output", "logs_quality"),
        "env": "local",
        "version": "v1"
    }

    # CrÃ©er les dossiers nÃ©cessaires
    for key in ["extract_dir", "log_exec_path", "log_quality_path"]:
        os.makedirs(params[key], exist_ok=True)

    # CrÃ©er aussi le dossier input si absent
    input_dir = os.path.join(base_path, "data", "input")
    os.makedirs(input_dir, exist_ok=True)

    return params


# ================================================================================
# ğŸ“„ FONCTION DE DÃ‰TECTION AUTOMATIQUE
# ================================================================================

def is_databricks() -> bool:
    """
    DÃ©tecte si le code s'exÃ©cute sur Databricks

    Returns:
        True si sur Databricks, False sinon
    """
    try:
        # VÃ©rifier si nous sommes dans un environnement Databricks
        return 'DATABRICKS_RUNTIME_VERSION' in os.environ
    except:
        return False


def get_config(dbutils=None, base_path: Optional[str] = None) -> Dict[str, Any]:
    """
    DÃ©tecte automatiquement l'environnement et retourne la config appropriÃ©e

    Args:
        dbutils: L'objet dbutils (None en local, prÃ©sent sur Databricks)
        base_path: Chemin de base pour mode local (auto-dÃ©tectÃ© si None)

    Returns:
        Dict contenant la configuration

    Examples:
        >>> # Mode local
        >>> config = get_config()
        
        >>> # Mode Databricks
        >>> config = get_config(dbutils)
    """
    # Auto-dÃ©tection si dbutils n'est pas fourni
    if dbutils is None and is_databricks():
        try:
            # Importer dbutils depuis l'environnement Databricks
            from pyspark.dbutils import DBUtils
            from pyspark.sql import SparkSession
            spark = SparkSession.builder.getOrCreate()
            dbutils = DBUtils(spark)
        except:
            pass

    if dbutils is not None:
        print("ğŸŒ Mode: DATABRICKS dÃ©tectÃ©")
        return get_databricks_config(dbutils)
    else:
        print("ğŸ’» Mode: LOCAL dÃ©tectÃ© (TEST)")
        return get_local_config(base_path)


# ================================================================================
# ğŸ“‹ AFFICHAGE DE LA CONFIGURATION
# ================================================================================

def print_config(params: Dict[str, Any]) -> None:
    """
    Affiche la configuration de maniÃ¨re lisible
    
    Args:
        params: Dictionnaire de configuration
    """
    print("=" * 80)
    print("ğŸ“¥ CONFIGURATION CHARGÃ‰E")
    print("=" * 80)
    for key, value in params.items():
        # Tronquer les chemins trop longs
        display_value = value
        if isinstance(value, str) and len(value) > 60:
            display_value = "..." + value[-57:]
        print(f"  {key:20s}: {display_value}")
    print("=" * 80)


# ================================================================================
# ğŸ” VALIDATION DE LA CONFIGURATION
# ================================================================================

def validate_config(params: Dict[str, Any]) -> tuple[bool, list[str]]:
    """
    Valide la configuration et retourne les erreurs Ã©ventuelles
    
    Args:
        params: Dictionnaire de configuration
        
    Returns:
        Tuple (is_valid, errors)
        - is_valid: True si config valide
        - errors: Liste des messages d'erreur
    """
    errors = []
    
    # VÃ©rifier les clÃ©s obligatoires
    required_keys = ["zip_path", "excel_path", "extract_dir", "log_exec_path", "log_quality_path", "env", "version"]
    
    for key in required_keys:
        if key not in params:
            errors.append(f"âŒ ClÃ© manquante: {key}")
        elif not params[key]:
            errors.append(f"âŒ Valeur vide pour: {key}")
    
    # VÃ©rifier l'existence des fichiers en mode local
    if params.get("env") == "local":
        # VÃ©rifier ZIP
        if not os.path.exists(params.get("zip_path", "")):
            errors.append(f"âŒ Fichier ZIP introuvable: {params.get('zip_path')}")
        
        # VÃ©rifier Excel
        if not os.path.exists(params.get("excel_path", "")):
            errors.append(f"âŒ Fichier Excel introuvable: {params.get('excel_path')}")
    
    is_valid = len(errors) == 0
    
    if not is_valid:
        print("\nâš ï¸  ERREURS DE CONFIGURATION:")
        for error in errors:
            print(f"  {error}")
    
    return is_valid, errors


------------------------------>
------------------------------>


# ğŸ“¦ WAX Pipeline - Package Complet

## ğŸ¯ Structure du Projet

CrÃ©ez cette arborescence sur votre systÃ¨me :

```
wax_pipeline/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ .gitignore
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input/
â”‚   â”‚   â””â”€â”€ .gitkeep
â”‚   â”œâ”€â”€ temp/
â”‚   â”‚   â””â”€â”€ .gitkeep
â”‚   â””â”€â”€ output/
â”‚       â””â”€â”€ .gitkeep
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â”œâ”€â”€ validators.py
â”‚   â”œâ”€â”€ delta_manager.py
â”‚   â”œâ”€â”€ logging_manager.py
â”‚   â”œâ”€â”€ ingestion.py
â”‚   â”œâ”€â”€ file_processor.py
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_utils.py
â”‚   â””â”€â”€ test_validators.py
â””â”€â”€ notebooks/
    â””â”€â”€ wax_databricks.py
```

---

## ğŸ“„ Contenu des Fichiers

### 1ï¸âƒ£ README.md

```markdown
# WAX Pipeline - Data Ingestion Framework

Pipeline ETL industriel pour l'ingestion de donnÃ©es CSV/Excel vers Delta Lake.

## ğŸš€ Installation

```bash
# Cloner et installer
git clone <your-repo>
cd wax_pipeline
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

## ğŸ¯ Usage

### Mode Local
```bash
python src/main.py
```

### Mode Databricks
Utiliser le notebook `notebooks/wax_databricks.py`

## ğŸ“Š Configuration

Le fichier Excel doit contenir 2 sheets :
- **Field-Column** : MÃ©tadonnÃ©es des colonnes
- **File-Table** : ParamÃ¨tres des fichiers

## ğŸ”§ Modes d'Ingestion

1. **FULL_SNAPSHOT** : Ã‰crase complÃ¨tement
2. **DELTA_FROM_LOT** : Append simple
3. **DELTA_FROM_NON_HISTORIZED** : Merge avec update
4. **FULL_KEY_REPLACE** : Delete + Insert

## ğŸ“ Logs

- ExÃ©cution : `/mnt/logs/wax_execution_logs_delta`
- QualitÃ© : `/mnt/logs/wax_data_quality_errors_delta`

## ğŸ§ª Tests

```bash
pytest tests/
```
```

---

### 2ï¸âƒ£ requirements.txt

```
pyspark==3.5.0
pandas==2.1.4
openpyxl==3.1.2
delta-spark==3.0.0
pytest==7.4.3
pytest-cov==4.1.0
black==23.12.1
flake8==6.1.0
```

---

### 3ï¸âƒ£ setup.py

```python
from setuptools import setup, find_packages

setup(
    name="wax-pipeline",
    version="1.0.0",
    description="Pipeline ETL pour ingestion Delta Lake",
    author="WAX Team",
    packages=find_packages(where="src"),
    package_dir={"": "src"},
    install_requires=[
        "pyspark>=3.5.0",
        "pandas>=2.1.0",
        "openpyxl>=3.1.0",
        "delta-spark>=3.0.0",
    ],
    python_requires=">=3.9",
    entry_points={
        "console_scripts": [
            "wax-pipeline=main:main",
        ],
    },
)
```

---

### 4ï¸âƒ£ .gitignore

```
__pycache__/
*.py[cod]
venv/
.vscode/
.idea/
data/temp/*
data/output/*
*.csv
*.zip
*.log
.DS_Store
metastore_db/
spark-warehouse/
!data/**/.gitkeep
```

---

### 5ï¸âƒ£ src/__init__.py

```python
"""
WAX Pipeline - Data Ingestion Framework
Version: 1.0.0
"""

__version__ = "1.0.0"
__author__ = "WAX Team"
```

---

### 6ï¸âƒ£ tests/__init__.py

```python
"""
Tests for WAX Pipeline
"""
```

---

### 7ï¸âƒ£ tests/test_utils.py

```python
import pytest
from src.utils import parse_bool, normalize_delimiter, parse_tolerance


def test_parse_bool():
    assert parse_bool("true") == True
    assert parse_bool("0") == False
    assert parse_bool(None, default=True) == True
    assert parse_bool("yes") == True


def test_normalize_delimiter():
    assert normalize_delimiter(",") == ","
    assert normalize_delimiter(";") == ";"
    assert normalize_delimiter(None) == ","
    
    with pytest.raises(ValueError):
        normalize_delimiter(";;")


def test_parse_tolerance():
    assert parse_tolerance("10%", 1000) == 0.1
    assert parse_tolerance("50", 1000) == 0.05
    assert parse_tolerance(None, 1000, default="5%") == 0.05
```

---

### 8ï¸âƒ£ tests/test_validators.py

```python
import pytest
from pyspark.sql import SparkSession
from src.validators import spark_type_from_config
import pandas as pd


@pytest.fixture(scope="module")
def spark():
    return SparkSession.builder.master("local[2]").appName("test").getOrCreate()


def test_spark_type_from_config():
    row = pd.Series({"Field Type": "STRING"})
    from pyspark.sql.types import StringType
    assert isinstance(spark_type_from_config(row), StringType)
    
    row = pd.Series({"Field Type": "INTEGER"})
    from pyspark.sql.types import IntegerType
    assert isinstance(spark_type_from_config(row), IntegerType)
```

---

### 9ï¸âƒ£ data/input/.gitkeep, data/temp/.gitkeep, data/output/.gitkeep

```
# Fichier vide pour garder la structure des dossiers dans Git
```

---

## ğŸš€ Installation Rapide

### Ã‰tape 1 : CrÃ©er la structure

```bash
# CrÃ©er le dossier principal
mkdir wax_pipeline
cd wax_pipeline

# CrÃ©er les sous-dossiers
mkdir -p data/input data/temp data/output
mkdir -p src tests notebooks

# CrÃ©er les fichiers .gitkeep
touch data/input/.gitkeep data/temp/.gitkeep data/output/.gitkeep
```

### Ã‰tape 2 : Copier les fichiers Python

Copiez tous les fichiers Python que je vous ai fournis dans les bons dossiers :

- **src/** : Tous les fichiers .py principaux
- **tests/** : Les fichiers de tests
- **notebooks/** : Le notebook Databricks

### Ã‰tape 3 : Installer les dÃ©pendances

```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou venv\Scripts\activate sur Windows

pip install -r requirements.txt
```

### Ã‰tape 4 : PrÃ©parer les donnÃ©es

```bash
# Placer vos fichiers dans data/input/
cp /path/to/your/site.zip data/input/
cp /path/to/your/config.xlsx data/input/
```

### Ã‰tape 5 : ExÃ©cuter

```bash
python src/main.py
```

---

## ğŸ“¥ TÃ©lÃ©chargement

Comme je ne peux pas crÃ©er de fichier ZIP directement, voici **3 options** pour obtenir le projet complet :

### Option 1 : Script Bash (Linux/Mac)

CrÃ©ez un fichier `create_wax_project.sh` :

```bash
#!/bin/bash

# CrÃ©er la structure
mkdir -p wax_pipeline/{data/{input,temp,output},src,tests,notebooks}
cd wax_pipeline

# CrÃ©er README.md
cat > README.md << 'EOF'
# WAX Pipeline
Pipeline ETL pour Delta Lake
EOF

# CrÃ©er requirements.txt
cat > requirements.txt << 'EOF'
pyspark==3.5.0
pandas==2.1.4
openpyxl==3.1.2
delta-spark==3.0.0
EOF

# CrÃ©er .gitkeep
touch data/input/.gitkeep data/temp/.gitkeep data/output/.gitkeep
touch src/__init__.py tests/__init__.py

echo "âœ… Structure crÃ©Ã©e ! Copiez maintenant les fichiers Python."
```

ExÃ©cutez :
```bash
chmod +x create_wax_project.sh
./create_wax_project.sh
```

### Option 2 : Script PowerShell (Windows)

```powershell
# CrÃ©er la structure
New-Item -ItemType Directory -Path "wax_pipeline\data\input","wax_pipeline\data\temp","wax_pipeline\data\output","wax_pipeline\src","wax_pipeline\tests","wax_pipeline\notebooks" -Force

# CrÃ©er fichiers vides
New-Item -ItemType File -Path "wax_pipeline\data\input\.gitkeep","wax_pipeline\src\__init__.py","wax_pipeline\tests\__init__.py" -Force

Write-Host "âœ… Structure crÃ©Ã©e !"
```

### Option 3 : GitHub Repository

Je recommande de crÃ©er un repository Git :

```bash
cd wax_pipeline
git init
git add .
git commit -m "Initial commit"
git remote add origin <your-github-repo>
git push -u origin main
```

---

## ğŸ”§ Fichiers Manquants Ã  Copier

Je vous ai fourni prÃ©cÃ©demment les fichiers Python complets. Copiez-les dans les bons dossiers :

### Ã€ copier dans `src/` :
- âœ… `config.py` (fourni prÃ©cÃ©demment)
- âœ… `utils.py` (dans votre document 9)
- âœ… `validators.py` (dans votre document 10)
- âœ… `delta_manager.py` (dans votre document 4)
- âœ… `logging_manager.py` (dans votre document 7)
- âœ… `ingestion.py` (dans votre document 6)
- âœ… `file_processor.py` (dans votre document 5)
- âœ… `main.py` (dans votre document 8)

### Ã€ crÃ©er dans `notebooks/` :

**wax_databricks.py** :

```python
# Databricks notebook source
# MAGIC %md
# MAGIC # WAX Pipeline - Databricks

# COMMAND ----------

# Installation des dÃ©pendances
%pip install openpyxl

# COMMAND ----------

# Import du module principal
import sys
sys.path.append("/dbfs/wax_pipeline/src")

from main import main

# COMMAND ----------

# ExÃ©cution du pipeline
main()

# COMMAND ----------

# MAGIC %md
# MAGIC ## RÃ©sultats
# MAGIC 
# MAGIC VÃ©rifier les logs :
# MAGIC - `/mnt/logs/wax_execution_logs_delta`
# MAGIC - `/mnt/logs/wax_data_quality_errors_delta`
```

---

## âœ… VÃ©rification de l'Installation

```bash
# VÃ©rifier la structure
tree wax_pipeline

# Tester l'import
cd wax_pipeline
python -c "from src import config; print('âœ… Import OK')"

# Lancer les tests
pytest tests/ -v

# ExÃ©cuter le pipeline
python src/main.py
```

---

## ğŸ“ Support

Pour toute question, rÃ©fÃ©rez-vous aux documents fournis ou consultez la documentation inline dans chaque fichier Python.

**Tous les fichiers Python sont fournis dans les documents prÃ©cÃ©dents. Copiez-les dans la structure ci-dessus.**



---------------------->
---------------------->

#!/usr/bin/env python3
"""
WAX Pipeline Generator
GÃ©nÃ¨re automatiquement la structure complÃ¨te du projet WAX Pipeline

Usage:
    python generate_wax_pipeline.py
    
Cela crÃ©era un dossier 'wax_pipeline/' avec tous les fichiers nÃ©cessaires.
"""

import os
import shutil
from pathlib import Path


def create_directory_structure(base_path: str):
    """CrÃ©e la structure de dossiers"""
    dirs = [
        "data/input",
        "data/temp",
        "data/output",
        "src",
        "tests",
        "notebooks",
    ]
    
    for d in dirs:
        path = os.path.join(base_path, d)
        os.makedirs(path, exist_ok=True)
        print(f"âœ… CrÃ©Ã©: {path}")


def create_file(base_path: str, filepath: str, content: str):
    """CrÃ©e un fichier avec son contenu"""
    full_path = os.path.join(base_path, filepath)
    os.makedirs(os.path.dirname(full_path), exist_ok=True)
    
    with open(full_path, 'w', encoding='utf-8') as f:
        f.write(content)
    
    print(f"âœ… CrÃ©Ã©: {filepath}")


def generate_project():
    """GÃ©nÃ¨re le projet complet"""
    base_path = "wax_pipeline"
    
    print("=" * 80)
    print("ğŸš€ GÃ‰NÃ‰RATION DU PROJET WAX PIPELINE")
    print("=" * 80)
    
    # Supprimer le dossier existant si prÃ©sent
    if os.path.exists(base_path):
        response = input(f"âš ï¸  Le dossier '{base_path}' existe dÃ©jÃ . Le supprimer ? (y/n): ")
        if response.lower() == 'y':
            shutil.rmtree(base_path)
            print(f"ğŸ—‘ï¸  Dossier supprimÃ©: {base_path}")
        else:
            print("âŒ AnnulÃ©")
            return
    
    # CrÃ©er la structure
    print("\nğŸ“ CrÃ©ation de la structure...")
    create_directory_structure(base_path)
    
    # README.md
    create_file(base_path, "README.md", """# WAX Pipeline - Data Ingestion Framework

Pipeline ETL industriel pour l'ingestion de donnÃ©es CSV/Excel vers Delta Lake.

## ğŸš€ Installation

```bash
cd wax_pipeline
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou venv\\Scripts\\activate sur Windows
pip install -r requirements.txt
```

## ğŸ¯ Usage

### Mode Local
```bash
python src/main.py
```

### Mode Databricks
Utiliser le notebook `notebooks/wax_databricks.py`

## ğŸ“Š Configuration

Le fichier Excel doit contenir 2 sheets :
- **Field-Column** : MÃ©tadonnÃ©es des colonnes  
- **File-Table** : ParamÃ¨tres des fichiers

## ğŸ”§ Modes d'Ingestion

1. **FULL_SNAPSHOT** : Ã‰crase complÃ¨tement la table
2. **DELTA_FROM_LOT** : Ajout simple (append)
3. **DELTA_FROM_NON_HISTORIZED** : Merge avec mise Ã  jour
4. **FULL_KEY_REPLACE** : Suppression puis insertion par clÃ©

## ğŸ“ Logs

- ExÃ©cution : `/mnt/logs/wax_execution_logs_delta`
- QualitÃ© : `/mnt/logs/wax_data_quality_errors_delta`

## ğŸ§ª Tests

```bash
pytest tests/
```

## ğŸ“„ License

MIT License
""")
    
    # requirements.txt
    create_file(base_path, "requirements.txt", """pyspark==3.5.0
pandas==2.1.4
openpyxl==3.1.2
delta-spark==3.0.0
pytest==7.4.3
pytest-cov==4.1.0
black==23.12.1
flake8==6.1.0
""")
    
    # setup.py
    create_file(base_path, "setup.py", """from setuptools import setup, find_packages

setup(
    name="wax-pipeline",
    version="1.0.0",
    description="Pipeline ETL pour ingestion Delta Lake",
    author="WAX Team",
    packages=find_packages(where="src"),
    package_dir={"": "src"},
    install_requires=[
        "pyspark>=3.5.0",
        "pandas>=2.1.0",
        "openpyxl>=3.1.0",
        "delta-spark>=3.0.0",
    ],
    python_requires=">=3.9",
    entry_points={
        "console_scripts": [
            "wax-pipeline=main:main",
        ],
    },
)
""")
    
    # .gitignore
    create_file(base_path, ".gitignore", """__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
dist/
*.egg-info/
venv/
ENV/
.vscode/
.idea/
*.swp
data/temp/*
data/output/*
*.csv
*.zip
*.parquet
*.log
logs/
.ipynb_checkpoints/
.DS_Store
Thumbs.db
metastore_db/
derby.log
spark-warehouse/
!data/**/.gitkeep
""")
    
    # .gitkeep files
    for d in ["data/input", "data/temp", "data/output"]:
        create_file(base_path, f"{d}/.gitkeep", "")
    
    # src/__init__.py
    create_file(base_path, "src/__init__.py", '''"""
WAX Pipeline - Data Ingestion Framework
Version: 1.0.0
"""

__version__ = "1.0.0"
__author__ = "WAX Team"
''')
    
    # tests/__init__.py
    create_file(base_path, "tests/__init__.py", '''"""Tests for WAX Pipeline"""
''')
    
    print("\n" + "=" * 80)
    print("âœ… PROJET GÃ‰NÃ‰RÃ‰ AVEC SUCCÃˆS !")
    print("=" * 80)
    print(f"\nğŸ“ Le projet a Ã©tÃ© crÃ©Ã© dans : {os.path.abspath(base_path)}")
    print("\nğŸ“‹ Prochaines Ã©tapes :")
    print("   1. cd wax_pipeline")
    print("   2. Copiez vos fichiers Python corrigÃ©s dans src/")
    print("      (config.py, utils.py, validators.py, etc.)")
    print("   3. Placez vos donnÃ©es dans data/input/")
    print("   4. python -m venv venv && source venv/bin/activate")
    print("   5. pip install -r requirements.txt")
    print("   6. python src/main.py")
    print("\n" + "=" * 80)


if __name__ == "__main__":
    generate_project()
