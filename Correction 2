import os
from pyspark.sql import SparkSession

# =========================================================
# üîß D√©tection environnement (local vs Databricks)
# =========================================================
def is_databricks():
    """
    D√©tecte si on est dans un environnement Databricks.
    """
    return "DATABRICKS_RUNTIME_VERSION" in os.environ


# =========================================================
# üìÇ Liste des fichiers (compatible local + Databricks)
# =========================================================
def list_files(extract_dir: str, spark: SparkSession = None):
    """
    Liste les fichiers dans un dossier local ou sur Databricks.
    - Si Databricks : utilise dbutils.fs.ls()
    - Si local : utilise os.listdir()
    """
    if is_databricks():
        try:
            from pyspark.dbutils import DBUtils
            dbutils = DBUtils(spark)
            files = [f.path for f in dbutils.fs.ls(extract_dir)]
            print(f"üìÇ Fichiers d√©tect√©s sur DBFS : {len(files)} trouv√©s")
            return files
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur DBFS ({e}) ‚Üí fallback local")
    
    # Mode local (par d√©faut)
    if os.path.exists(extract_dir):
        files = [os.path.join(extract_dir, f) for f in os.listdir(extract_dir)]
        print(f"üìÇ Fichiers locaux d√©tect√©s : {len(files)} trouv√©s")
        return files
    else:
        print(f"‚ùå Dossier inexistant : {extract_dir}")
        return []
